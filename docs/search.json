[
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Here are the webpages to install the two pieces of software R and RStudio:\n\nDownload and install R 4.1.3 or 4.2.0: https://cran.r-project.org/\nDownload and install RStudio Desktop: https://www.rstudio.com/products/rstudio/. The Open Source Edition version is sufficient.\n\nThe teaching team is always happy to provide help with any computational question."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you‚Äôll upload your PDF and them mark the page(s) where each question can be found. It‚Äôs OK if a question spans multiple pages, just mark them all. It‚Äôs also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Software\nüîóR and RStudio (demo)\n\n\n\n\nAssignment submission & feedback\nüîó on Gradescope\n\n\nTextbooks\nüîóIntroduction to Modern Statistics and R for Data Science\n\n\nRStudio Cheatsheets\nüîóRStudio, RMarkdown, ggplot2 and dplyr"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "STA 101L: Data Analysis and Statistical Inference",
    "section": "",
    "text": "Did you know that statistics was first used in agriculture and that data science has probably revolutionized your favorite sport? Check out these slides to learn more about the impact that statistics and data sciences have had on various fields through short videos."
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you feel comfortable seeking help and can identify where to find what you need without getting too frustrated. This is especially important in a summer class."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once a week. You can find a list of everyone‚Äôs office hours here."
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nYou can email Dr.¬†Raphael Morsomme at rnm16@duke.edu. please include ‚ÄúSTA 101‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA101L emails within 24 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. ARC services are available free to any Duke undergraduate student, in any year, studying in any discipline. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact theARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness is of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu,\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu, or studentaffairs.duke.edu/duwell\n\nIf your mental health concerns you and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach. Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student‚Äôs behavior or health visit the website for resources and assistance. studentaffairs.duke.edu/dukereach\nCounseling and Psychological Services (CAPS). CAPS helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000. studentaffairs.duke.edu/caps\nBlue Devils Care. A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling. bluedevilscare.duke.edu\nTwo-Click Support. Duke Student Government and DukeReach partnership that connects students to help in just two clicks. bit.ly/TwoClickSupport\nWellTrack. Sign up for WellTrack at app.welltrack.com."
  },
  {
    "objectID": "course-support.html#technology-accommodations",
    "href": "course-support.html#technology-accommodations",
    "title": "Course support",
    "section": "Technology accommodations",
    "text": "Technology accommodations\nStudents with demonstrated high financial need who have limited access to computers may request assistance in the form of loaner laptops. For technology assistance requests, please go here. Please note that supplies are limited.\nNote that all software we use in this course is open-source and/or freely available."
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbook, journal articles, etc.)."
  },
  {
    "objectID": "course-support.html#assistance-with-zoom-or-sakai",
    "href": "course-support.html#assistance-with-zoom-or-sakai",
    "title": "Course support",
    "section": "Assistance with Zoom or Sakai",
    "text": "Assistance with Zoom or Sakai\nFor technical help with Sakai or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Sakai here.\nNote that we will be making minimal use of Sakai in this course (primarily for announcements and grade book). All assignment submission will take place on Gradescope.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "",
    "text": "Click here to download a PDF copy of the syllabus."
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Course info",
    "text": "Course info\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nMon, Tue & Thu\n3:30 pm - 5:35 pm\nOld Chemistry Building 123\n\n\nLabs\nWed & Fri\n3:30 pm - 4:45 pm\nOld Chemistry Building 123"
  },
  {
    "objectID": "course-syllabus.html#overview",
    "href": "course-syllabus.html#overview",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Overview",
    "text": "Overview\nWelcome to STA101L Data Analysis and Statistical Inference. The goal of this class is to prepare you to be critical consumers of statistical analyses in your scientific fields of practice and future professions. Our point of departure will be to think about data collection: how to (not) collect data, and how the way in which data are collected impacts the analysis that we conduct. We will then quickly delve into data visualization. Ever heard of a mosaic plot or an inter-quartile range? This is an area of data analysis where creativity and an eye for good design can make a difference. Once we have good grasp of how to visualize data, we will construct statistical models to make predictions and to understand the relations that exist between variables.\nWhile models can be useful (especially if their predictions are accurate!), all models are inherently wrong. The statistical tests that we will learn in the second half of the course will help us quantify how (un)certain we are that the models we construct pick up real patterns in the data and not just background noise.\nThroughout the class, you will attend hands-on labs in which you will learn to implement all these techniques in the statistical computing software R."
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the term you will be able to‚Ä¶\n\nvisualize and summarize data sets with numerical and categorical variables;\nconstruct and investigate linear regression models for forecasting; this includes fitting, evaluating, comparing and selecting models, as well as interpreting their output;\nconduct hypothesis tests and construct confidence intervals for proportions, differences between proportions, means, differences between means and regression coefficients;\nimplement these techniques in R, and use RMarkdown to write reproducible reports;\nplan and complete a statistical analysis of a real-world phenomenon using visual and numerical summaries, hypothesis tests, confidence intervals and regression models."
  },
  {
    "objectID": "course-syllabus.html#prerequisite",
    "href": "course-syllabus.html#prerequisite",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Prerequisite",
    "text": "Prerequisite\nThere is no prerequisite for this class."
  },
  {
    "objectID": "course-syllabus.html#community",
    "href": "course-syllabus.html#community",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Community",
    "text": "Community\n\nDuke Community Standard\nAs a student in this course, you have agreed to uphold the Duke Community Standard as well as the practices specific to this course.\n\n\n\n\nInclusive community\nIt is my intent that students from all backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and organize activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nif you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me; if you prefer to speak with someone outside of the course, your academic dean is an excellent resource;\nI (like many people) am still in the process of learning about diverse perspectives and identities; if something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website at rmorsomme.github.io/website.\nI will regularly send course announcements via email, make sure to check your mail box regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course forum Conversations. There is a chance another student has already asked a similar question, so please check the other posts in Conversations before adding a new question. If you know the answer to a question posted in the discussion forum, I encourage you to respond!\nEmails should be reserved for questions not appropriate for the public forum. If you email me, please include ‚ÄúSTA 101‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 101 emails within 24 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "course-syllabus.html#textbook",
    "href": "course-syllabus.html#textbook",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Textbook",
    "text": "Textbook\nThe class will closely follow the book Introduction to Modern Statistics (first edition) by Mine √áetinkaya-Rundel and Johanna Hardin. This is an open-source book freely available online. You‚Äôre welcomed to read on screen or print it out. If you prefer a paperback version you can buy it at the cost of printing (around $20) on Amazon. The textbook store will not carry copies of this text.\nChapters 1-7 of the book R for Data Science by Garret Grolemund and Hadley Wickham (also open-source and freely available) will also be useful."
  },
  {
    "objectID": "course-syllabus.html#lectures-and-labs",
    "href": "course-syllabus.html#lectures-and-labs",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom may not be sufficient to accommodate everyone. More information on loaner laptops can be found here."
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Assessment",
    "text": "Assessment\nAssessment for the course is comprised of three components: regular homework assignments, a prediction group project and an inference group project.\n\nHomework assignments\nTo reduce the number of assignments during the summer session, problem sets and lab exercises will be merged. In these homework assignments, you will apply what you‚Äôve learned during lectures and labs to show conceptual understanding of the content and complete data analysis tasks in R. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Homework must be typed up using RMarkdown and submitted as a PDF on Gradescope.\nThe homework assignment with the lowest grade will be dropped at the end of the term.\n\n\nProjects\nThere will be two group projects. Through these, you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The projects will focus on the two pillars of statistics and data science: prediction and inference. In the first project, you will construct a regression model for prediction. The goal will be to build a model that provides predictions that are as accurate as possible. In the second project, you will analyze a phenomenon that interests you using real-world data. More detail about the projects will be given during the term."
  },
  {
    "objectID": "course-syllabus.html#grading",
    "href": "course-syllabus.html#grading",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\nAssessment\nPercentage\n\n\n\n\nHomework\n50%\n\n\nPrediction project\n20%\n\n\nInference project\n30%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n>= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n< 60\n\n\n\nNote that the final grade may be curved."
  },
  {
    "objectID": "course-syllabus.html#five-tips-for-success",
    "href": "course-syllabus.html#five-tips-for-success",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. Your TA and I will help you by providing you with materials and answering questions, but for this to work you need to do the following:\n\ncomplete the assigned reading before the lectures;\nask questions quickly; don‚Äôt let a day pass by with lingering questions;\ndo the homework assignments thoroughly;\npractice, practice, practice;\ndon‚Äôt procrastinate; start on the homework assignments and the projects early."
  },
  {
    "objectID": "course-syllabus.html#course-policies",
    "href": "course-syllabus.html#course-policies",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic integrity\nDon‚Äôt cheat!\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard:\nStudents affirm their commitment to uphold the values of the Duke University community by signing a pledge that states:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;\nI will act if the Standard is compromised\n\nRegardless of course delivery format, it is your responsibility to understand and follow Duke policies regarding academic integrity, including doing one‚Äôs own work, following proper citation of sources, and adhering to guidance around group work projects. Ignoring these requirements is a violation of the Duke Community Standard. If you have any questions about how to follow these requirements, please contact Jeanna McCullers (jeanna.mccullers@duke.edu), Director of the Office of Student Conduct.\n\n\nCollaboration policy\nOnly work that is clearly assigned as team work should be completed collaboratively.\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the TA.\nFor the projects, collaboration between teams at a high level is also allowed; however, you may not share code or components of the project with other teams.\n\n\n\nPolicy on sharing and reusing code\nI am well aware that a huge volume of code is available on the web to solve any number of problems. You may make use of any online resources (e.g.¬†RStudio Community, StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. On homework assignments you may not directly share code with another student in this class, and on the projects you may not directly share code with another team.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. Given the fast pace of a summer class, any assignment submitted after the deadline will not be graded. The homework assignments will be published early to give you ample time to work on them. Note that the lowest homework will be dropped.\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email Raphael Morsomme and our TA Rohit Roy before the deadline to obtain a 24-hour extension. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let a member of the instruction team know if you need help contacting your academic dean.\n\n\nRegrade request policy\nRegrade requests must be submitted on Gradescope within 48 hours of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the inference project report is due.\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this term. Lab time is dedicated to working on your lab assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. If you know you‚Äôre going to miss a lab session and you‚Äôre feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others‚Äô time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university‚Äôs top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have symptoms related to COVID-19, have had a known exposure to COVID-19, or have tested positive for COVID-19. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health at 919-681-9355. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\nPlease read and follow university guidance here. The current guidelines are for students and instructors to wear masks during class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously or watching a recording of the class.\n\n\nPolicy on video recording course content\nIf you feel that you need record the lectures, you must get permission from the instructor ahead of time and these recordings should be used for personal study only, no for distribution. The full policy on recording of lectures falls under the Duke University Policy on Intellectual Property Rights, available at provost.duke.edu/sites/default/files/FHB_App_P.pdf. Unauthorized distribution is a cause for disciplinary action by the Judicial Board."
  },
  {
    "objectID": "course-syllabus.html#learning-during-a-pandemic",
    "href": "course-syllabus.html#learning-during-a-pandemic",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Learning during a pandemic",
    "text": "Learning during a pandemic\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don‚Äôt hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you‚Äôre always welcome to talk to me. If I can‚Äôt help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this health crisis.\n\nNote: If you‚Äôve read this far in the syllabus, email me a picture of your pet if you have one or your favourite meme!"
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Important dates",
    "text": "Important dates\n\nMay 11: Classes begin (Monday meeting schedule)\nMay 13: Drop/add ends\nMay 16: Regular class meeting schedule begins\nMay 30: Memorial Day holiday, no class is held\nMay 31: Prediction project\nJune 8: Last day to withdraw with W\nJune 16: Inference project presentation\nJune 17: Classes end\nJune 20: Juneteenth holiday\nJune 21: Reading period\nJune 23: Last assignment: inference project report\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "course-syllabus.html#attribution",
    "href": "course-syllabus.html#attribution",
    "title": "Syllabus - STA101L Summer I 2022",
    "section": "Attribution",
    "text": "Attribution\nSome portions of this syllabus are based on the syllabus of STA 210 - Spring 2022 by Prof.¬†Mine √áetinkaya-Rundel."
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Raphael Morsomme (he/him/his) is a Ph.D.¬†candidate at the Department of Statistical Science at Duke University. Raphael‚Äôs work focuses on the development of flexible and scalable stochastic epidemic models along with efficient Markov Chain Monte Carlo algorithms to fit these models. In addition, he uses data augmentation to study the overdiagnosis rate of breast cancer.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday, 10:00-11:00am\nZoom\n\n\nWednesday, 10:00-11:00am\nZoom\n\n\n\nIf these times don‚Äôt work for you or you‚Äôd like to schedule a one-on-one meeting, simply send an email to raphael.morsomme@duke.edu."
  },
  {
    "objectID": "course-team.html#teaching-assistant",
    "href": "course-team.html#teaching-assistant",
    "title": "Teaching team",
    "section": "Teaching assistant",
    "text": "Teaching assistant\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\n\nRohit Roy - rohit.roy@duke.edu\nWed, 4:45-5:45pm\nFri, 4:45-5:45pm\nHybrid (Zoom and room 119 [TBC] )\nHybrid (Zoom and room 119 [TBC])"
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "HW 1 - Introduction to data",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a pdf on Gradescope. Feel free to use the template that has been provided."
  },
  {
    "objectID": "hw/hw-1.html#problem-set-25-points",
    "href": "hw/hw-1.html#problem-set-25-points",
    "title": "HW 1 - Introduction to data",
    "section": "Problem set (25 points)",
    "text": "Problem set (25 points)\n\nExercise 1.2 (2 points)\nExercise 1.8 (5 points)\nExercise 1.12 (5 points)\nExercise 1.20 (2 points)\nExercise 2.8 (2 points)\nExercise 2.12 (3 points)\nExercise 2.14 (1 point)\nExercise 2.22 (2 points)\nExercise 2.22 - follow-up (3 points)\n\nIs your study blind?\nIs it double-bind?\nIf it is not blind, can you make it blind?"
  },
  {
    "objectID": "hw/hw-1.html#lab-exercises---dr.-arbuthnots-baptism-records-25-points",
    "href": "hw/hw-1.html#lab-exercises---dr.-arbuthnots-baptism-records-25-points",
    "title": "HW 1 - Introduction to data",
    "section": "Lab exercises - Dr.¬†Arbuthnot‚Äôs baptism records (25 points)",
    "text": "Lab exercises - Dr.¬†Arbuthnot‚Äôs baptism records (25 points)\nYou can find the lab here\n\nExercise 1 (2 points)\nExercise 2 (3 points)\nExercise 3 (4 points)\nExercise 4 (4 points)\nExercise 5 (2 points)\nExercise 6 (5 points)\nExercise 7 (5 points)"
  },
  {
    "objectID": "hw/hw-2.html",
    "href": "hw/hw-2.html",
    "title": "HW 2 - Data visualization and summary",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-2.html#problem-set-25-points",
    "href": "hw/hw-2.html#problem-set-25-points",
    "title": "HW 2 - Data visualization and summary",
    "section": "Problem set (25 points)",
    "text": "Problem set (25 points)\n\nExercise 5.2 (2 points)\nExercise 5.4 (2 points) ‚Äì instead of drawing, you may describe the relationship in words.\nExercise 5.6 (3 points)\nExercise 5.10 (3 points)\nExercise 5.16 (5 points)\nExercise 5.20 (1 points)\nExercise 5.20 follow up (1 points)\n\nWould side-by-side boxplots have been suitable to visualize these data? Briefly explain.\n\nExercise 5.22 (2 points)\nExercise 5.26 (2 points)\nExercise 4.6 (4 points)"
  },
  {
    "objectID": "hw/hw-2.html#lab-exercises---new-york-flights-25-points",
    "href": "hw/hw-2.html#lab-exercises---new-york-flights-25-points",
    "title": "HW 2 - Data visualization and summary",
    "section": "Lab exercises - New York flights (25 points)",
    "text": "Lab exercises - New York flights (25 points)\nYou can find the lab here\n\nExercise 1 (2 points)\nExercise 2 (3 points)\nExercise 3 (3 points)\nExercise 4 (3 points)\nExercise 5 (2 points)\nExercise 6 (2 points)\nExercise 7 (2 points)\nExercise 8 (3 points)\nExercise 9 (5 points)"
  },
  {
    "objectID": "hw/hw-3.html",
    "href": "hw/hw-3.html",
    "title": "HW 3 - Simple linear regression",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-3.html#problem-set-25-points",
    "href": "hw/hw-3.html#problem-set-25-points",
    "title": "HW 3 - Simple linear regression",
    "section": "Problem set (25 points)",
    "text": "Problem set (25 points)\n\n7.6, a-c (3 points)\n7.18 (2 points)\n7.20 (4 points)\n7.22 (7 points)\n\nfit the model in R; the variables sho_gi and hgt respectively correspond to shoulder girth and height.\n\nd <- openintro::bdims\n\ndo parts a-f\n\n7.24 (5 points)\n\nfit the model in R\n\nd <- MASS::cats\n\ndo parts a-d\n\n7.26 (3 points)\n7.28 (1 point)"
  },
  {
    "objectID": "hw/hw-3.html#lab-exercises---the-human-freedom-index-25-points",
    "href": "hw/hw-3.html#lab-exercises---the-human-freedom-index-25-points",
    "title": "HW 3 - Simple linear regression",
    "section": "Lab exercises - the Human Freedom Index (25 points)",
    "text": "Lab exercises - the Human Freedom Index (25 points)\nYou can find the lab here\n\nExercise 1 (2 points)\nExercise 2 (2 points)\nExercise 3 (3 points)\nExercise 4 (2 points)\nExercise 5 (2 points)\nExercise 6 (3 points)\nExercise 7 (2 points)\nExercise 8 (2 points)\n\nSkip exercises 9-10.\n\nPractice 1 (2 points)\nPractice 2 (2 points)\nPractice 3 (1 points) ‚Äì only evaluate linearity\nPractice 4 (2 points)"
  },
  {
    "objectID": "hw/hw-4.html",
    "href": "hw/hw-4.html",
    "title": "HW 4 - Multiple linear regression",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-4.html#problem-set-and-applied-exercises-20-points",
    "href": "hw/hw-4.html#problem-set-and-applied-exercises-20-points",
    "title": "HW 4 - Multiple linear regression",
    "section": "Problem set and applied exercises (20 points)",
    "text": "Problem set and applied exercises (20 points)\n\n8.6 (2 points)\n8.10 (11 points)\n\nRead in the data in R with the following command. How many observations and variables are there?\n\nd <- palmerpenguins::penguins\n\nIdentify the type of each variable.\nFit the model in R using the lm command. You should obtain the same estimates as in the book.\nIdentify the baseline level of the categorical predictors.\nDo parts a-d.\nCompute \\(R^2\\) by hand (do not use glance). Hint: compute SSR and SST.\n\n8.12 (1 points)\n8.14 (3 points)\n\nFit the candidate model in R and compute its adjusted-\\(R^2\\) value. You should obtain the same value as in the book.\n\nd <- palmerpenguins::penguins\n\nDo exercise 8.14.\n\n25.10 (3 points)"
  },
  {
    "objectID": "hw/hw-4.html#lab-exercises---grading-the-professor-15-points",
    "href": "hw/hw-4.html#lab-exercises---grading-the-professor-15-points",
    "title": "HW 4 - Multiple linear regression",
    "section": "Lab exercises - Grading the professor (15 points)",
    "text": "Lab exercises - Grading the professor (15 points)\nYou can find the lab here\n\nExercise 1 (2 points)\nExercise 2 (2 points)\nExercise 3 (1 points)\n\nSkip exercises 4, 5, 6, 7, 8\n\nExercise 9 (2 points)\nExercise 10 (2 points)\n\nSkip exercises 11, 12\n\nExercise 13 (1 point)\nExercise 14 (5 point)\n\nDrop one variable at a time and peek at the adjusted \\(R^2\\). Removing which variable increases adjusted \\(R^2\\) the most?\nSkip the rest of the question.\n\n\nSkip the remaining exercises."
  },
  {
    "objectID": "hw/hw-4.html#function-for-loop-and-cross-validation-in-r-15-points",
    "href": "hw/hw-4.html#function-for-loop-and-cross-validation-in-r-15-points",
    "title": "HW 4 - Multiple linear regression",
    "section": "Function, for-loop and cross-validation in R (15 points)",
    "text": "Function, for-loop and cross-validation in R (15 points)\n\nFunction ‚Äì Write a function in R that computes the average \\(\\bar{x}\\) of numerical variable from scratch (do not use the command mean). Your function should be called my_mean, should take a vector of numbers as input and should output its average. Show that your function works by applying in on the variable cty of the ggplot2::mpg data. You should obtain the same value as the command mean. (5 points)\nFor-loop ‚Äì Write a for-loop that computes the first 20 Fibonacci numbers. The first few Fibonacci numbers are \\(0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ‚Ä¶\\). These numbers follow the following rule: each is the sum of the previous two; that is,\n\\[\nx_n = x_{n-1}+x_{n-2},\n\\]\nand the first two numbers are \\(x_1=0\\) and \\(x_2=1\\). For instance, the third number is \\(1 = x_3 = x_2 + x_1 = 1 + 0\\) and the seventh number is \\(8 = x_7 = x_6 + x_5 = 5 + 3\\) (5 points).\nCross-validation ‚Äì Use the code from lecture to implement \\(5\\)-fold cross-validation on the prediction project data to compare the simple regression model with gestation_week as the predictor and the full model with all 15 raw predictors. (5 points)"
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "HW 5 - Inference for proportions",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-5.html#problem-set-25-points",
    "href": "hw/hw-5.html#problem-set-25-points",
    "title": "HW 5 - Inference for proportions",
    "section": "Problem set (25 points)",
    "text": "Problem set (25 points)\n\n11.2 (1 points) ‚Äì parts a-d (skip part e)\n11.4 (2 points) ‚Äì parts a-b (skip part c)\n11.6 (2 points)\n12.2 (2 points)\n12.6 (4 points)\n12.8 (3 points)\n16.2 (1 point)\n16.10 (4 points)\n17.2 (2 points) ‚Äì parts a and c (skip part b)\n17.4 (1 points) ‚Äì part c (skip parts a and b)\n17.6 (3 points)"
  },
  {
    "objectID": "hw/hw-5.html#lab-exercises---the-youth-risk-behavior-surveillance-system-25-points",
    "href": "hw/hw-5.html#lab-exercises---the-youth-risk-behavior-surveillance-system-25-points",
    "title": "HW 5 - Inference for proportions",
    "section": "Lab exercises - the Youth Risk Behavior Surveillance System (25 points)",
    "text": "Lab exercises - the Youth Risk Behavior Surveillance System (25 points)\nYou can find the lab here\n\nExercise 1 (2 points)\nExercise 2 (2 points)\nExercise 3 (2 points)\nExercise 4 (2 points)\nExercise 5 (2 points)\nExercise 6 (2 points)\nExercise 7 (2 points)\nExercise 8 (2 points)\nExercise 9 (3 points)\nExercise 10 (3 points)\nExercise 11 (3 points)"
  },
  {
    "objectID": "hw/hw-6.html",
    "href": "hw/hw-6.html",
    "title": "HW 6 - Inference for means",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-6.html#problem-set-25-points",
    "href": "hw/hw-6.html#problem-set-25-points",
    "title": "HW 6 - Inference for means",
    "section": "Problem set (25 points)",
    "text": "Problem set (25 points)\n\n19.2 (2 points)\n19.6 (2 points) ‚Äì part b only\n19.20 (2.5 points)\n20.4 (2 points)\n20.6 (1 point) ‚Äì part a only\n20.8 (6 points)\n21.2 (2 points)\n21.4 (1.5 points)\n21.6 (2.5 points)\n21.8 (1.5 points) ‚Äì skip part b; in part c, only interpret CI from part a\n21.10 (2 points)"
  },
  {
    "objectID": "hw/hw-6.html#lab-exercises---the-youth-risk-behavior-surveillance-system-25-points",
    "href": "hw/hw-6.html#lab-exercises---the-youth-risk-behavior-surveillance-system-25-points",
    "title": "HW 6 - Inference for means",
    "section": "Lab exercises - the Youth Risk Behavior Surveillance System (25 points)",
    "text": "Lab exercises - the Youth Risk Behavior Surveillance System (25 points)\nYou can find the lab here.\n\nExercise 1 (2 points)\nExercise 2 (1 point)\nExercise 3 (2 points) ‚Äì see the ggplot2 cheatsheet for the command to make violin plot.\nExercise 4 (1 point)\nExercise 5 (1 point)\nExercise 6 (2 points)\nExercise 7 (1 point)\nExercise 8 (1 point)\nExercise 9 (2 points)\nExercise 10 (2 points)\nExercise 11 (2 points)\nExercise 12 (2 points)\nExercise 13 (2 points)\nExercise 14 (4 points)"
  },
  {
    "objectID": "hw/hw-7.html",
    "href": "hw/hw-7.html",
    "title": "HW 7 - Modern statistical inference (one group)",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-7.html#problem-set-10-points",
    "href": "hw/hw-7.html#problem-set-10-points",
    "title": "HW 7 - Modern statistical inference (one group)",
    "section": "Problem set (10 points)",
    "text": "Problem set (10 points)\n\n9.2 ‚Äì parts b and d (1 point)\n9.4 (4 points) ‚Äì Hint: use the symbol $ to write mathematical equations; for instance, to write the fraction \\(\\frac{e^{\\mu}}{\\beta_0}\\) simply use $\\frac{e^{\\mu}}{\\beta_0}$.\n9.8 (3 points)\n24.8 ‚Äì parts b-c with a percentile bootstrap interval, the usual bootstrap interval that we have constructed in class (2 points)\n\nFor the remaining exercises, use the birth data set\n\nlibrary(readr)\nd <- read_csv(\"https://rmorsomme.github.io/website/projects/training_set.csv\")"
  },
  {
    "objectID": "hw/hw-7.html#confidence-intervals-via-bootstrap-18-points",
    "href": "hw/hw-7.html#confidence-intervals-via-bootstrap-18-points",
    "title": "HW 7 - Modern statistical inference (one group)",
    "section": "Confidence intervals via bootstrap (18 points)",
    "text": "Confidence intervals via bootstrap (18 points)\nConstruct (5 points) and interpret in the context of the problem (1 point) a 95% confidence interval using 10000 bootstrap samples for\n\nthe proportion of female newborn (6 points)\nthe mean weight of newborns (6 points)\nthe slope of gestation week in a simple linear regression model for newborn_birth_weight (6 points)"
  },
  {
    "objectID": "hw/hw-7.html#hypothesis-test-via-simulation-21-points",
    "href": "hw/hw-7.html#hypothesis-test-via-simulation-21-points",
    "title": "HW 7 - Modern statistical inference (one group)",
    "section": "Hypothesis test via simulation (21 points)",
    "text": "Hypothesis test via simulation (21 points)\nConduct a hypothesis test using simulation for the following cases. State the hypotheses in words and in mathematical symbols (2 points) and use 10000 samples simulated samples and the significance level \\(\\alpha=0.05\\) (5 points).\n\nYou want to determine whether exactly half of the newborns are female (7 points)\nYou want to determine whether the mean weight of newborns is 3500 grams (7 points)\nYou want to determine whether newborn_birth_weight is independent of gestation week (7 points)"
  },
  {
    "objectID": "hw/hw-7.html#reproducibility-1-point",
    "href": "hw/hw-7.html#reproducibility-1-point",
    "title": "HW 7 - Modern statistical inference (one group)",
    "section": "Reproducibility (1 point)",
    "text": "Reproducibility (1 point)\nA seed is set with the command set.seed before any code with a stochastic component."
  },
  {
    "objectID": "hw/hw-8.html",
    "href": "hw/hw-8.html",
    "title": "HW 8 - Modern statistical inference (two groups)",
    "section": "",
    "text": "This assignment needs to be completed with RMarkdown and submitted as a PDF on Gradescope. Feel free to re-use the template provided for HW1.\nWhen submitting your work on Gradescope, please assign a page for each question."
  },
  {
    "objectID": "hw/hw-8.html#problem-set-19-points",
    "href": "hw/hw-8.html#problem-set-19-points",
    "title": "HW 8 - Modern statistical inference (two groups)",
    "section": "Problem set (19 points)",
    "text": "Problem set (19 points)\n\n9.8 (3 points)\n\nFor the remaining exercises, use the birth data set\n\nlibrary(readr)\nd <- read_csv(\"https://rmorsomme.github.io/website/projects/training_set.csv\")"
  },
  {
    "objectID": "hw/hw-8.html#confidence-intervals-via-bootstrap-14-points",
    "href": "hw/hw-8.html#confidence-intervals-via-bootstrap-14-points",
    "title": "HW 8 - Modern statistical inference (two groups)",
    "section": "Confidence intervals via bootstrap (14 points)",
    "text": "Confidence intervals via bootstrap (14 points)\nConstruct (7 points) and interpret in the context of the problem (1 point) a 95% confidence interval using 10000 bootstrap samples for\n\nthe difference in proportion of female newborns among women with and without risk factor (7 points)\nthe difference in the mean weight of newborns among women with and without risk factor (7 points)"
  },
  {
    "objectID": "hw/hw-8.html#hypothesis-test-via-simulation-16-points",
    "href": "hw/hw-8.html#hypothesis-test-via-simulation-16-points",
    "title": "HW 8 - Modern statistical inference (two groups)",
    "section": "Hypothesis test via simulation (16 points)",
    "text": "Hypothesis test via simulation (16 points)\nConduct a hypothesis test using simulation for the following cases. State the hypotheses in words and in mathematical symbols (2 points) and use 10000 samples simulated samples and the significance level \\(\\alpha=0.05\\) (6 points).\n\nYou want to determine whether women with and without risk factor have the same proportion of female newborns (8 points)\nYou want to determine whether women with and without risk factor have newborns with the same mean weight (8 points)"
  },
  {
    "objectID": "hw/hw-8.html#reproducibility-1-point",
    "href": "hw/hw-8.html#reproducibility-1-point",
    "title": "HW 8 - Modern statistical inference (two groups)",
    "section": "Reproducibility (1 point)",
    "text": "Reproducibility (1 point)\nA seed is set with the command set.seed before any code with a stochastic component."
  },
  {
    "objectID": "hw/template.html",
    "href": "hw/template.html",
    "title": "HW 1 - Introduction to data",
    "section": "",
    "text": "Lab exercises - Dr.¬†Arbuthnot‚Äôs baptism records (25 points)\nYou can find the lab here\n\nlibrary(tidyverse)\nlibrary(openintro)\n\n\nExercise 1 (2 points)\nInsert any text here.\n\n# Insert code for Exercise 1 here\n\n\n\nExercise 2 (3 points)\nInsert any text here.\n\n# Insert code for Exercise 2 here\n\n\n\nExercise 3 (4 points)\nInsert any text here.\n\n# Insert code for Exercise 3 here\n\n\n\nExercise 4 (4 points)\nInsert any text here.\n\n# Insert code for Exercise 4 here\n\n\n\nExercise 5 (2 points)\nInsert any text here.\n\n# Insert code for Exercise 5 here\n\n\n\nExercise 6 (5 points)\nInsert any text here.\n\n# Insert code for Exercise 6 here\n\n\n\nExercise 7 (5 points)\nInsert any text here.\n\n# Insert code for Exercise 7 here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 101L: Data Analysis and Statistical Inference",
    "section": "",
    "text": "Week\nDate\nTopic\nReading\nSlides\nLab\nHW\nProject\n\n\n\n\n0.5\nWed, May 11\nWelcome to STA 101!\nTypes of data and studies\n(1.2; 2.2-3)\nüñ•Ô∏è\nüñ•Ô∏è\n\n\n\n\n\n\nThu, May 12\nVisualization I\n5.1-5.6\nüñ•Ô∏è\n\n\n\n\n\n\nFri, May 13\nLab 1 - Introduction to R and RStudio\n\n\nüíª\n\n\n\n\n\nSun, May 15\nDue: homework 1\n\n\n\n‚úçÔ∏è\n\n\n\n1\nMon, May 16\nVisualization II\n4.1-4.5\nüñ•Ô∏è\n\n\n\n\n\n\nTue, May 17\nSimple linear regression\n7.1-7.3\nüñ•Ô∏è\n\n\n\n\n\n\nWed, May 18\nLab 2 - Visualization\nDue: Homework 2\n\n\nüíª\n‚úçÔ∏è\n\n\n\n\nThu, May 19\nMultiple linear regression\n8.1-8.2\nüñ•Ô∏è\n\n\n\n\n\n\nFri, May 20\nLab 3 - Linear regression\n\n\nüíª\n\n\n\n\n\nSun, May 22\nDue: Homework 3\n\n\n\n‚úçÔ∏è\n\n\n\n2\nMon, May 23\nModel selection I\n8.3\nüñ•Ô∏è\n\n\n\n\n\n\nTue, May 24\nModel selection II\n8.4; 25.3\nüñ•Ô∏è\n\n\n\n\n\n\nWed, May 25\nLab 4 - functions, loops\n\n\nüíª\n\n\n\n\n\nThu, May 26\nLogistic regression\nWork on project\nDue: Homework 4\n9.1-9.3\nüñ•Ô∏è\n\n‚úçÔ∏è\nüìÇ\n\n\n\nFri, May 27\nLab 5 - work on prediction project (PP)\n\n\n\n\n\n\n\n3\nMon, May 30\nMemorial Day holiday\n\n\n\n\n\n\n\n\nTue, May 31\nDue: PP model and presentation\nPrinciples of inference\n11.0; 11.3; 12.0; 12.3\nüñ•Ô∏è\n\n\nPP model (9:00am)\nPP presentation\n\n\n\nWed, June 1\nLab 6 - Lecture on inference for proportions I\nDue: PP written report\n16.1\nüñ•Ô∏è\n\n\nPP written report\n\n\n\nThu, June 2\nInference for proportions II\n17.1-17.2\nüñ•Ô∏è\n\n\n\n\n\n\nFri, June 3\nLab 7 - Inference for proportions\n\n\nüíª\n\n\n\n\n\nSun, June 5\nDue: homework 5\n\n\n\n‚úçÔ∏è\n\n\n\n4\nMon, June 6\nInference for means I\n13.2; 19.1; 20.1-20.2\nüñ•Ô∏è\n\n\n\n\n\n\nTue, June 7\nInference for means II\n21.1-21.2\nüñ•Ô∏è\n\n\n\n\n\n\nWed, June 8\nLab 8 - Inference for means\n\n\nüíª\n\n\n\n\n\nThu, June 9\nInference for regression\n24.2-24.3\nüñ•Ô∏è\n\n‚úçÔ∏è\n\n\n\n\nFri, June 10\nLab 9 - inference for regression\n\n\nüíª\n\n\n\n\n5\nMon, June 13\nClassical inference I\nDue: Homework 7\n13.1-13.2\n\n\n‚úçÔ∏è\n\n\n\n\nTue, June 14\nClassical inference II\nANOVA and \\(\\chi^2\\) tests\n18.1-18.2; 22.2-22.3\n\n\n\n\n\n\n\nWed, June 15\nLab 10 - work on inference project (IP)\n\n\n\n\n\n\n\n\nThu, June 16\nWork on IP\nDue: Homework 8\n\n\n\n\n\n\n\n\nFri, June 17\nLab 11 - IP presentations\n\n\n\n\nIP presentation\n\n\n\nSun, June 19\nDue: Homework 9\n\n\n\n\n\n\n\n6\nMon, June 20\nJuneteenth holiday\n\n\n\n\n\n\n\n\nTue, June 21\nReading period\n\n\n\n\n\n\n\n\nWed, June 22\n\n\n\n\n\n\n\n\n\nThu, June 23\nDue: IP written report\n\n\n\n\nIP written report"
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "STA 101L - Summer I 2022",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "project-inference.html",
    "href": "project-inference.html",
    "title": "Inference project",
    "section": "",
    "text": "In the inference project, you will work in pairs to conduct a statistical analysis of a data set that interests you. You will present your work on Friday, June 17 during the lab and submit a written report by Thursday, June 23, 9:00pm. You can find your teammate here.\nüçÄ Good luck! üçÄ"
  },
  {
    "objectID": "project-inference.html#introduction",
    "href": "project-inference.html#introduction",
    "title": "Inference project",
    "section": "Introduction",
    "text": "Introduction\nThe goal of the inference project is for you to use data visualization, regression modelling and statistical inference to analyze a data set of your choice, demonstrating proficiency in the techniques covered in class and applying them to a real data set in a meaningful way.\nAll analyses must be done in RMarkdown, and all components of the project must be reproducible (with the exception of the presentation)."
  },
  {
    "objectID": "project-inference.html#academic-integrity",
    "href": "project-inference.html#academic-integrity",
    "title": "Inference project",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBy participating in this project, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "project-inference.html#data",
    "href": "project-inference.html#data",
    "title": "Inference project",
    "section": "Data",
    "text": "Data\nThe data set that you analyze needs to have at least 100 observations and 5 meaningful variables (identifier variables such as ‚Äúname‚Äù, ‚Äúsocial security number‚Äù or ‚Äúid‚Äù do not count), including at least one categorical and one numerical variable.\nYou are welcome to analyze data from your own work/research or to use any real data set that is publicly available. Here are a few examples.\n\nU.S. birth data (see prediction project)\nWorld birth data\nDuke Lemur Center\nCar fuel economy ‚Äì EPA"
  },
  {
    "objectID": "project-inference.html#submission",
    "href": "project-inference.html#submission",
    "title": "Inference project",
    "section": "Submission",
    "text": "Submission\nThe three primary deliverables for this project are:\n\nFormal presentation: you will present your work orally to rest of the class. The presentation should be no longer than 10 minutes (aim for 10 slides). The two team members should speak roughly the same amount of time. Each presentation will be followed by a short QA session.\nFinal report (pdf): The final report details your work. It needs to be realized using RMarkdown and submitted on Gradescope as a PDF. This report should not contain any R code, message or warning. To ensure that this is the case, simply use the following as the first code chunk of your document.\n\n```{r set-up, include = FALSE}\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\n```\n\nThe page limit is 10 pages (excluding the appendix). Figures should go in the appendix, along with any work that you wish to include, but which does not fit in the main body. Grading will largely be based on the content in the main body of the report. You should assume that the reader will not see the material in the appendix unless prompted to view it in the main body of the report.\nFinal report (RMD): You need to submit the RMD file used to make the report for reproducibility. You can simply email it to the instruction team. The instructors need to be able to reproduce your analysis by knitting your document. If the RMD file depends on other files, e.g.¬†data sets, make sure to send these as well."
  },
  {
    "objectID": "project-inference.html#report-content",
    "href": "project-inference.html#report-content",
    "title": "Inference project",
    "section": "Report content",
    "text": "Report content\nThe report should include the following sections, though you should feel free to include additional sections as necessary.\n\nIntroduction: introduce the subject matter that you are investigating, the general research question you are exploring, the motivation for this research question and the data you are analyzing to answer that question.\nData: describe the data set and the variables that you consider in the analysis (no need to list all 500 variables present in your data). Visualize and summarize the response variable, important predictors and any new variable that you have engineered.\nMethodology: describe the modeling and inferential process. Motivate your decisions (type of regression model, type of statistical inference, outliers, feature engineering, model selection, etc).\nResults and discussion: provide the results of your analysis; including the output of the selectd regression model, the confidence interval and the result of the hypothesis test. Interpret the results you have obtained in the context of the subject matter and original research question. Are any of your findings unexpected? Briefly discuss any limitation to your work."
  },
  {
    "objectID": "project-prediction.html",
    "href": "project-prediction.html",
    "title": "Prediction project",
    "section": "",
    "text": "In this prediction project, you will work in pairs to construct a regression model that will be used to make predictions. The prediction model is due on Tuesday, May 31, at 9:00 am, and the final report is due Wednesday, June 1 at 9:00pm. You can find your teammate here.\nüçÄ Good luck! üçÄ"
  },
  {
    "objectID": "project-prediction.html#introduction",
    "href": "project-prediction.html#introduction",
    "title": "Prediction project",
    "section": "Introduction",
    "text": "Introduction\nThe goal of the prediction project is for you to use regression analysis to construct a linear regression model with good prediction accuracy, demonstrating proficiency in the techniques we have covered in class so far and applying them to a real data set in a meaningful way.\nAll analyses must be done in RMarkdown, and all components of the project must be reproducible (with the exception of the presentation)."
  },
  {
    "objectID": "project-prediction.html#academic-integrity",
    "href": "project-prediction.html#academic-integrity",
    "title": "Prediction project",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBy participating in this project, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "project-prediction.html#data",
    "href": "project-prediction.html#data",
    "title": "Prediction project",
    "section": "Data",
    "text": "Data\nWe will work with the natality data for the U.S. in 2020. The response variable that we are interested in is newborn‚Äôs weight. This variable is important to medical professionals since a newborn with a low birth weight is more likely to require additional care. You have access to a random sample of 1,000 observations to construct your prediction model. I have kept a separate set of random 20,000 observations to evaluate the prediction model that you select.\nHere is a detailed overview of the variable\n\nnewborn_birth_weight: newborn birth weight in grams (response)\nmonth: birth month (1 = January, ‚Ä¶, 12 = December)\nmother_age: age of the mother in years\nprenatal_care_starting_month: month in which prenatal care began; if 0, there was no prenatal care\ndaily_cigarette_prepregnancy: daily number of cigarettes smoked before the pregnancy\ndaily_cigarette_trimester_1: daily number of cigarettes smoked during the 1st trimester of the pregnancy\ndaily_cigarette_trimester_2: daily number of cigarettes smoked during the 2nd trimester of the pregnancy\ndaily_cigarette_trimester_3: daily number of cigarettes smoked during the 3rd trimester of the pregnancy\nmother_height: height of the mother in inches\nmother_bmi: body mass index of the mother\nmother_weight_prepregnancy: weight of the mother before the pregnancy in pounds\nmother_weight_delivery: weight of the mother at delivery in pounds\nmother_diabetes_gestational: whether the mother had diabetes during the pregnancy\nnewborn_sex: sex of the newborn\ngestation_week: number of gestational weeks\nmother_risk_factors: whether the mother had any risk factor (diabetes, hypertension, previous preterm birth, previous cesarean, infertility treatment used, etc)"
  },
  {
    "objectID": "project-prediction.html#submission",
    "href": "project-prediction.html#submission",
    "title": "Prediction project",
    "section": "Submission",
    "text": "Submission\nThe three primary deliverables for this project are:\n\nPrediction model: you need to submit a .RDATA file that contains your prediction model. The prediction model must consist in a model fitted using the lm command in R. You can only use the 1,000 observations provided here to fit your model. The .RDATA file should not contain anything else.\nInformal presentation: you will present your work orally to rest of the class. The presentation should be no longer than 5 minutes (aim for 2-6 slides). It is fine if the presentation is shorter than 5 minutes, but it cannot exceed 5 minutes. The two team members should speak roughly the same amount of time. Each presentation will be followed by a short QA session.\nFinal report: The final report details your work. It needs to be realized using RMarkdown and submitted on Gradescope as a PDF. The RMD file also needs to be sent via email to the instruction team (reproducibility). The page limit is 6 pages (including code chunks, but excluding the appendix). Figures should go in the appendix, along with any work that you wish to include. Grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report."
  },
  {
    "objectID": "project-prediction.html#final-report-content",
    "href": "project-prediction.html#final-report-content",
    "title": "Prediction project",
    "section": "Final report content",
    "text": "Final report content\nThe final report should include the following sections, though you should feel free to include additional sections as necessary.\n\nShort introduction: briefly mention the variables you have considered, those you have engineered, and the approach you have chosen for model selection.\nVariable selection and engineering: list the variables that you have chosen to consider; briefly explain your reasoning; describe new variables that you have engineered; visualize every variable that you create.\nOutliers: if there were outliers, explain how you treated them.\nModel fitting: you need to fit at least 5 models:\n\na simple linear regression model with gestation_week as the predictor\nthe full model with all 15 raw predictors\nat least 3 other models containing new variables that you have created\n\nModel selection: select the model that you will submit using\n\na model selection criterion,\nthe holdout method, and\ncross-validation\n\nDiscussion/conclusion: briefly discuss your result, any limitation to your work, and what predictor(s) you have liked to have in the dataset (e.g.¬†weight of father) to make the prediction model more accurate."
  },
  {
    "objectID": "projects/prediction-data.html",
    "href": "projects/prediction-data.html",
    "title": "Prediction Project - Data Prep",
    "section": "",
    "text": "The data can be found here and the help file here"
  },
  {
    "objectID": "projects/prediction-data.html#raw-data-import",
    "href": "projects/prediction-data.html#raw-data-import",
    "title": "Prediction Project - Data Prep",
    "section": "Raw Data import",
    "text": "Raw Data import\n\nknitr::opts_chunk$set(\n  eval = FALSE\n)\n\n\nset.seed(0)\nlibrary(tidyverse)\n\nd_sub <- read_csv(\n  \"../Nat2020us/Nat2020PublicUS.c20210506.r20210812.txt\",\n  col_names = FALSE, trim_ws = FALSE\n  ) %>%\n  slice_sample(n = 5e4)"
  },
  {
    "objectID": "projects/prediction-data.html#variable-extraction",
    "href": "projects/prediction-data.html#variable-extraction",
    "title": "Prediction Project - Data Prep",
    "section": "Variable extraction",
    "text": "Variable extraction\n\nclean_data <- function(data){\n  \n  data_clean <- data %>%\n    rename(string = X1) %>%\n    mutate(\n      month                            = str_sub(string, 13 , 14 ) %>% as.numeric,\n      mother_age                       = str_sub(string, 75 , 76 ) %>% as.numeric,\n      prenatal_care_starting_month     = str_sub(string, 224, 225) %>% as.numeric,\n      daily_cigarette_prepregnancy     = str_sub(string, 253, 254) %>% as.numeric,\n      daily_cigarette_trimester_1      = str_sub(string, 255, 256) %>% as.numeric,\n      daily_cigarette_trimester_2      = str_sub(string, 257, 258) %>% as.numeric,\n      daily_cigarette_trimester_3      = str_sub(string, 259, 260) %>% as.numeric,\n      mother_height                    = str_sub(string, 280, 281) %>% as.numeric,\n      mother_bmi                       = str_sub(string, 283, 286) %>% as.numeric,\n      mother_weight_prepregnancy       = str_sub(string, 292, 294) %>% as.numeric,\n      mother_weight_delivery           = str_sub(string, 299, 301) %>% as.numeric,\n      mother_diabetes_prepregnancy     = str_sub(string, 313, 313),\n      mother_diabetes_gestational      = str_sub(string, 314, 314),\n      mother_hypertension_prepregnancy = str_sub(string, 315, 315),\n      mother_hypertension_gestational  = str_sub(string, 316, 316),\n      mother_no_risk_factor            = str_sub(string, 337, 337) %>% as.numeric,\n      mother_gonorrhea                 = str_sub(string, 343, 343),\n      mother_syphilis                  = str_sub(string, 344, 344),\n      mother_chlamydia                 = str_sub(string, 345, 345),\n      mother_hepatitis_B               = str_sub(string, 346, 346),\n      mother_hepatitis_C               = str_sub(string, 347, 347),\n      number_newborns                  = str_sub(string, 454, 454) %>% as.numeric,\n      newborn_sex                      = str_sub(string, 475, 475),\n      gestation_week                   = str_sub(string, 490, 491) %>% as.numeric,\n      newborn_birth_weight             = str_sub(string, 504, 507) %>% as.numeric\n      ) %>%\n    filter(\n      gestation_week               <  99  ,\n      number_newborns              == 1   ,\n      mother_no_risk_factor        <= 1   ,\n      mother_weight_delivery       <  999 ,\n      mother_weight_prepregnancy   <  999 ,\n      mother_bmi                   <  99  ,\n      mother_height                <  99  ,\n      daily_cigarette_trimester_3  <  98  ,\n      daily_cigarette_trimester_2  <  98  ,\n      daily_cigarette_trimester_1  <  98  ,\n      daily_cigarette_prepregnancy <  98  ,\n      prenatal_care_starting_month <  99  ,\n      newborn_birth_weight         <  9999,\n      mother_diabetes_prepregnancy == \"N\",\n      mother_hypertension_gestational == \"N\",\n      mother_gonorrhea == \"N\",\n      mother_syphilis == \"N\",\n      mother_chlamydia == \"N\",\n      mother_hepatitis_B == \"N\",\n      mother_hepatitis_C == \"N\"\n      ) %>%\n  mutate(mother_risk_factor = !mother_no_risk_factor) %>%\n    select(\n      - string, - number_newborns,\n      - mother_diabetes_prepregnancy, - mother_hypertension_gestational, - mother_hypertension_prepregnancy, - mother_gonorrhea, - mother_syphilis, - mother_chlamydia, - mother_hepatitis_B, - mother_hepatitis_C, - mother_no_risk_factor\n      ) %>%\n    select(\n      newborn_birth_weight, everything()\n    )\n      \n  return(data_clean)\n  \n}\n\n\nd_sub_clean <- clean_data(d_sub)\n\nd_train <- slice(d_sub_clean, 1     : 1e3)\nd_test  <- slice(d_sub_clean, 1e4+1 : 2e4)\n\nwrite_csv(d_train, \"projects/training_set.csv\")\n#write_csv(d_test , \"projects/test_set.csv\"    )"
  },
  {
    "objectID": "projects/prediction-data.html#checking-subset",
    "href": "projects/prediction-data.html#checking-subset",
    "title": "Prediction Project - Data Prep",
    "section": "Checking subset",
    "text": "Checking subset\n\nd_num <- d_sub_clean %>% select_if(is.numeric)\nfor(i in 1 : ncol(d_num)){\n  hist(d_num[[i]], breaks = 30, xlab = names(d_num)[i], main = i)\n}\n\nd_cha <- d_sub_clean %>% select_if(is.character)\nfor(i in 1 : ncol(d_cha)){\n  print(names(d_cha)[i])\n  print(table(d_cha[[i]]))\n}\n\n\nm_simple <- lm(newborn_birth_weight ~ gestation_week, data = d_train)\nsummary(m_simple)\nm_full <- lm(newborn_birth_weight ~ ., data = d_train)\nsummary(m_full)\ny     <- d_test$newborn_birth_weight\nRMST <- sqrt(mean((y - mean(y))^2))\ny_hat <- predict.lm(m_full, d_test)\nsqrt(mean((y - y_hat  )^2))\ny_hat <- predict.lm(m_simple, d_test)\nsqrt(mean((y - y_hat  )^2))"
  },
  {
    "objectID": "projects/prediction-evaluation.html",
    "href": "projects/prediction-evaluation.html",
    "title": "Prediction Project - Model Evaluation",
    "section": "",
    "text": "knitr::opts_chunk$set(\n  eval = FALSE\n)\n\n\nlibrary(tidyverse)\nlibrary(broom)"
  },
  {
    "objectID": "projects/prediction-evaluation.html#import-models",
    "href": "projects/prediction-evaluation.html#import-models",
    "title": "Prediction Project - Model Evaluation",
    "section": "Import models",
    "text": "Import models\n\npath <- \"../projects/\"\nload(paste0(path, \"Ma-Yu.RDATA\"))\nm_my <- best_prediction_model\nrm(best_prediction_model)\n\nload(paste0(path, \"Me-Ha.RDATA\"))\nm_mh <- wombp12rf\nrm(wombp12rf)\n\nload(paste0(path, \"Ja-Ke.RDATA\"))\nm_jk <- Gestation_week_inv\nrm(Gestation_week_inv)\n\nrbind(glance(m_my), glance(m_mh), glance(m_jk))"
  },
  {
    "objectID": "projects/rmd_tips.html",
    "href": "projects/rmd_tips.html",
    "title": "rmd_demo",
    "section": "",
    "text": "This document contains a few tips for working with RMD files. The source file has been sent by email."
  },
  {
    "objectID": "projects/rmd_tips.html#level-2-title-typically-better-than-level-1",
    "href": "projects/rmd_tips.html#level-2-title-typically-better-than-level-1",
    "title": "rmd_demo",
    "section": "Level-2 title (typically better than level-1)",
    "text": "Level-2 title (typically better than level-1)\nYou always want to start by loading the packages you will need.\n\nlibrary(tidyverse)\nlibrary(broom)\n\nThe next step typically consists in downloading the data\n\nd <- read_csv(\"https://rmorsomme.github.io/website/projects/training_set.csv\")\n\nRows: 1000 Columns: 16\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (2): mother_diabetes_gestational, newborn_sex\ndbl (13): newborn_birth_weight, month, mother_age, prenatal_care_starting_mo...\nlgl  (1): mother_risk_factor\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nand then perhaps fitting a model.\n\nm <- lm(mother_bmi ~ mother_weight_prepregnancy, data = d)\nm\n\n\nCall:\nlm(formula = mother_bmi ~ mother_weight_prepregnancy, data = d)\n\nCoefficients:\n               (Intercept)  mother_weight_prepregnancy  \n                    2.6622                      0.1552  \n\n\nThis is simply an illustration. In the project, you should not use mother_bmi as the response!\nFor the project, simply save the model you have selected in a RDATA file and send it to the instruction team.\n\nsave(m, file = \"my_predictive_model.RDATA\")\nrm(m) # removes the object `m` from the environment\n\nOnce we have everybody‚Äôs model, we will use the following commands to load them into R and test them on new data.\n\nload(\"my_predictive_model.RDATA\")\n\nnew_data <- tibble(mother_weight_prepregnancy = 150) # new data\npredict(m, new_data)\n\n       1 \n25.93563"
  },
  {
    "objectID": "projects/rmd_tips.html#we-can-use-rmd-to-write-mathematical-expressions",
    "href": "projects/rmd_tips.html#we-can-use-rmd-to-write-mathematical-expressions",
    "title": "rmd_demo",
    "section": "We can use RMD to write mathematical expressions",
    "text": "We can use RMD to write mathematical expressions\nTo write a small mathematical expression, simply write it between $ signs.\nFor instance, \\(x = 5 + 9\\).\nTo write longer expression, use two $ signs.\n\\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\n\\]\nTo write a word in a math equation, use \\text{}\n\\[\n\\text{BMI} \\approx \\beta_0 + \\beta_1 \\text{weight} + \\beta_3 X^2\n\\]"
  },
  {
    "objectID": "projects/rmd_tips.html#running-r-code",
    "href": "projects/rmd_tips.html#running-r-code",
    "title": "rmd_demo",
    "section": "Running R code",
    "text": "Running R code\nYou can run R code in R chunks:\n\n5+5\n\n[1] 10\n\n\nYou can also directly run R code in a paragraph as follows: using `¬†r 5+5`. For instance, the \\(R^2\\) value of model m is 0.882."
  },
  {
    "objectID": "projects/rmd_tips.html#learning-more-about-rmd",
    "href": "projects/rmd_tips.html#learning-more-about-rmd",
    "title": "rmd_demo",
    "section": "Learning more about RMD",
    "text": "Learning more about RMD\nCome to OH, ask questions during/after class.\nCheck the RMD cheatsheet or the longer reference guide."
  },
  {
    "objectID": "projects/rmd_tips.html#overall-goal-of-the-prediction-project",
    "href": "projects/rmd_tips.html#overall-goal-of-the-prediction-project",
    "title": "rmd_demo",
    "section": "Overall goal of the prediction project",
    "text": "Overall goal of the prediction project\n\ntwo baseline models (simple and full) + 3 models that you construct\nfeature engineering + model selection\nthe outcome of that procedure should be a model that predicts the response variable reasonably well\nplease refer to the rubric"
  },
  {
    "objectID": "projects/rmd_tips.html#missing-values-in-the-penguin-dataset",
    "href": "projects/rmd_tips.html#missing-values-in-the-penguin-dataset",
    "title": "rmd_demo",
    "section": "Missing values in the penguin dataset?",
    "text": "Missing values in the penguin dataset?\n\nd <- palmerpenguins::penguins\n\nfilter(d,  is.na(body_mass_g)) # keeps the rows with a missing value for body_mass_g\n\n# A tibble: 2 x 8\n  species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  \n  <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n1 Adelie  Torge~             NA            NA               NA          NA <NA> \n2 Gentoo  Biscoe             NA            NA               NA          NA <NA> \n# ... with 1 more variable: year <int>\n\nfilter(d, !is.na(body_mass_g)) # keeps the rows without a missing value for body_mass_g\n\n# A tibble: 342 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           36.7          19.3               193        3450\n 5 Adelie  Torgersen           39.3          20.6               190        3650\n 6 Adelie  Torgersen           38.9          17.8               181        3625\n 7 Adelie  Torgersen           39.2          19.6               195        4675\n 8 Adelie  Torgersen           34.1          18.1               193        3475\n 9 Adelie  Torgersen           42            20.2               190        4250\n10 Adelie  Torgersen           37.8          17.1               186        3300\n# ... with 332 more rows, and 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "projects/rmd_tips.html#conclusion",
    "href": "projects/rmd_tips.html#conclusion",
    "title": "rmd_demo",
    "section": "Conclusion",
    "text": "Conclusion\nThis is the end of the main body"
  },
  {
    "objectID": "projects/rmd_tips.html#appendix",
    "href": "projects/rmd_tips.html#appendix",
    "title": "rmd_demo",
    "section": "Appendix",
    "text": "Appendix\nFigures should go here.\n\nggplot(mpg, aes(cty, hwy)) + geom_point()"
  },
  {
    "objectID": "slides/applications.html#outline",
    "href": "slides/applications.html#outline",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Outline",
    "text": "Outline\n\n7 billion ‚Äì are you typical?\n200 years of history through health and wealth\nBuilding a better NBA team through data science\nWhen data seem to contradict widely held believes\nStatistics in psychiatry and agriculture\nCan you still predict elections?\nInferring ethnicity from X-rays"
  },
  {
    "objectID": "slides/applications.html#billion-are-you-typical",
    "href": "slides/applications.html#billion-are-you-typical",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "7 billion ‚Äì are you typical?",
    "text": "7 billion ‚Äì are you typical?\nThis short video by National Geographic illustrates the various ways in which demographic variables can be summarized."
  },
  {
    "objectID": "slides/applications.html#years-of-history-through-health-and-wealth",
    "href": "slides/applications.html#years-of-history-through-health-and-wealth",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "200 years of history through health and wealth",
    "text": "200 years of history through health and wealth\n\n\nThis short video and this Ted talk show the work of the public health scientist Hans Rosling who uses data visualization with great skills to tell a story.\nCan you identify all 5 variables that were necessary to make the animation in the short video? Visit the Gapminder website to explore other variables."
  },
  {
    "objectID": "slides/applications.html#building-a-better-nba-team-through-data-science",
    "href": "slides/applications.html#building-a-better-nba-team-through-data-science",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Building a better NBA team through data science",
    "text": "Building a better NBA team through data science\n\n\nLearn about Dr.¬†Ivana Seric‚Äôs work for the Philadelphia 76ers in this short video. She has used her mathematical background and passion for basketball to dive in the data and help the NBA team devise better strategies.\nNot interested in basketball, check this article in which the R blogger Bill K shares 10 tips for any fan of sports to become a sports data analyst."
  },
  {
    "objectID": "slides/applications.html#when-data-seem-to-contradict-widely-held-believes",
    "href": "slides/applications.html#when-data-seem-to-contradict-widely-held-believes",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "When data seem to contradict widely held believes",
    "text": "When data seem to contradict widely held believes\nIn this Ted talk, Prof.¬†Steven Levitt argues that the data do not seem to corroborate the idea that car seats are no more effective than seat belts in protecting kids from dying in cars."
  },
  {
    "objectID": "slides/applications.html#statistics-in-psychiatry-and-agriculture",
    "href": "slides/applications.html#statistics-in-psychiatry-and-agriculture",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Statistics in psychiatry and agriculture",
    "text": "Statistics in psychiatry and agriculture\nHear about the work of the statistician Susan Murphy and agricultural ecologist David Lobell, both recipients of the prestigious MacArthur Fellowship.\nBelieve it or not, agriculture is where statistical methods (experiments, hypothesis tests, etc) were first used during the 1920s."
  },
  {
    "objectID": "slides/applications.html#estimating-the-effect-of-commute-time-on-rent-in-new-york",
    "href": "slides/applications.html#estimating-the-effect-of-commute-time-on-rent-in-new-york",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Estimating the effect of commute time on rent in New York",
    "text": "Estimating the effect of commute time on rent in New York\n\n\nIn this article, Carl Bialik looked at more than 100,000 homes present on StreetEasy. He observed that, unsurprisingly, the distance to the nearest metro does impact rent prices, and he even managed to put a number on that effect: $56/minute.\n\n\n\n Source: FiverThirtyEight"
  },
  {
    "objectID": "slides/applications.html#can-you-still-predict-elections",
    "href": "slides/applications.html#can-you-still-predict-elections",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Can you still predict elections?",
    "text": "Can you still predict elections?\n\n\nIn this podcast from Andrew Gelman, Professor of statistics and political science at Columbia University, reflects on the use of statistics in election forecasting shortly before the 2020 US election.\nProf.¬†Gelman is widely decorated and very influential statistician."
  },
  {
    "objectID": "slides/applications.html#inferring-ethnicity-from-x-rays",
    "href": "slides/applications.html#inferring-ethnicity-from-x-rays",
    "title": "Data science in action ‚Äì examples in various fields",
    "section": "Inferring ethnicity from X-rays",
    "text": "Inferring ethnicity from X-rays\nIn this recent article, researchers from Harvard and the MIT that AI programs are able to determine a person‚Äôs self-reported ethnicity from an X-ray image. This is noteworthy since these researchers and medical doctors in general did not know it was even possible to do so from X-rays. Which features of the image are used by the AI algorithms to accomplish this feat is still uknown.\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#setup",
    "href": "slides/hw-1-lab-answers.html#setup",
    "title": "Homework 1 - lab answers",
    "section": "Setup",
    "text": "Setup\nYou can find the lab here.\n\nlibrary(tidyverse)\nlibrary(openintro)"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-1-2-points",
    "href": "slides/hw-1-lab-answers.html#exercise-1-2-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 1 (2 points)",
    "text": "Exercise 1 (2 points)\n\narbuthnot$girls\n\n [1] 4683 4457 4102 4590 4839 4820 4928 4605 4457 4952 4784 5332 5200 4910 4617\n[16] 3997 3919 3395 3536 3181 2746 2722 2840 2908 2959 3179 3349 3382 3289 3013\n[31] 2781 3247 4107 4803 4881 5681 4858 4319 5322 5560 5829 5719 6061 6120 5822\n[46] 5738 5717 5847 6203 6033 6041 6299 6533 6744 7158 7127 7246 7119 7214 7101\n[61] 7167 7302 7392 7316 7483 6647 6713 7229 7767 7626 7452 7061 7514 7656 7683\n[76] 5738 7779 7417 7687 7623 7380 7288"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-2-3-points",
    "href": "slides/hw-1-lab-answers.html#exercise-2-3-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 2 (3 points)",
    "text": "Exercise 2 (3 points)\n\narbuthnot %>%\n  ggplot(aes(x = year, y = girls)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\narbuthnot %>%\n  ggplot() +\n  geom_line(aes(x = year, y = girls))"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-3-4-points",
    "href": "slides/hw-1-lab-answers.html#exercise-3-4-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 3 (4 points)",
    "text": "Exercise 3 (4 points)\n\narbuthnot %>%\n  mutate(prop_boys = boys / (boys + girls)) %>%\n  ggplot() +\n  geom_line(aes(year, prop_boys))"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-4-4-points",
    "href": "slides/hw-1-lab-answers.html#exercise-4-4-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 4 (4 points)",
    "text": "Exercise 4 (4 points)\n\npresent\n\n# A tibble: 63 x 3\n    year    boys   girls\n   <dbl>   <dbl>   <dbl>\n 1  1940 1211684 1148715\n 2  1941 1289734 1223693\n 3  1942 1444365 1364631\n 4  1943 1508959 1427901\n 5  1944 1435301 1359499\n 6  1945 1404587 1330869\n 7  1946 1691220 1597452\n 8  1947 1899876 1800064\n 9  1948 1813852 1721216\n10  1949 1826352 1733177\n# ... with 53 more rows\n\npresent$year\n\n [1] 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954\n[16] 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969\n[31] 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n[46] 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999\n[61] 2000 2001 2002\n\nmin(present$year)\n\n[1] 1940\n\nmax(present$year)\n\n[1] 2002\n\nnrow(present)\n\n[1] 63\n\nncol(present)\n\n[1] 3\n\ndim(present)\n\n[1] 63  3\n\npresent\n\n# A tibble: 63 x 3\n    year    boys   girls\n   <dbl>   <dbl>   <dbl>\n 1  1940 1211684 1148715\n 2  1941 1289734 1223693\n 3  1942 1444365 1364631\n 4  1943 1508959 1427901\n 5  1944 1435301 1359499\n 6  1945 1404587 1330869\n 7  1946 1691220 1597452\n 8  1947 1899876 1800064\n 9  1948 1813852 1721216\n10  1949 1826352 1733177\n# ... with 53 more rows"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-5-2-points",
    "href": "slides/hw-1-lab-answers.html#exercise-5-2-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 5 (2 points)",
    "text": "Exercise 5 (2 points)\n\n# Insert code for Exercise 5 here"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-6-5-points",
    "href": "slides/hw-1-lab-answers.html#exercise-6-5-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 6 (5 points)",
    "text": "Exercise 6 (5 points)\n\npresent %>%\n  mutate(prop_boys = boys / (boys + girls)) %>%\n  ggplot() +\n  geom_line(aes(year, prop_boys))"
  },
  {
    "objectID": "slides/hw-1-lab-answers.html#exercise-7-5-points",
    "href": "slides/hw-1-lab-answers.html#exercise-7-5-points",
    "title": "Homework 1 - lab answers",
    "section": "Exercise 7 (5 points)",
    "text": "Exercise 7 (5 points)\n\npresent %>%\n  mutate(total = boys + girls) %>%\n  arrange(desc(total))\n\n# A tibble: 63 x 4\n    year    boys   girls   total\n   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1961 2186274 2082052 4268326\n 2  1960 2179708 2078142 4257850\n 3  1957 2179960 2074824 4254784\n 4  1959 2173638 2071158 4244796\n 5  1958 2152546 2051266 4203812\n 6  1962 2132466 2034896 4167362\n 7  1956 2133588 2029502 4163090\n 8  1990 2129495 2028717 4158212\n 9  1991 2101518 2009389 4110907\n10  1963 2101632 1996388 4098020\n# ... with 53 more rows\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-instructor",
    "href": "slides/lec-1.html#meet-the-instructor",
    "title": "Welcome to STA 101L!",
    "section": "Meet the instructor",
    "text": "Meet the instructor\n\n\n\n\n\nDr.¬†Raphael Morsomme (he/him/his)\n\n\n\n\nPh.D.¬†candidate at the Department of Statistical Science\nResearch focus: epidemic models and cancer overdiagnosis\nFan of Formula 1, model building and cooking"
  },
  {
    "objectID": "slides/lec-1.html#meet-the-ta",
    "href": "slides/lec-1.html#meet-the-ta",
    "title": "Welcome to STA 101L!",
    "section": "Meet the TA",
    "text": "Meet the TA\n\n\n\n\n\nRohit Roy (he/him/his)\n\n\n\n\nPh.D.¬†student in biochemistry\nWorks at the Al-Hashimi Lab\nLikes traveling and cooking"
  },
  {
    "objectID": "slides/lec-1.html#meet-each-other",
    "href": "slides/lec-1.html#meet-each-other",
    "title": "Welcome to STA 101L!",
    "section": "Meet each other",
    "text": "Meet each other\n\n\n\nGroup activity - meeting each other\n\n\nIn groups of 2, ask your partner the following questions\n\npreferred name and pronouns\nfield of study, major\n(least) favorite thing about Durham\nfavorite food, song or artist\n\nYou will introduce your partner to the class.\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-1.html#what-is-data-analysis",
    "href": "slides/lec-1.html#what-is-data-analysis",
    "title": "Welcome to STA 101L!",
    "section": "What is data analysis?",
    "text": "What is data analysis?\n\n\n‚ÄúData analysis is a process of inspecting, cleaning, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. (‚Ä¶) In today‚Äôs business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.‚Äù\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lec-1.html#statistics",
    "href": "slides/lec-1.html#statistics",
    "title": "Welcome to STA 101L!",
    "section": "Statistics",
    "text": "Statistics\n\n‚ÄúEvery field worth studying connects to one‚Äôs everyday life. Statistics is not an isolated field, it connects to everything.‚Äù ‚Äî Andrew Gelman\n\n\nStatistical inference: set of procedures for making rigorous claims about a population from a (small) sample in the presence of uncertainty."
  },
  {
    "objectID": "slides/lec-1.html#course-faq",
    "href": "slides/lec-1.html#course-faq",
    "title": "Welcome to STA 101L!",
    "section": "Course FAQ",
    "text": "Course FAQ\n\n\nWhat background is assumed for the course? There is no prerequisite.\nWill we be doing coding? Yes. We will use R.\nWhat if I have never coded? We do not expect students to have any experience with R or any other programming language.\nWill we learn the mathematical theory of statistics? Yes and no. The course is primarily focused on application; however, we will discuss some of the mathematics of hypothesis testing and simple linear regression."
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to STA 101L!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nBecome a critical consumer of statistical analyses.\nSummarize and visualize categorical and continuous data.\nBuild and investigate linear regression models for forecasting.\nConduct hypothesis tests and construct confidence intervals for proportions, means, and regression coefficients\nPlan and complete a statistical analysis of a real-world phenomenon."
  },
  {
    "objectID": "slides/lec-1.html#examples-of-data-analyses-in-practice",
    "href": "slides/lec-1.html#examples-of-data-analyses-in-practice",
    "title": "Welcome to STA 101L!",
    "section": "Examples of data analyses in practice",
    "text": "Examples of data analyses in practice\n\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight‚Äôs 2020 Presidential Forecast Works ‚Äî And What‚Äôs Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it‚Äôs so freaking hard to make a good COVID-19 model"
  },
  {
    "objectID": "slides/lec-1.html#homepage",
    "href": "slides/lec-1.html#homepage",
    "title": "Welcome to STA 101L!",
    "section": "Homepage",
    "text": "Homepage\nhttps://rmorsomme.github.io/website/\n\nAll course materials\nLinks to Sakai, Gradescope, the textbook, etc.\nLet‚Äôs take a tour!"
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to STA 101L!",
    "section": "Course toolkit",
    "text": "Course toolkit\nAll linked from the course website:\n\nAssignment submission and feedback: Gradescope\nComputing software: R and RStudio\nIntroduction to Modern Statistics (IMS): Openintro\n\n\n\n\n\n\n\nImportant\n\n\nInstall and download R and RStudio before Friday‚Äôs lab!"
  },
  {
    "objectID": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "href": "slides/lec-1.html#activities-prepare-participate-practice-perform",
    "title": "Welcome to STA 101L!",
    "section": "Activities: Prepare, Participate, Practice, Perform",
    "text": "Activities: Prepare, Participate, Practice, Perform\n\nPrepare: Prepare for lectures by completing the readings.\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Solve plenty of exercises; this is the best way to learn statistics.\nPerform: Put together what you‚Äôve learned to analyze real-world data\n\nFrequent homework assignments\nPrediction group project\nInference group project\n\n\n\n\n\n\n\n\nTip for practicing\n\n\nIn the IMS book, the solutions of the odd-numbered exercises are provided in the appendix."
  },
  {
    "objectID": "slides/lec-1.html#cadence---homework",
    "href": "slides/lec-1.html#cadence---homework",
    "title": "Welcome to STA 101L!",
    "section": "Cadence - homework",
    "text": "Cadence - homework\n\ntwo sets of homework per week\ncombination of problems from IMS and lab exercises\ndue dates TBD\ntyped up using RMarkdown and submitted as a PDF on Gradescope\nlowest grade dropped\nyou may discuss homework with other students; however, your answers should be completed and submitted individually"
  },
  {
    "objectID": "slides/lec-1.html#cadence---two-group-projects",
    "href": "slides/lec-1.html#cadence---two-group-projects",
    "title": "Welcome to STA 101L!",
    "section": "Cadence - two group projects",
    "text": "Cadence - two group projects\n\nPrediction project:\n\ngroups of 2\ngoal: build a model that makes the most accurate predictions possible.\nsome lab and lecture time dedicated to working on it\n2-page write up and informal presentation (2 slides)\n\n\n\n\nInference project:\n\ngroups of 2\ngoal: analyze a phenomenon of your choice using real-world data\napply all the techniques learned in class (visualization, hypothesis test, confidence interval and linear regression)\nsome lab and lecture time dedicated to working on it\n5-page report and formal presentation (+-10 slides)"
  },
  {
    "objectID": "slides/lec-1.html#teams",
    "href": "slides/lec-1.html#teams",
    "title": "Welcome to STA 101L!",
    "section": "Teams",
    "text": "Teams\n\nTeam assignments\n\nassigned by me\nin-class exercises, and the two project\n\nExpectations and roles\n\neveryone is expected to contribute equal effort\neveryone is expected to understand all code turned in"
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to STA 101L!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework and Labs\n50%\n\n\nPrediction Project\n20%\n\n\nInference Project\n30%\n\n\n\nSee course syllabus for how the final letter grade will be determined."
  },
  {
    "objectID": "slides/lec-1.html#support",
    "href": "slides/lec-1.html#support",
    "title": "Welcome to STA 101L!",
    "section": "Support",
    "text": "Support\n\nAsk questions in class and labs quickly.\nAttend office hours.\nUse the course forum Conversations on Sakai.\nEmails should be reserved for questions not appropriate for the public forum..\nRead the course support page for more information."
  },
  {
    "objectID": "slides/lec-1.html#announcements",
    "href": "slides/lec-1.html#announcements",
    "title": "Welcome to STA 101L!",
    "section": "Announcements",
    "text": "Announcements\n\nSent via email, be sure to check your mail box regularly\nI‚Äôll assume that you‚Äôve read an announcement by the next ‚Äúbusiness‚Äù day"
  },
  {
    "objectID": "slides/lec-1.html#diversity-inclusion",
    "href": "slides/lec-1.html#diversity-inclusion",
    "title": "Welcome to STA 101L!",
    "section": "Diversity + inclusion",
    "text": "Diversity + inclusion\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\n\nIf you have a name that differs from those that appear in your official Duke records, please let us know!\nPlease let us know your preferred pronouns; correct us if necessary.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. I want to be a resource for you. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/lec-1.html#accessibility",
    "href": "slides/lec-1.html#accessibility",
    "title": "Welcome to STA 101L!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nI am committed to making all course materials accessible and I‚Äôm always learning how to do this better. If any course component is not accessible to you in any way, please don‚Äôt hesitate to let me know."
  },
  {
    "objectID": "slides/lec-1.html#covid-policies",
    "href": "slides/lec-1.html#covid-policies",
    "title": "Welcome to STA 101L!",
    "section": "COVID policies",
    "text": "COVID policies\n\nWear a mask at all times!\nRead and follow university guidance here."
  },
  {
    "objectID": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "href": "slides/lec-1.html#late-work-waivers-regrades-policy",
    "title": "Welcome to STA 101L!",
    "section": "Late work, waivers, regrades policy",
    "text": "Late work, waivers, regrades policy\n\nWe have policies!\nto ensure timely feedback, assignments submitted after the deadline will not be graded\none waiver available for a 24-hour extension\nregrade requests within 48 hours\nRead the course syllabus for more details"
  },
  {
    "objectID": "slides/lec-1.html#collaboration-policy",
    "href": "slides/lec-1.html#collaboration-policy",
    "title": "Welcome to STA 101L!",
    "section": "Collaboration policy",
    "text": "Collaboration policy\n\nOnly work that is clearly assigned as team work should be completed collaboratively.\nHomeworks must be completed individually. You may not directly share answers / code with others, however you are welcome to discuss the problems in general and ask for advice."
  },
  {
    "objectID": "slides/lec-1.html#sharing-reusing-code-policy",
    "href": "slides/lec-1.html#sharing-reusing-code-policy",
    "title": "Welcome to STA 101L!",
    "section": "Sharing / reusing code policy",
    "text": "Sharing / reusing code policy\n\nWe are aware that a huge volume of code is available on the web, and many tasks may have solutions posted\nUnless explicitly stated otherwise, this course‚Äôs policy is that you may use any online resources (e.g.¬†RStudio Community, StackOverflow, etc.) but you must explicitly cite where you obtained any code you directly use or use as inspiration in your solution(s).\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source"
  },
  {
    "objectID": "slides/lec-1.html#academic-integrity",
    "href": "slides/lec-1.html#academic-integrity",
    "title": "Welcome to STA 101L!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "slides/lec-1.html#most-importantly",
    "href": "slides/lec-1.html#most-importantly",
    "title": "Welcome to STA 101L!",
    "section": "Most importantly!",
    "text": "Most importantly!\nAsk if you‚Äôre not sure if something violates a policy!"
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to STA 101L!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\ncomplete the reading before the lectures;\nask questions quickly; don‚Äôt let a day pass by with lingering questions;\ndo the homework assignments thoroughly;\npractice, practice, practice;\ndon‚Äôt procrastinate; start on the homework assignments and the projects early."
  },
  {
    "objectID": "slides/lec-1.html#section",
    "href": "slides/lec-1.html#section",
    "title": "Welcome to STA 101L!",
    "section": "",
    "text": "Tip for practicing\n\n\nRemember the odd-number exercises in IMS! The teaching team will always be happy to provide feedback on your work."
  },
  {
    "objectID": "slides/lec-1.html#learning-during-a-pandemic-learning-during-the-summer",
    "href": "slides/lec-1.html#learning-during-a-pandemic-learning-during-the-summer",
    "title": "Welcome to STA 101L!",
    "section": "Learning during a pandemic, learning during the summer",
    "text": "Learning during a pandemic, learning during the summer\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don‚Äôt hesitate to ask.\n\n\nYou never owe me personal information about your health (mental or physical) but you‚Äôre always welcome to talk to me. If I can‚Äôt help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this summer session"
  },
  {
    "objectID": "slides/lec-1.html#this-weeks-tasks",
    "href": "slides/lec-1.html#this-weeks-tasks",
    "title": "Welcome to STA 101L!",
    "section": "This week‚Äôs tasks",
    "text": "This week‚Äôs tasks\n\nDownload and install R and RStudio before Friday‚Äôs lab.\nRead the syllabus.\nComplete the assigned reading before tomorrow.\nWatch out for announcement emails."
  },
  {
    "objectID": "slides/lec-1.html#important-dates",
    "href": "slides/lec-1.html#important-dates",
    "title": "Welcome to STA 101L!",
    "section": "Important dates",
    "text": "Important dates\n\nMay 11: Classes begin (Monday meeting schedule)\nMay 13: Drop/add ends\nMay 16: Regular class meeting schedule begins\nMay 30: Memorial Day holiday, no class is held\nMay 31: Prediction project\nJune 8: Last day to withdraw with W\nJune 16: Inference project presentation\nJune 17: Classes end\nJune 20: Juneteenth holiday\nJune 21: Reading period\nJune 23: Last assignment: inference project report\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "slides/lec-1.html#kai-says",
    "href": "slides/lec-1.html#kai-says",
    "title": "Welcome to STA 101L!",
    "section": "Kai says‚Ä¶",
    "text": "Kai says‚Ä¶\n\n\n\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-10.html#announcements",
    "href": "slides/lec-10.html#announcements",
    "title": "Inference for means",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 6 due Thursday\nInference project\n\nStart to think about data or a question that interests you.\n\nRemaining homework due on Mon/Thu instead of Wed/Sun?"
  },
  {
    "objectID": "slides/lec-10.html#announcements-1",
    "href": "slides/lec-10.html#announcements-1",
    "title": "Inference for means",
    "section": "Announcements",
    "text": "Announcements\nMidterm survey\n\nreview holdout and CV\nmore live coding (?)\nmore individual assignments in class\nOH before Sunday‚Äôs HW \\(\\Rightarrow\\) HW due on Monday (OH Monday morning)\nwebsite\n\nexternal links will open a new window\nslide changes will not be pushed to browser history\n\nsee these steps to print the slides"
  },
  {
    "objectID": "slides/lec-10.html#recap",
    "href": "slides/lec-10.html#recap",
    "title": "Inference for means",
    "section": "Recap",
    "text": "Recap\n\nOne proportion\n\nHT via simulation\nCI via bootstrap\n\nTwo proportions\n\nHT via simulation\nCI via bootstrap"
  },
  {
    "objectID": "slides/lec-10.html#outline",
    "href": "slides/lec-10.html#outline",
    "title": "Inference for means",
    "section": "Outline",
    "text": "Outline\n\nThe normal distribution\nOne mean (case 3)\nTwo means (case 4)\nPaired means (case 3)"
  },
  {
    "objectID": "slides/lec-10.html#the-normal-distribution-1",
    "href": "slides/lec-10.html#the-normal-distribution-1",
    "title": "Inference for means",
    "section": "The normal distribution",
    "text": "The normal distribution\nThe distribution of a numerical variable is often modeled with a normal distribution.\n\ntibble(x = seq(-5, 5, by = 0.01)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))"
  },
  {
    "objectID": "slides/lec-10.html#two-parameters",
    "href": "slides/lec-10.html#two-parameters",
    "title": "Inference for means",
    "section": "Two parameters",
    "text": "Two parameters\nThe mean (\\(\\mu\\)) ‚Äì location\nThe standard deviation (\\(\\sigma\\)) ‚Äì spread\n\\[\nN(\\mu, \\sigma)\n\\] The standard normal distribution \\(N(\\mu = 0, \\sigma = 1)\\) is plotted on the previous slide."
  },
  {
    "objectID": "slides/lec-10.html#section",
    "href": "slides/lec-10.html#section",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - normal distribution\n\n\nModify the following R code to plot the two normal distributions\n\n\\(N(\\mu = 0, \\sigma = 0.5)\\)\n\\(N(\\mu = 1, \\sigma = 0.5)\\)\n\n\ntibble(x = seq(-5, 5, by = 0.01)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-10.html#property-i",
    "href": "slides/lec-10.html#property-i",
    "title": "Inference for means",
    "section": "Property I",
    "text": "Property I\nIf the variable \\(X\\sim N(\\mu, \\sigma)\\), then for any number \\(a\\) and \\(b\\)\n\\[\nX + a \\sim N(\\mu+a, \\sigma),\n\\]\n\\[\nbX \\sim N(b\\mu, b \\sigma)\n\\]\nand\n\\[\nbX + a \\sim N(b\\mu+a, b\\sigma).\n\\]"
  },
  {
    "objectID": "slides/lec-10.html#property-ii",
    "href": "slides/lec-10.html#property-ii",
    "title": "Inference for means",
    "section": "Property II",
    "text": "Property II\nIf the variables \\(X\\sim N(\\mu_1, \\sigma_1)\\) and \\(Y\\sim N(\\mu_2, \\sigma_2)\\) are independent, then\n\\[\nX+Y \\sim N(\\mu_{tot} = \\mu_1 + \\mu_2, \\sigma_{tot} = \\sqrt{\\sigma_1^2 + \\sigma_2^2})\n\\]"
  },
  {
    "objectID": "slides/lec-10.html#alternative-parameterization",
    "href": "slides/lec-10.html#alternative-parameterization",
    "title": "Inference for means",
    "section": "Alternative parameterization",
    "text": "Alternative parameterization\n\\(N(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is the variance.\n\nProperty I gives\n\\[\nbX + a \\sim N(b\\mu + a, b^2 \\sigma^2)\n\\]\nand property II gives\n\\[\nX+Y \\sim N(\\mu_{tot} = \\mu_1 + \\mu_2, \\sigma_{tot}^2 = \\sigma_1^2 + \\sigma_2^2)\n\\]\n\n\nThe mean of the sum is the sum of the means.\nThe variance of the sum is the sum of the variances."
  },
  {
    "objectID": "slides/lec-10.html#section-1",
    "href": "slides/lec-10.html#section-1",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - normal property\n\n\nSuppose you have a sample with \\(n=10\\) independent observations in which each observation follows a standard normal distribution. That is,\n\\[\nX_1 \\sim N(0,1), X_2 \\sim N(0,1), \\dots, X_{10} \\sim N(0, 1).\n\\]\nUse the normal properties to derive the distribution of the sample average\n\\[\n\\bar{x} = \\dfrac{X_1 + X_2 + \\dots + X_{10}}{10}\n\\]\nWhat happens to the sd/variance of \\(\\bar{x}\\) as the sample size \\(n\\) increases?\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-10.html#section-2",
    "href": "slides/lec-10.html#section-2",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise -\n\n\nExercise 19.5 ‚Äì for our purpose in part b standard error is equivalent to standard deviation.\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-10.html#the-68-95-99.7-rule",
    "href": "slides/lec-10.html#the-68-95-99.7-rule",
    "title": "Inference for means",
    "section": "The 68, 95, 99.7 rule",
    "text": "The 68, 95, 99.7 rule"
  },
  {
    "objectID": "slides/lec-10.html#setup",
    "href": "slides/lec-10.html#setup",
    "title": "Inference for means",
    "section": "Setup",
    "text": "Setup\nPopulation parameter: mean \\(\\mu\\)\nSample statistic: sample average \\(\\bar{x}\\)\n\nHypothesis testing:\n\n\\(H_0:\\mu=\\mu_0\\) where \\(\\mu_0\\) is a fixed number\n\\(H_a:\\mu\\neq \\mu_0\\)\n\nConfidence interval: range of plausible values for \\(\\mu\\)."
  },
  {
    "objectID": "slides/lec-10.html#section-3",
    "href": "slides/lec-10.html#section-3",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - statistic and parameter\n\n\nExercise 19.1\n\n\n\n\n\n\n01:00"
  },
  {
    "objectID": "slides/lec-10.html#example-car-price",
    "href": "slides/lec-10.html#example-car-price",
    "title": "Inference for means",
    "section": "Example ‚Äì car price",
    "text": "Example ‚Äì car price\nWhat is the average price of a car on Awesome Car?\n\\[\\begin{align*}\n\\bar{x}\n& = \\dfrac{18,300+20,100+9,600+10,700+27,000}{5} \\\\\n& = 17,140\n\\end{align*}\\]\n\nSource: IMS"
  },
  {
    "objectID": "slides/lec-10.html#normal-model",
    "href": "slides/lec-10.html#normal-model",
    "title": "Inference for means",
    "section": "Normal model",
    "text": "Normal model\nLet us assume that car prices follow a normal distribution with mean \\(\\mu\\) and sd \\(\\sigma\\):\n\\[\n\\text{price} \\sim N(\\mu, \\sigma)\n\\]\n\\(H_0:\\mu=10,000\\)\n\\(H_a:\\mu\\neq 10,000\\)\n\n\nSimulate many samples under \\(H_0\\).\nDetermine if the observed data could have plausibly arisen under \\(H_0\\)."
  },
  {
    "objectID": "slides/lec-10.html#problem",
    "href": "slides/lec-10.html#problem",
    "title": "Inference for means",
    "section": "Problem",
    "text": "Problem\nTo simulate from the normal distribution, we need to specify both the mean \\(\\mu\\) and the sd \\(\\sigma\\).\nProblem: under \\(H_0\\) we only know that \\(\\mu = 10,000\\); we do not know what value to use for \\(\\sigma\\)!\n\n\\(\\Rightarrow\\) We cannot conduct a hypothesis test via simulation!"
  },
  {
    "objectID": "slides/lec-10.html#bootstrapping",
    "href": "slides/lec-10.html#bootstrapping",
    "title": "Inference for means",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nSample with repetition from the observed sample to construct many bootstrap samples.\nBootstrap samples \\(\\Rightarrow\\) sampling distribution \\(\\Rightarrow\\) CI"
  },
  {
    "objectID": "slides/lec-10.html#section-4",
    "href": "slides/lec-10.html#section-4",
    "title": "Inference for means",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-10.html#bootstrap-in-r",
    "href": "slides/lec-10.html#bootstrap-in-r",
    "title": "Inference for means",
    "section": "Bootstrap in R",
    "text": "Bootstrap in R\n\nsample_observed <- tibble(price = c(18300, 20100, 9600, 10700, 27000))\n\n\nsample_bootstrap <- function(data){ # same function as before\n  n                <- nrow(data)\n  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)\n  return(sample_bootstrap)\n}\n\n\nset.seed(1)\nsample_bootstrap(sample_observed)\n\n# A tibble: 5 x 1\n  price\n  <dbl>\n1 18300\n2 10700\n3 18300\n4 20100\n5 27000"
  },
  {
    "objectID": "slides/lec-10.html#section-5",
    "href": "slides/lec-10.html#section-5",
    "title": "Inference for means",
    "section": "",
    "text": "results <- tibble(stat_boot = numeric())\nset.seed(0)\nfor(i in 1 : 1e3){\n  d_boot    <- sample_bootstrap(sample_observed) # bootstrap sample\n  stat_boot <- mean(d_boot$price)                # bootstrap statistic\n  results   <- results %>% add_row(stat_boot) \n}\n\n\nquantile(results$stat_boot, c(0.025, 0.975)) # 95% CI\n\n 2.5% 97.5% \n11780 23520 \n\n\n\nquantile(results$stat_boot, c(0.01 , 0.99 )) # 98% CI (wider)\n\n     1%     99% \n11557.8 24240.0"
  },
  {
    "objectID": "slides/lec-10.html#section-6",
    "href": "slides/lec-10.html#section-6",
    "title": "Inference for means",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(stat_boot), binwidth = 1350) + \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.025, 0.975)), col = \"gold1\", size = 2) + # 95% CI \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.01 , 0.99)), col = \"maroon\", size = 2) # 98% CI"
  },
  {
    "objectID": "slides/lec-10.html#section-7",
    "href": "slides/lec-10.html#section-7",
    "title": "Inference for means",
    "section": "",
    "text": "Group exercise - Bootstrap CI\n\n\nExercises 19.11 and 19.13\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-10.html#ci-for-the-standard-deviation-sigma",
    "href": "slides/lec-10.html#ci-for-the-standard-deviation-sigma",
    "title": "Inference for means",
    "section": "CI for the standard deviation \\(\\sigma\\)",
    "text": "CI for the standard deviation \\(\\sigma\\)\n\nresults <- tibble(stat_boot = numeric())\nset.seed(0)\nfor(i in 1 : 1e3){\n  d_boot    <- sample_bootstrap(sample_observed)\n  stat_boot <- sd(d_boot$price  )              # sd instead of mean\n  results   <- results %>% add_row(stat_boot) \n}"
  },
  {
    "objectID": "slides/lec-10.html#section-8",
    "href": "slides/lec-10.html#section-8",
    "title": "Inference for means",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(stat_boot)) + \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.05 , 0.95 )), col = \"gold1\", size = 2) + # 90% CI \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.01, 0.99)), col = \"maroon\", size = 2) # 98% CI"
  },
  {
    "objectID": "slides/lec-10.html#any-statistic-but-not-any-sample",
    "href": "slides/lec-10.html#any-statistic-but-not-any-sample",
    "title": "Inference for means",
    "section": "Any statistic, but not any sample",
    "text": "Any statistic, but not any sample\n\n\n\n\n\n\nBootstrap any statistic\n\n\nYou can bootstrap almost any statistic you want!\n\n\n\n\n\n\n\n\n\nThe quality of the sample matters!\n\n\nA statistical analysis can only be as good as the sample collected. Here, a sample of \\(5\\) cars contains very limited information about the population; it would be useful to have a larger sample."
  },
  {
    "objectID": "slides/lec-10.html#setup-1",
    "href": "slides/lec-10.html#setup-1",
    "title": "Inference for means",
    "section": "Setup",
    "text": "Setup\nA population divided in two groups.\nPopulation parameter: difference in mean\n\\[\n\\mu_{diff}=\\mu_1-\\mu_2\n\\]\nSample statistic: difference in proportion in the sample\n\\[\n\\bar{x}_{diff}=\\bar{x}_1-\\bar{x}_2\n\\]\n\n\\(H_0:\\mu_{diff}=0\\) (no difference between the two groups)\n\\(H_a:\\mu_{diff}\\neq0\\)"
  },
  {
    "objectID": "slides/lec-10.html#section-9",
    "href": "slides/lec-10.html#section-9",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise -\n\n\nExercise 20.1\n\n\n\n\n\n\n01:30"
  },
  {
    "objectID": "slides/lec-10.html#example-two-class-exams",
    "href": "slides/lec-10.html#example-two-class-exams",
    "title": "Inference for means",
    "section": "Example ‚Äì two class exams",
    "text": "Example ‚Äì two class exams\nA professor considers two designs for an exam. Are the two types of exam equally difficult?\n\nd <- openintro::classdata %>% \n  filter(lecture %in% c(\"a\", \"b\")) %>%\n  rename(score = m1, exam = lecture)\nd\n\n# A tibble: 113 x 2\n   score exam \n   <dbl> <fct>\n 1    67 a    \n 2    59 a    \n 3   100 a    \n 4    81 a    \n 5    80 a    \n 6    63 a    \n 7   100 a    \n 8    44 a    \n 9    82 a    \n10    67 a    \n# ... with 103 more rows"
  },
  {
    "objectID": "slides/lec-10.html#section-10",
    "href": "slides/lec-10.html#section-10",
    "title": "Inference for means",
    "section": "",
    "text": "ggplot(d, aes(score, exam, col = exam)) + \n  geom_boxplot() +\n  geom_jitter(width = 0, height = 0.1) # add vertical jitter"
  },
  {
    "objectID": "slides/lec-10.html#section-11",
    "href": "slides/lec-10.html#section-11",
    "title": "Inference for means",
    "section": "",
    "text": "Group exercise - two proportions\n\n\nCompute are \\(\\bar{x}_a\\), \\(\\bar{x}_b\\) and \\(\\bar{x}_{diff}\\)? Do you intuitively feel that the data provide convincing evidence that the two exams are not equally difficult?\nHint: use the command summarize.\n\nd <- openintro::classdata %>% \n  filter(lecture %in% c(\"a\", \"b\")) %>%\n  rename(score = m1, exam = lecture)\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-10.html#section-12",
    "href": "slides/lec-10.html#section-12",
    "title": "Inference for means",
    "section": "",
    "text": "\\(H_0:\\mu_{diff}=0\\)\n\\(H_a:\\mu_{diff}\\neq 0\\)\n\n\nSimulate many samples under \\(H_0\\) (no difference)\nDetermine if the observed data could have plausibly arisen under \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-10.html#simulating-under-h_0",
    "href": "slides/lec-10.html#simulating-under-h_0",
    "title": "Inference for means",
    "section": "Simulating under \\(H_0\\)",
    "text": "Simulating under \\(H_0\\)\nUnder \\(H_0\\), there is no difference between the two exams\n\\(\\Rightarrow\\) the score is independent of the type of exam\n\\(\\Rightarrow\\) randomly re-assign the scores independently of the exam type.\n\n\n\n\n\n\n\nTip\n\n\nThis is very similar to the procedure for two proportions."
  },
  {
    "objectID": "slides/lec-10.html#section-13",
    "href": "slides/lec-10.html#section-13",
    "title": "Inference for means",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-10.html#section-14",
    "href": "slides/lec-10.html#section-14",
    "title": "Inference for means",
    "section": "",
    "text": "d_sim <- d %>% mutate(score = sample(score)) # shuffle the scores\nd_sim\n\n# A tibble: 113 x 2\n   score exam \n   <dbl> <fct>\n 1    67 a    \n 2    93 a    \n 3    82 a    \n 4    82 a    \n 5    70 a    \n 6    82 a    \n 7    66 a    \n 8    84 a    \n 9    50 a    \n10    64 a    \n# ... with 103 more rows"
  },
  {
    "objectID": "slides/lec-10.html#function-for-computing-the-test-statistic",
    "href": "slides/lec-10.html#function-for-computing-the-test-statistic",
    "title": "Inference for means",
    "section": "Function for computing the test statistic",
    "text": "Function for computing the test statistic\n\ncompute_x_diff <- function(data){\n  x_bar <- data %>%\n    group_by(exam) %>%\n    summarize(x_bar = mean(score))\n  x_diff_bar <- x_bar$x_bar[1] - x_bar$x_bar[2]\n  return(x_diff_bar)\n}\ncompute_x_diff(d_sim)\n\n[1] -0.437931"
  },
  {
    "objectID": "slides/lec-10.html#for-loop-for-simulating-under-h_0",
    "href": "slides/lec-10.html#for-loop-for-simulating-under-h_0",
    "title": "Inference for means",
    "section": "For-loop for simulating under \\(H_0\\)",
    "text": "For-loop for simulating under \\(H_0\\)\n\n# Setup\nresults   <- tibble(x_diff_bar = numeric())\n\n# Simulations\nset.seed(0)\nfor(i in 1 : 1e3){\n  d_sim <- d %>% mutate(score = sample(score)) # simulate under H0\n  x_diff_bar <- compute_x_diff(d_sim) # test statistic\n  results <- results %>% add_row(x_diff_bar)\n}"
  },
  {
    "objectID": "slides/lec-10.html#sampling-distribution",
    "href": "slides/lec-10.html#sampling-distribution",
    "title": "Inference for means",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nx_diff_obs <- compute_x_diff(d)\nx_diff_obs\n\n[1] 3.139812\n\nggplot(results) + \n  geom_histogram(aes(x_diff_bar)) +\n  geom_vline(xintercept = x_diff_obs, col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-10.html#p-value",
    "href": "slides/lec-10.html#p-value",
    "title": "Inference for means",
    "section": "p-value",
    "text": "p-value\n\nthe probability that \\(\\bar{x}_{diff}^{sim}\\ge\\) 3.1 or \\(\\bar{x}_{diff}^{sim}\\le\\) -3.1.\n\n\nresults %>%\n  mutate(is_more_extreme = x_diff_bar >= x_diff_obs | x_diff_bar <= -x_diff_obs) %>%\n  summarize(p_value = mean(is_more_extreme))\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1   0.227"
  },
  {
    "objectID": "slides/lec-10.html#conclusion",
    "href": "slides/lec-10.html#conclusion",
    "title": "Inference for means",
    "section": "Conclusion",
    "text": "Conclusion\nUsing the usual significance level \\(\\alpha = 0.05\\), we fail to reject the null hypothesis\n\nit is plausible that the observed difference in scores is due to random luck\nthe difference is not statistically significant."
  },
  {
    "objectID": "slides/lec-10.html#section-15",
    "href": "slides/lec-10.html#section-15",
    "title": "Inference for means",
    "section": "",
    "text": "Group exercise - HT\n\n\nExercises 20.3 and 20.7\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-10.html#bootstrap-ci",
    "href": "slides/lec-10.html#bootstrap-ci",
    "title": "Inference for means",
    "section": "Bootstrap CI",
    "text": "Bootstrap CI\nSame idea as before: sample with repetition from the observed data to construct many bootstrap samples.\nBootstrap samples \\(\\Rightarrow\\) sampling distribution \\(\\Rightarrow\\) CI"
  },
  {
    "objectID": "slides/lec-10.html#section-16",
    "href": "slides/lec-10.html#section-16",
    "title": "Inference for means",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-10.html#bootstrap-in-r-1",
    "href": "slides/lec-10.html#bootstrap-in-r-1",
    "title": "Inference for means",
    "section": "Bootstrap in R",
    "text": "Bootstrap in R\n\nsample_observed_a <- d %>% filter(exam == \"a\")\nsample_observed_b <- d %>% filter(exam == \"b\")\n\nset.seed(0)\nsample_bootstrap(sample_observed_a) # bootstrap sample\n\n# A tibble: 58 x 2\n   score exam \n   <dbl> <fct>\n 1    58 a    \n 2    59 a    \n 3    81 a    \n 4    71 a    \n 5    67 a    \n 6    58 a    \n 7    80 a    \n 8    72 a    \n 9    58 a    \n10    72 a    \n# ... with 48 more rows\n\nsample_bootstrap(sample_observed_b) # bootstrap sample\n\n# A tibble: 55 x 2\n   score exam \n   <dbl> <fct>\n 1    61 b    \n 2    83 b    \n 3    68 b    \n 4    71 b    \n 5    82 b    \n 6    46 b    \n 7    78 b    \n 8    72 b    \n 9    64 b    \n10    83 b    \n# ... with 45 more rows"
  },
  {
    "objectID": "slides/lec-10.html#section-17",
    "href": "slides/lec-10.html#section-17",
    "title": "Inference for means",
    "section": "",
    "text": "results <- tibble(x_diff_bar = numeric())\nfor(i in 1 : 1e3){\n  d_boot_a   <- sample_bootstrap(sample_observed_a) # bootstrap sample\n  d_boot_b   <- sample_bootstrap(sample_observed_b) # bootstrap sample\n  x_diff_bar <- compute_x_diff(rbind(d_boot_a, d_boot_b)) # bootstrap statistic\n  results    <- results %>% add_row(x_diff_bar)\n}\n\n\nquantile(results$x_diff_bar, c(0.05 , 0.95 )) %>% signif(2) # 90% CI\n\n  5%  95% \n-1.2  7.4 \n\nquantile(results$x_diff_bar, c(0.025, 0.975)) %>% signif(2) # 95% CI\n\n 2.5% 97.5% \n -2.0   8.3"
  },
  {
    "objectID": "slides/lec-10.html#section-18",
    "href": "slides/lec-10.html#section-18",
    "title": "Inference for means",
    "section": "",
    "text": "ggplot(results) +\n  geom_histogram(aes(x_diff_bar))  + \n  geom_vline(xintercept = quantile(results$x_diff_bar, c(0.05 , 0.95 )), col = \"gold1\", size = 2) + # 90% CI \n  geom_vline(xintercept = quantile(results$x_diff_bar, c(0.025, 0.975)), col = \"maroon\", size = 2) # 95% CI"
  },
  {
    "objectID": "slides/lec-10.html#ci-and-ht",
    "href": "slides/lec-10.html#ci-and-ht",
    "title": "Inference for means",
    "section": "CI and HT",
    "text": "CI and HT\n\n\n\n\n\n\nTwo sides of the same coin\n\n\nThe two CIs include 0. This indicates that 0 is a plausible value for the difference in mean in the population. This is exactly what the HT concluded!"
  },
  {
    "objectID": "slides/lec-10.html#section-19",
    "href": "slides/lec-10.html#section-19",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercises - CI\n\n\nExercise 20.5 part a only\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-10.html#paired-data",
    "href": "slides/lec-10.html#paired-data",
    "title": "Inference for means",
    "section": "Paired data",
    "text": "Paired data\nPaired data: two groups in which each observation in one group has exactly one corresponding observation in the other group.\nExample: pre/post-evaluations; supermarket items; batteries and electronic devices; tires and cars.\n\n\n\n\n\n\n\nPaired data are like one mean data\n\n\nPaired data can be analyzed like the one-mean case (case 3)!"
  },
  {
    "objectID": "slides/lec-10.html#example-tire-brand",
    "href": "slides/lec-10.html#example-tire-brand",
    "title": "Inference for means",
    "section": "Example ‚Äì tire brand",
    "text": "Example ‚Äì tire brand\nWe want to compare the longevity of two brands of tire. The response variable is tire tread after 1000 miles.\n25 cars drove 1000 miles. On each car, one tire was from Smooth Turn and another one was from Quick Spin. The other two tires were baseline tires."
  },
  {
    "objectID": "slides/lec-10.html#section-20",
    "href": "slides/lec-10.html#section-20",
    "title": "Inference for means",
    "section": "",
    "text": "set.seed(1)\nbias <- runif(25, -0.01, 0.01)\nbrandA <- rnorm(25, bias + 0.310, 0.003)\nbrandB <- rnorm(25, bias + 0.308, 0.003)\ncar <- c(paste(\"car\", 1:25))\nminy <- min(brandA, brandB) - .003\nmaxy <- max(brandA, brandB) + .003\ntires <- tibble(\n  tread = c(brandA, brandB),\n  car = rep(car, 2),\n  brand = c(rep(\"Smooth Turn\", 25), rep(\"Quick Spin\", 25))\n) %>%\n  arrange(car)\nhead(tires)\n\n# A tibble: 6 x 3\n  tread car    brand      \n  <dbl> <chr>  <chr>      \n1 0.304 car 1  Smooth Turn\n2 0.307 car 1  Quick Spin \n3 0.302 car 10 Smooth Turn\n4 0.303 car 10 Quick Spin \n5 0.307 car 11 Smooth Turn\n6 0.305 car 11 Quick Spin"
  },
  {
    "objectID": "slides/lec-10.html#section-21",
    "href": "slides/lec-10.html#section-21",
    "title": "Inference for means",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-10.html#section-22",
    "href": "slides/lec-10.html#section-22",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - paired data\n\n\nDo you intuitively feel that the tire data provide convincing evidence that one tire is more durable than the other?\nExercises 21.1, 21.3 and 21.5\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-10.html#ci-via-booststrap",
    "href": "slides/lec-10.html#ci-via-booststrap",
    "title": "Inference for means",
    "section": "CI via booststrap",
    "text": "CI via booststrap\nExactly the same as with one mean (case 3).\n\n\ntires_diff <- tires %>% \n  pivot_wider(names_from = brand, values_from = tread) %>%\n  mutate(tread_diff = `Smooth Turn` - `Quick Spin`) %>%\n  select(car, tread_diff)\nhead(tires_diff)\n\n# A tibble: 6 x 2\n  car    tread_diff\n  <chr>       <dbl>\n1 car 1    -0.00258\n2 car 10   -0.00107\n3 car 11    0.00192\n4 car 12    0.00251\n5 car 13    0.00271\n6 car 14    0.00639\n\n\n\n\nSimply construct a CI for the variable tread_diff with bootstrap samples."
  },
  {
    "objectID": "slides/lec-10.html#section-23",
    "href": "slides/lec-10.html#section-23",
    "title": "Inference for means",
    "section": "",
    "text": "Group exercise - CI for paired data\n\n\nUse the R code from the one-mean case (case 3) to construct a CI for the difference in tire tread.\n\n\n\n\n\n\n10:00"
  },
  {
    "objectID": "slides/lec-10.html#ht-via-simulation",
    "href": "slides/lec-10.html#ht-via-simulation",
    "title": "Inference for means",
    "section": "HT via simulation",
    "text": "HT via simulation\n\\(H_0:\\mu_{diff}=0\\)\n\\(H_a:\\mu_{diff}\\neq 0\\)\n\n\nSimulate many samples under \\(H_0\\) (no difference)\nDetermine if the observed data could have plausibly arisen under \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-10.html#simulating-under-h_0-1",
    "href": "slides/lec-10.html#simulating-under-h_0-1",
    "title": "Inference for means",
    "section": "Simulating under \\(H_0\\)",
    "text": "Simulating under \\(H_0\\)\nUnder \\(H_0\\), there is no difference between the two tire brands\n\\(\\Rightarrow\\) tire tread is independent of tire brand\n\\(\\Rightarrow\\) randomly re-assign tire tread independently of tire brand.\n\n\n\n\n\n\nRe-assign within\n\n\nThe re-assignment happens within a car; either switch the two values or keep the original allocation."
  },
  {
    "objectID": "slides/lec-10.html#re-assigning-two-cars",
    "href": "slides/lec-10.html#re-assigning-two-cars",
    "title": "Inference for means",
    "section": "Re-assigning two cars",
    "text": "Re-assigning two cars"
  },
  {
    "objectID": "slides/lec-10.html#re-assigning-all-cars",
    "href": "slides/lec-10.html#re-assigning-all-cars",
    "title": "Inference for means",
    "section": "Re-assigning all cars",
    "text": "Re-assigning all cars"
  },
  {
    "objectID": "slides/lec-10.html#section-24",
    "href": "slides/lec-10.html#section-24",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - simulated difference\n\n\nExercise 21.9\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-10.html#r-function-to-shuffle-data",
    "href": "slides/lec-10.html#r-function-to-shuffle-data",
    "title": "Inference for means",
    "section": "R function to shuffle data",
    "text": "R function to shuffle data\n\nshuffle_data <- function(data){\n  tires %>%\n  group_by(car) %>%\n  mutate(tread = sample(tread))\n}\n\nset.seed(0)\ntires_shuffled <- shuffle_data(tires)\nhead(tires)\n\n# A tibble: 6 x 3\n  tread car    brand      \n  <dbl> <chr>  <chr>      \n1 0.304 car 1  Smooth Turn\n2 0.307 car 1  Quick Spin \n3 0.302 car 10 Smooth Turn\n4 0.303 car 10 Quick Spin \n5 0.307 car 11 Smooth Turn\n6 0.305 car 11 Quick Spin \n\nhead(tires_shuffled)\n\n# A tibble: 6 x 3\n# Groups:   car [3]\n  tread car    brand      \n  <dbl> <chr>  <chr>      \n1 0.307 car 1  Smooth Turn\n2 0.304 car 1  Quick Spin \n3 0.303 car 10 Smooth Turn\n4 0.302 car 10 Quick Spin \n5 0.307 car 11 Smooth Turn\n6 0.305 car 11 Quick Spin"
  },
  {
    "objectID": "slides/lec-10.html#r-function-for-computing-the-test-statistic",
    "href": "slides/lec-10.html#r-function-for-computing-the-test-statistic",
    "title": "Inference for means",
    "section": "R function for computing the test statistic",
    "text": "R function for computing the test statistic\n\ncompute_stat <- function(data){\n  \n  data %>% \n    pivot_wider(names_from = brand, values_from = tread) %>%\n    mutate(tread_diff = `Smooth Turn` - `Quick Spin`) %>%\n    ungroup() %>%\n    summarise(x_diff_bar = mean(tread_diff)) %>%\n    pull(x_diff_bar)\n  \n}\ncompute_stat(tires_shuffled)\n\n[1] -0.001036578"
  },
  {
    "objectID": "slides/lec-10.html#for-loop-for-simulating-under-h_0-1",
    "href": "slides/lec-10.html#for-loop-for-simulating-under-h_0-1",
    "title": "Inference for means",
    "section": "For-loop for simulating under \\(H_0\\)",
    "text": "For-loop for simulating under \\(H_0\\)\n\n# Setup\nresults   <- tibble(stat_sim = numeric())\n\n# Simulations\nset.seed(0)\nfor(i in 1 : 1e3){\n  d_sim    <- shuffle_data(tires) # simulate under H0\n  stat_sim <- compute_stat(d_sim) # test statistic\n  results  <- results %>% add_row(stat_sim)\n}"
  },
  {
    "objectID": "slides/lec-10.html#sampling-distribution-1",
    "href": "slides/lec-10.html#sampling-distribution-1",
    "title": "Inference for means",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nstat_obs <- compute_stat(tires)\nstat_obs\n\n[1] 0.0020934\n\nggplot(results) + \n  geom_histogram(aes(stat_sim)) +\n  geom_vline(xintercept = stat_obs, col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-10.html#p-value-and-conclusion",
    "href": "slides/lec-10.html#p-value-and-conclusion",
    "title": "Inference for means",
    "section": "p-value and conclusion",
    "text": "p-value and conclusion\n\nthe probability that \\(\\bar{x}_{diff}^{sim}\\ge\\) 0.0021 or \\(\\bar{x}_{diff}^{sim}\\le\\) -0.0021.\n\n\nresults %>%\n  mutate(is_more_extreme = stat_sim >= stat_obs | stat_sim <= -stat_obs) %>%\n  summarize(p_value = mean(is_more_extreme))\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1   0.012\n\n\nUsing the usual significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "slides/lec-10.html#section-25",
    "href": "slides/lec-10.html#section-25",
    "title": "Inference for means",
    "section": "",
    "text": "Individual exercise - HT\n\n\nExercise 21.11\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-10.html#section-26",
    "href": "slides/lec-10.html#section-26",
    "title": "Inference for means",
    "section": "",
    "text": "Always pair\n\n\nIf the data can paired, you should always do it! Pairing data yields an analysis that is more powerful:\n\nnarrower CI\nsmaller p-values"
  },
  {
    "objectID": "slides/lec-10.html#ht-for-two-means-case-4",
    "href": "slides/lec-10.html#ht-for-two-means-case-4",
    "title": "Inference for means",
    "section": "HT for two means (case 4)",
    "text": "HT for two means (case 4)\nLet us conduct a hypothesis test for the tire data, but this time without pairing the data.\n\\(\\Rightarrow\\) This is simply a hypothesis test for two means."
  },
  {
    "objectID": "slides/lec-10.html#larger-p-value",
    "href": "slides/lec-10.html#larger-p-value",
    "title": "Inference for means",
    "section": "Larger p-value",
    "text": "Larger p-value\n\ncompute_x_diff <- function(data){\n  x_bar <- data %>%\n    group_by(brand) %>%\n    summarize(x_bar = mean(tread))\n  x_diff_bar <- x_bar$x_bar[1] - x_bar$x_bar[2]\n  return(x_diff_bar)\n}\ncompute_x_diff(tires)\n\n[1] -0.0020934\n\n\n\nresults   <- tibble(x_diff_bar = numeric())\nset.seed(0)\nfor(i in 1 : 1e3){\n  d_sim      <- tires %>% mutate(tread = sample(tread)) # simulate under H0\n  x_diff_bar <- compute_x_diff(d_sim) # test statistic\n  results    <- results %>% add_row(x_diff_bar)\n}\n\n\nx_diff_obs <- compute_x_diff(tires)\nresults %>%\n  mutate(is_more_extreme = abs(x_diff_bar) >= abs(x_diff_obs)) %>%\n  summarize(p_value = mean(is_more_extreme))\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1   0.229\n\n\nThe p-value is larger than \\(\\alpha=0.05\\); we fail to reject the null hypothesis."
  },
  {
    "objectID": "slides/lec-10.html#section-27",
    "href": "slides/lec-10.html#section-27",
    "title": "Inference for means",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(x_diff_bar)) +\n  geom_vline(xintercept = x_diff_obs, col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-10.html#section-28",
    "href": "slides/lec-10.html#section-28",
    "title": "Inference for means",
    "section": "",
    "text": "Group exercise - wider CI\n\n\nUse the R code from the two-mean case (case 4) to construct a CI for the difference in tire tread (the observations are not paired).\nYou should obtain a wider interval.\n\n\n\n\n\n\n10:00"
  },
  {
    "objectID": "slides/lec-10.html#recap-2",
    "href": "slides/lec-10.html#recap-2",
    "title": "Inference for means",
    "section": "Recap",
    "text": "Recap\n\nThe normal distribution\nOne mean (case 3)\nTwo means (case 4)\nPaired means (case 3)\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-11.html#announcements-remaining-deadlines",
    "href": "slides/lec-11.html#announcements-remaining-deadlines",
    "title": "Inference for regression",
    "section": "Announcements ‚Äì remaining deadlines",
    "text": "Announcements ‚Äì remaining deadlines\n\nHomework\n\n6 ‚Äì tonight\n7 ‚Äì Monday, June 13\n8 ‚Äì Thursday, June 16\n9 ‚Äì Sunday, June 19 (no penalty for 24 hours ‚Äì Monday, June 20)\none free 24-hour deadline extension; lowest grade dropped\n\nInference project\n\nPresentation ‚Äì Friday, June 17 (lab)\nWritten report ‚Äì Thursday, June 23"
  },
  {
    "objectID": "slides/lec-11.html#announcements-inference-project",
    "href": "slides/lec-11.html#announcements-inference-project",
    "title": "Inference for regression",
    "section": "Announcements ‚Äì inference project",
    "text": "Announcements ‚Äì inference project\n\nTeams\nOverview\n2 data sets or topics before tomorrow‚Äôs lab\nLeave lab with a data set"
  },
  {
    "objectID": "slides/lec-11.html#recap",
    "href": "slides/lec-11.html#recap",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nThe normal distribution\n\n\n\nOne mean (case 3)\n\nCI via bootstrap\n\n\n\n\n\nTwo means (case 4)\n\nHT via simulation\nCI via bootstrap\n\n\n\n\n\nPaired means (similar to one mean)\n\nHT via simulation\nCI via bootstrap\nAlways pair!"
  },
  {
    "objectID": "slides/lec-11.html#outline",
    "href": "slides/lec-11.html#outline",
    "title": "Inference for regression",
    "section": "Outline",
    "text": "Outline\n\nSimple linear regression (case 5)\n\nHT via simulation\nCI via bootstrap"
  },
  {
    "objectID": "slides/lec-11.html#setup",
    "href": "slides/lec-11.html#setup",
    "title": "Inference for regression",
    "section": "Setup",
    "text": "Setup\nSimple linear regression: \\(Y \\approx \\beta_0 + \\beta_1 X\\)\nPopulation parameter: slope parameter \\(\\beta_1\\)\nSample statistic: least-square estimate \\(\\hat{\\beta}_1\\)\n\nConfidence interval: range of plausible values for \\(\\beta_1\\)\nHypothesis test: is the response variable \\(Y\\) independent of the predictor \\(X\\)?\n\n\\(H_0:\\beta_1 = 0\\) (\\(Y\\) does not depend on \\(X\\))\n\\(H_a:\\beta_1\\neq0\\)"
  },
  {
    "objectID": "slides/lec-11.html#example-birth-weight",
    "href": "slides/lec-11.html#example-birth-weight",
    "title": "Inference for regression",
    "section": "Example ‚Äì birth weight",
    "text": "Example ‚Äì birth weight\n\n\n\nset.seed(47)\nd <- openintro::births14 %>%\n  sample_n(100) %>% # take a random sample of 100 births\n  select(weight, mage) %>% # only keep the variables weight (newborn's weight) and mage (mother's age) \n  rename(mother_age = mage)\nhead(d)\n\n# A tibble: 6 x 2\n  weight mother_age\n   <dbl>      <dbl>\n1   6.94         27\n2   9.19         29\n3   7.39         26\n4   8.82         28\n5   7.87         35\n6   9.39         29"
  },
  {
    "objectID": "slides/lec-11.html#original-data",
    "href": "slides/lec-11.html#original-data",
    "title": "Inference for regression",
    "section": "Original data",
    "text": "Original data\n\nggplot(d, aes(x = mother_age, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, col = \"maroon\") # regression line"
  },
  {
    "objectID": "slides/lec-11.html#section",
    "href": "slides/lec-11.html#section",
    "title": "Inference for regression",
    "section": "",
    "text": "Individual exercise - gut feeling\n\n\nDo you intuitively feel that the pattern could be explained by chance alone?\n\n\n\n\n\n\n01:00"
  },
  {
    "objectID": "slides/lec-11.html#hypotheses-and-simulations",
    "href": "slides/lec-11.html#hypotheses-and-simulations",
    "title": "Inference for regression",
    "section": "Hypotheses and simulations",
    "text": "Hypotheses and simulations\n\\(H_0:\\beta_1=0\\)\n\\(H_a:\\beta_1\\neq 0\\)\n\n\nSimulate many samples under \\(H_0\\) (the response variable is independent of the predictor).\nDetermine if the observed data could have plausibly arisen under \\(H_0\\)."
  },
  {
    "objectID": "slides/lec-11.html#simulating-under-h_0",
    "href": "slides/lec-11.html#simulating-under-h_0",
    "title": "Inference for regression",
    "section": "Simulating under \\(H_0\\)",
    "text": "Simulating under \\(H_0\\)\nUnder \\(H_0\\), there is no relation between the response and the predictor\n\\(\\Rightarrow\\) the newborn‚Äôs weight is independent of the mother‚Äôs age\n\\(\\Rightarrow\\) randomly re-assign the predictor independently of the response."
  },
  {
    "objectID": "slides/lec-11.html#section-1",
    "href": "slides/lec-11.html#section-1",
    "title": "Inference for regression",
    "section": "",
    "text": "d_sim <- d %>% mutate(mother_age = sample(mother_age)) # shuffle the response\nhead(d)\n\n# A tibble: 6 x 2\n  weight mother_age\n   <dbl>      <dbl>\n1   6.94         27\n2   9.19         29\n3   7.39         26\n4   8.82         28\n5   7.87         35\n6   9.39         29\n\nhead(d_sim)\n\n# A tibble: 6 x 2\n  weight mother_age\n   <dbl>      <dbl>\n1   6.94         23\n2   9.19         23\n3   7.39         23\n4   8.82         22\n5   7.87         30\n6   9.39         29"
  },
  {
    "objectID": "slides/lec-11.html#function-for-computing-the-test-statistic",
    "href": "slides/lec-11.html#function-for-computing-the-test-statistic",
    "title": "Inference for regression",
    "section": "Function for computing the test statistic",
    "text": "Function for computing the test statistic\n\ncompute_beta_LS <- function(data){\n  m     <- lm(weight ~ mother_age, data = data)\n  coef  <- m$coefficients\n  slope <- coef[[\"mother_age\"]] \n  return(slope)\n}\ncompute_beta_LS(d_sim)\n\n[1] 0.03606311"
  },
  {
    "objectID": "slides/lec-11.html#for-loop-for-simulating-under-h_0",
    "href": "slides/lec-11.html#for-loop-for-simulating-under-h_0",
    "title": "Inference for regression",
    "section": "For-loop for simulating under \\(H_0\\)",
    "text": "For-loop for simulating under \\(H_0\\)\n\nresults <- tibble(stat_sim = numeric())\nset.seed(1)\nfor(i in 1 : 10e3){ # 10,000 iterations\n  d_sim    <- d %>% mutate(mother_age = sample(mother_age)) # simulate under H0\n  stat_sim <- compute_beta_LS(d_sim) # statistic\n  results  <- results %>% add_row(stat_sim)\n}"
  },
  {
    "objectID": "slides/lec-11.html#sampling-distribution",
    "href": "slides/lec-11.html#sampling-distribution",
    "title": "Inference for regression",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nstat_obs <- compute_beta_LS(d)\nstat_obs\n\n[1] 0.05542352\n\nggplot(results) + \n  geom_histogram(aes(stat_sim)) +\n  geom_vline(xintercept = stat_obs, col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-11.html#p-value",
    "href": "slides/lec-11.html#p-value",
    "title": "Inference for regression",
    "section": "p-value",
    "text": "p-value\nProbability that \\(\\hat{\\beta}_{1}^{sim}\\ge \\hat{\\beta}^{obs}_1\\) or \\(\\hat{\\beta}_{1}^{sim}\\le -\\hat{\\beta}^{obs}_1\\).\n\np_value <- results %>%\n  mutate(is_more_extreme = stat_sim >= stat_obs | stat_sim <= -stat_obs) %>%\n  summarize(p_value = mean(is_more_extreme))\np_value\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1  0.0228\n\n\nUsing the usual significance level \\(\\alpha = 0.05\\), we reject \\(H_0\\).\n\n\nm <- lm(weight~mother_age, data = d)\nbroom::tidy(m)\n\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   5.59      0.669       8.36 4.35e-13\n2 mother_age    0.0554    0.0237      2.34 2.13e- 2"
  },
  {
    "objectID": "slides/lec-11.html#section-2",
    "href": "slides/lec-11.html#section-2",
    "title": "Inference for regression",
    "section": "",
    "text": "Group exercise - HT\n\n\nExercise 24.1 ‚Äì in part c, refer to the output of the lm command (mathematical model)\nExercise 24.9 ‚Äì in part c, refer to the output of the lm command (mathematical model)\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-11.html#bootstrap",
    "href": "slides/lec-11.html#bootstrap",
    "title": "Inference for regression",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nsample_bootstrap <- function(data){ # same function as before\n  n                <- nrow(data)\n  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)\n  return(sample_bootstrap)\n}\n\n\nset.seed(0)\nsample_bootstrap(d)\n\n# A tibble: 100 x 2\n   weight mother_age\n    <dbl>      <dbl>\n 1   7.44         35\n 2   6.83         30\n 3   5.3          29\n 4   6.94         27\n 5   5.07         23\n 6   6.03         24\n 7   6.44         26\n 8   7.44         35\n 9   7.13         33\n10   7.25         23\n# ... with 90 more rows"
  },
  {
    "objectID": "slides/lec-11.html#section-3",
    "href": "slides/lec-11.html#section-3",
    "title": "Inference for regression",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-11.html#section-4",
    "href": "slides/lec-11.html#section-4",
    "title": "Inference for regression",
    "section": "",
    "text": "results <- tibble(stat_boot = numeric())\nset.seed(0)\nfor(i in 1 : 5e3){ # 5,000 iterations\n  d_boot    <- sample_bootstrap(d)     # bootstrap sample\n  stat_boot <- compute_beta_LS(d_boot) # bootstrap statistic\n  results   <- results %>% add_row(stat_boot) \n}"
  },
  {
    "objectID": "slides/lec-11.html#section-5",
    "href": "slides/lec-11.html#section-5",
    "title": "Inference for regression",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-11.html#simulated-slopes",
    "href": "slides/lec-11.html#simulated-slopes",
    "title": "Inference for regression",
    "section": "Simulated slopes",
    "text": "Simulated slopes\n\nggplot(results) + \n  geom_histogram(aes(stat_boot)) + \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.05, 0.95)), col = \"gold1\" , size = 2) + # 90% CI \n  geom_vline(xintercept = quantile(results$stat_boot, c(0.01, 0.99)), col = \"maroon\", size = 2) # 98% CI"
  },
  {
    "objectID": "slides/lec-11.html#ci-and-ht",
    "href": "slides/lec-11.html#ci-and-ht",
    "title": "Inference for regression",
    "section": "CI and HT",
    "text": "CI and HT\n\nquantile(results$stat_boot, c(0.05, 0.95)) # 90% CI (doesn't include 0)\n\n        5%        95% \n0.01508696 0.09440465 \n\n\n\nquantile(results$stat_boot, c(0.01, 0.99)) # 98% CI (includes 0)\n\n           1%           99% \n-0.0001734654  0.1117749557 \n\n\n\nRemember that the p-value is 0.023.\n\n\n\n\n\n\n\n\nTwo sides of the same coin\n\n\nThe 95% CI dos not include 0, but the 98% CI does. This indicates that 0 is a value plausible at the (usual) significance level \\(\\alpha=0.05\\) but not at the more conservative significance level \\(\\alpha=0.02\\). Again, this is exactly what the HT concluded."
  },
  {
    "objectID": "slides/lec-11.html#section-6",
    "href": "slides/lec-11.html#section-6",
    "title": "Inference for regression",
    "section": "",
    "text": "Group exercise - CI\n\n\nExercise 24.3. Does the CI include 0? Does it agree with the HT?\nExercise 24.11. Does the CI include 0? Does it agree with the HT?\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-11.html#recap-2",
    "href": "slides/lec-11.html#recap-2",
    "title": "Inference for regression",
    "section": "Recap",
    "text": "Recap\n\nSimple linear regression (case 5)\n\nHT via simulation\nCI via bootstrap\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-12.html#announcements",
    "href": "slides/lec-12.html#announcements",
    "title": "Classical inference",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "slides/lec-12.html#recap",
    "href": "slides/lec-12.html#recap",
    "title": "Classical inference",
    "section": "Recap",
    "text": "Recap\n\nSimple linear regression (case 5)\n\nHT via simulation\nCI via bootstrap"
  },
  {
    "objectID": "slides/lec-12.html#outline",
    "href": "slides/lec-12.html#outline",
    "title": "Classical inference",
    "section": "Outline",
    "text": "Outline\n\nNormal approximation\nStatistical inference via normal approximation\n\nHypothesis test\nConfidence interval\n\nConditions"
  },
  {
    "objectID": "slides/lec-12.html#normal-distribution",
    "href": "slides/lec-12.html#normal-distribution",
    "title": "Classical inference",
    "section": "Normal distribution",
    "text": "Normal distribution\n\ntibble(x = seq(-5, 5, by = 0.01)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n\n\n\\(\\Rightarrow\\) unimodal, symmetric, thin tails ‚Äì bell-shaped"
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-1",
    "href": "slides/lec-12.html#normal-approximation-1",
    "title": "Classical inference",
    "section": "Normal approximation",
    "text": "Normal approximation\nThe normal distribution describes the variability of the different statistics\n\n\\(\\hat{p}\\), \\(\\bar{x}\\), \\(\\hat{\\beta}\\)\nsimply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!\n\n\nClassical approach: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution."
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-for-barx",
    "href": "slides/lec-12.html#normal-approximation-for-barx",
    "title": "Classical inference",
    "section": "Normal approximation for \\(\\bar{x}\\)",
    "text": "Normal approximation for \\(\\bar{x}\\)\nWe have seen that if a numerical variable \\(X\\) is normally distributed\n\\[\nX\\sim N(\\mu, \\sigma^2)\n\\]\nthen the sample average is also normally distributed\n\\[\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]"
  },
  {
    "objectID": "slides/lec-12.html#condition-for-the-normality-of-barx",
    "href": "slides/lec-12.html#condition-for-the-normality-of-barx",
    "title": "Classical inference",
    "section": "Condition for the normality of \\(\\bar{x}\\)",
    "text": "Condition for the normality of \\(\\bar{x}\\)\nIn practice, we cannot assume that the variable \\(X\\) is exactly normally distributed.\nBut as long as\n\nthe sample is large \\((n\\ge 30)\\), or\nthe variable is approximately normal: unimodal, roughly symmetric and no serious outlier\n\n\\(\\bar{x}\\) is well approximated by a normal distribution\n\\[\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\n\nSee the numerous histograms for case 3 (one mean) where the distribution of \\(\\bar{x}\\) always looks pretty normal."
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-for-hatp",
    "href": "slides/lec-12.html#normal-approximation-for-hatp",
    "title": "Classical inference",
    "section": "Normal approximation for \\(\\hat{p}\\)",
    "text": "Normal approximation for \\(\\hat{p}\\)\nIf\n\nthe observations are independent ‚Äì the independence condition\n\\(p\\) is not extreme and \\(n\\) is not small \\((pn\\ge 10 \\text{ and } (1-p)n\\ge 10)\\) ‚Äì the success-failure condition\n\nthe distribution of \\(\\hat{p}\\) can be approximated by a normal distribution\n\\[\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right)\n\\]\n\n\n\n\n\n\nSuccess-failure condition for CI\n\n\nFor CI, we verify the success-failure condition using the sample proportion \\(\\hat{p}\\):\n\\[\n\\hat{p}n\\ge 10 \\text{ and } (1-\\hat{p})n\\ge 10\n\\]"
  },
  {
    "objectID": "slides/lec-12.html#conditions-are-satisfied",
    "href": "slides/lec-12.html#conditions-are-satisfied",
    "title": "Classical inference",
    "section": "Conditions are satisfied",
    "text": "Conditions are satisfied\n\np <- 0.4; n <- 100 # conditions are satisfied: n>30, p*n>10 and (1-p)*n>10\nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)"
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-2",
    "href": "slides/lec-12.html#normal-approximation-2",
    "title": "Classical inference",
    "section": "Normal approximation",
    "text": "Normal approximation\n\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))"
  },
  {
    "objectID": "slides/lec-12.html#conditions-are-not-satisfied",
    "href": "slides/lec-12.html#conditions-are-not-satisfied",
    "title": "Classical inference",
    "section": "Conditions are not satisfied",
    "text": "Conditions are not satisfied\n\np <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 \nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)"
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-fails",
    "href": "slides/lec-12.html#normal-approximation-fails",
    "title": "Classical inference",
    "section": "Normal approximation fails",
    "text": "Normal approximation fails\n\ntibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)"
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-for-hatbeta",
    "href": "slides/lec-12.html#normal-approximation-for-hatbeta",
    "title": "Classical inference",
    "section": "Normal approximation for \\(\\hat{\\beta}\\)",
    "text": "Normal approximation for \\(\\hat{\\beta}\\)\nIf\nthe distribution of \\(\\hat{\\beta}\\) can also be approximated by a normal distribution."
  },
  {
    "objectID": "slides/lec-12.html#the-classical-approach",
    "href": "slides/lec-12.html#the-classical-approach",
    "title": "Classical inference",
    "section": "The classical approach",
    "text": "The classical approach\nStep 1: we are interested in the distribution of the statistic under \\(H_0\\).\n\nModern approach: simulate from this distribution\nClassical approach: approximate this distribution with a normal distribution\n\n\nStep 2: we want to compute the p-value\n\nModern approach: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample\nClassical approach: the p-value is the area under the curve of the normal distribution that is at least as extreme as the observed statistic."
  },
  {
    "objectID": "slides/lec-12.html#the-classical-approach-in-r",
    "href": "slides/lec-12.html#the-classical-approach-in-r",
    "title": "Classical inference",
    "section": "The classical approach in R",
    "text": "The classical approach in R\nR will compute the p-value for you. Here is what R does behind the scene:"
  },
  {
    "objectID": "slides/lec-12.html#ht-for-one-proportion",
    "href": "slides/lec-12.html#ht-for-one-proportion",
    "title": "Classical inference",
    "section": "HT for one proportion",
    "text": "HT for one proportion\n\nn <- 1500 # sample size\nx <- 780  # number of successes\nprop.test(\n  x, n,             # observed data\n  p = 0.5,          # value in the null hypothesis\n  conf.level = 0.99 # confidence level for CI\n  )\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 2.3207, df = 1, p-value = 0.1277\nalternative hypothesis: true p is not equal to 0.5\n99 percent confidence interval:\n 0.4864251 0.5533970\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "slides/lec-12.html#comparison-with-simulation-based-ht",
    "href": "slides/lec-12.html#comparison-with-simulation-based-ht",
    "title": "Classical inference",
    "section": "Comparison with simulation-based HT",
    "text": "Comparison with simulation-based HT\nThe simulation-based HT yielded a p-value of 0.127.\n\n\n\n\n\n\nA good normal approximation\n\n\nWhen the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.\n\n\n\nConditions: independence, success-failure condition"
  },
  {
    "objectID": "slides/lec-12.html#section",
    "href": "slides/lec-12.html#section",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - CI\n\n\nExercises 13.4 and 14.5\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-12.html#ht-for-two-proportions",
    "href": "slides/lec-12.html#ht-for-two-proportions",
    "title": "Classical inference",
    "section": "HT for two proportions",
    "text": "HT for two proportions\n\nn_m <- 24; n_f <- 24     # sample sizes\nx_m <- 14; x_f <- 21     # numbers of promotions\nprop.test(\n  c(x_m, x_f), c(n_m, n_f), # observed data\n  )\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x_m, x_f) out of c(n_m, n_f)\nX-squared = 3.7978, df = 1, p-value = 0.05132\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.57084188 -0.01249145\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.8750000"
  },
  {
    "objectID": "slides/lec-12.html#comparison-with-simulation-based-ht-1",
    "href": "slides/lec-12.html#comparison-with-simulation-based-ht-1",
    "title": "Classical inference",
    "section": "Comparison with simulation-based HT",
    "text": "Comparison with simulation-based HT\nUsing the simulation-based HT, we found a p-value of 0.0435.\n\n\n\n\n\n\nA good normal approximation\n\n\nWhen the conditions for the normal approximation are not satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.\nA simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied."
  },
  {
    "objectID": "slides/lec-12.html#ht-for-one-mean",
    "href": "slides/lec-12.html#ht-for-one-mean",
    "title": "Classical inference",
    "section": "HT for one mean",
    "text": "HT for one mean\n\nd <- ggplot2::mpg\nx <- d$hwy\nt.test(x, mu = 25)\n\n\n    One Sample t-test\n\ndata:  x\nt = -4.0071, df = 233, p-value = 0.00008274\nalternative hypothesis: true mean is not equal to 25\n95 percent confidence interval:\n 22.67324 24.20710\nsample estimates:\nmean of x \n 23.44017"
  },
  {
    "objectID": "slides/lec-12.html#section-1",
    "href": "slides/lec-12.html#section-1",
    "title": "Classical inference",
    "section": "",
    "text": "When conditions are not satisfied\n\n\nWhen the conditions are not satisfied, the normal distribution will not be a good approximation to the sampling distribution. In this case, we should not use the classical approach to statistical inference, but instead use simulation (HT) or bootstrap (CI).\n\n\n\n\n\n\n\n\n\nWhen the conditions are satisfied\n\n\nWhen the conditions are satisfied, the normal distribution will be a good approximation. The classical and modern (simulation, bootstrap) approaches to statistical inference will give the same results."
  },
  {
    "objectID": "slides/lec-12.html#ht-for-two-means",
    "href": "slides/lec-12.html#ht-for-two-means",
    "title": "Classical inference",
    "section": "HT for two means",
    "text": "HT for two means\n\nd <- ggplot2::mpg\nx1 <- d$hwy\nx2 <- d$cty\nt.test(x1, x2)\n\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = 13.755, df = 421.79, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 5.640710 7.521683\nsample estimates:\nmean of x mean of y \n 23.44017  16.85897"
  },
  {
    "objectID": "slides/lec-12.html#ht-for-simple-linear-regression",
    "href": "slides/lec-12.html#ht-for-simple-linear-regression",
    "title": "Classical inference",
    "section": "HT for simple linear regression",
    "text": "HT for simple linear regression\n\nm <- lm(hwy ~ cty, data = mpg)\ntidy(m)\n\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    0.892    0.469       1.90 5.84e-  2\n2 cty            1.34     0.0270     49.6  1.87e-125"
  },
  {
    "objectID": "slides/lec-12.html#ht-for-multiple-linear-regression",
    "href": "slides/lec-12.html#ht-for-multiple-linear-regression",
    "title": "Classical inference",
    "section": "HT for multiple linear regression",
    "text": "HT for multiple linear regression\n\nm <- lm(hwy ~ cty + displ, data = mpg)\ntidy(m)\n\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   1.15      1.21       0.949 3.43e- 1\n2 cty           1.33      0.0449    29.6   1.43e-80\n3 displ        -0.0343    0.148     -0.232 8.17e- 1"
  },
  {
    "objectID": "slides/lec-12.html#ht-for-logistic-regression",
    "href": "slides/lec-12.html#ht-for-logistic-regression",
    "title": "Classical inference",
    "section": "HT for logistic regression",
    "text": "HT for logistic regression\n\nd <- heart_transplant %>% mutate(survived_binary = survived == \"alive\")\nm <- glm(survived_binary ~ age + transplant, family = \"binomial\", data = d)\ntidy(m)\n\n# A tibble: 3 x 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)           0.973     1.08       0.904 0.366  \n2 age                  -0.0763    0.0255    -2.99  0.00277\n3 transplanttreatment   1.82      0.668      2.73  0.00635"
  },
  {
    "objectID": "slides/lec-12.html#standard-error",
    "href": "slides/lec-12.html#standard-error",
    "title": "Classical inference",
    "section": "Standard error",
    "text": "Standard error\nStandard error: standard deviation of statistic\n\nthe sd of \\(\\hat{p}\\) is \\(SE=\\sqrt{\\frac{p(1-p)}{n}}\\)\nthe sd of \\(\\bar{x}\\) is \\(SE=\\sqrt{\\frac{\\sigma^2}{n}} \\approx \\sqrt{\\frac{s^2}{n}}\\) where \\(s^2\\) is an estimate of the population variance \\(\\sigma^2\\) based on the sample.\nthe sd of \\(\\hat{\\beta}\\) has a complicated form."
  },
  {
    "objectID": "slides/lec-12.html#hypothesis-test",
    "href": "slides/lec-12.html#hypothesis-test",
    "title": "Classical inference",
    "section": "Hypothesis test",
    "text": "Hypothesis test\nHow many SE is the observed sample from the null value?\n\\[\n\\dfrac{\\text{point estimate} - \\text{null value}}{SE}\n\\]\n\nFor one proportion: \\(Z = \\dfrac{\\hat{p}-p_0}{SE} = \\dfrac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\)\nFor one mean: \\(t = \\dfrac{\\bar{x}-\\mu_0}{SE} = \\dfrac{\\bar{x}-\\mu_0}{\\sqrt{\\frac{s^2}{n}}}\\)\nFor the slope in a regression model: \\(t = \\dfrac{\\hat{\\beta}-0}{SE}\\)"
  },
  {
    "objectID": "slides/lec-12.html#section-2",
    "href": "slides/lec-12.html#section-2",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for one proportion\n\n\nSuppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for \\(p\\), the proportion of US adults who are democrats? What is the length of the interval?\nWhat 95% CI do you obtained if 6000 out of 10000 US adults say they are democrat? What is its length?\nExercise 16.19 ‚Äì find the 95% CI. Are the conditions satisfied?\nExercise 16.21\nExercise 16.25\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-12.html#confidence-interval",
    "href": "slides/lec-12.html#confidence-interval",
    "title": "Classical inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\\[\nCI = \\text{point estimate} \\pm \\text{critical value} *SE\n\\]\nThe critical value depends on the confidence level (90%, 95%, 99% CI) and the type of data.\nTo find the critical value corresponding to the confidence level cl use the following commands\n\nProportion: qnorm(1-(1-cl)/2)\n\nfor a 95% CI, the critical value is 1.96, for a 99% CI it is 2.57\n\nMean and regression slope: qt(1-(1-cl)/2, n-1) where \\(n\\) is the sample size.\n\nthese critical values are slightly larger than those for proportions\nas \\(n\\) increases, this difference decreases."
  },
  {
    "objectID": "slides/lec-12.html#larger-sample-give-a-smaller-se",
    "href": "slides/lec-12.html#larger-sample-give-a-smaller-se",
    "title": "Classical inference",
    "section": "Larger sample give a smaller SE",
    "text": "Larger sample give a smaller SE\n\n\n\n\n\n\nSample size matters\n\n\nLarge \\(n\\)\n\\(\\Rightarrow\\) small SE\n\\(\\Rightarrow\\) normal approximation with small sd\n\\(\\Rightarrow\\) normal approximation is more concentrated\n\\(\\Rightarrow\\) tighter CI and smaller p-values."
  },
  {
    "objectID": "slides/lec-12.html#recap-2",
    "href": "slides/lec-12.html#recap-2",
    "title": "Classical inference",
    "section": "Recap",
    "text": "Recap\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-2.html#outline",
    "href": "slides/lec-2.html#outline",
    "title": "Introduction to Data",
    "section": "Outline",
    "text": "Outline\n\nMotivating example - stent and stroke\nPrinciples of statistical inference\nTypes of variable\nExperiments and observational studies"
  },
  {
    "objectID": "slides/lec-2.html#example---stents-and-strokes",
    "href": "slides/lec-2.html#example---stents-and-strokes",
    "title": "Introduction to Data",
    "section": "Example - Stents and strokes",
    "text": "Example - Stents and strokes\nStents are known to reduce the risk of an additional heart attack or death after a cardiac event.\n\nCould stents have similar benefits for patients at risk of stroke?\n\n\n\nIf so, we should use this well-known procedure to reduce the risk of stroke!\nIf not, the procedure (surgery) should be avoided."
  },
  {
    "objectID": "slides/lec-2.html#does-the-use-of-stents-reduce-the-risk-of-stroke",
    "href": "slides/lec-2.html#does-the-use-of-stents-reduce-the-risk-of-stroke",
    "title": "Introduction to Data",
    "section": "Does the use of stents reduce the risk of stroke?",
    "text": "Does the use of stents reduce the risk of stroke?\nWe have an experiment with 451 at-risk patients:\n\neach volunteer patient was randomly assigned to either the treatment (stent) or the control (no stent) group\ncheck with patients 30 days and 365 days later"
  },
  {
    "objectID": "slides/lec-2.html#dealing-with-randomness",
    "href": "slides/lec-2.html#dealing-with-randomness",
    "title": "Introduction to Data",
    "section": "Dealing with Randomness",
    "text": "Dealing with Randomness\nSuppose I flip a coin \\(100\\) times and count the number of times I obtain heads.\n\nI expect to observe about \\(50\\) heads.\n\n\n\nImagine that I observe \\(85\\) heads instead. That would be alarming; the coin is probably not fair.\n\n\n\n\nIf I had observed \\(55\\) heads then I would not be alarmed; this is a plausible result with a fair coin."
  },
  {
    "objectID": "slides/lec-2.html#intuition-about-randomness",
    "href": "slides/lec-2.html#intuition-about-randomness",
    "title": "Introduction to Data",
    "section": "Intuition about randomness",
    "text": "Intuition about randomness\n\n\n\nGroup exercise - gut feeling about randomness\n\n\n\nIn the coin example, what number of heads would start to make you doubt that the coin is fair?\nIn the stent study, is the difference large enough to make you doubt that the stents have no effect? In other words, do you think that the difference we observe between the two groups is plausible if stents have no effect?\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-2.html#observations-and-variables",
    "href": "slides/lec-2.html#observations-and-variables",
    "title": "Introduction to Data",
    "section": "Observations and variables",
    "text": "Observations and variables\n\n\n\nResults for three patients from the stent study.\n \n  \n    patient \n    group \n    30 days \n    365 days \n  \n \n\n  \n    1 \n    treatment \n    stroke \n    stroke \n  \n  \n    2 \n    treatment \n    no event \n    no event \n  \n  \n    3 \n    treatment \n    no event \n    no event \n  \n\n\n\n\n\n\n\nEach row represents an observation\nEach column represents a variable"
  },
  {
    "objectID": "slides/lec-2.html#some-examples",
    "href": "slides/lec-2.html#some-examples",
    "title": "Introduction to Data",
    "section": "Some examples",
    "text": "Some examples\n\nObservational units: individuals, families, student cohort, cities, counties, countries, cells (biology), animals, books, courses, apples\nVariables: height, weight, age, size, year, latitude, longitude, type, sex, diet, number of pages, genre, level, color"
  },
  {
    "objectID": "slides/lec-2.html#population",
    "href": "slides/lec-2.html#population",
    "title": "Introduction to Data",
    "section": "Population",
    "text": "Population\nWe are typically interested in the relation between variables in some population.\nThe population of interest is often large, but with well-defined limits\n\ne.g.¬†patients at risk of stroke, Duke students, trees in Duke Forest, US counties\nbut not the following: people, students, patients."
  },
  {
    "objectID": "slides/lec-2.html#census-and-sample",
    "href": "slides/lec-2.html#census-and-sample",
    "title": "Introduction to Data",
    "section": "Census and sample",
    "text": "Census and sample\nThere are two ways to learn about the relation between variables in a given population.\n\n\nCensus: collect data on the whole population\n\nideal\n‚Ä¶but typically impractical, expensive\n\n\n\n\n\nSample: small fraction of the population"
  },
  {
    "objectID": "slides/lec-2.html#statistical-inference",
    "href": "slides/lec-2.html#statistical-inference",
    "title": "Introduction to Data",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nPopulation parameter, e.g.¬†mean number of hours that Duke students sleep per night\n\nGreek letter: \\(\\mu\\), \\(\\beta\\), but also \\(p\\).\n\n\n\n\nSample statistic, e.g.¬†observed average number of hours Duke students sleep per night in some sample\n\nRoman letter: \\(\\bar{x}\\), \\(b\\), \\(\\hat{p}\\)\n\n\n\n\n\nHow to learn about the population from a sample?\n\n‚Ä¶from sample statistics to population parameters\nStatistical inference provides a rigorous framework to accomplish this."
  },
  {
    "objectID": "slides/lec-2.html#statistics-as-an-art-sampling",
    "href": "slides/lec-2.html#statistics-as-an-art-sampling",
    "title": "Introduction to Data",
    "section": "Statistics as an art ‚Äì sampling",
    "text": "Statistics as an art ‚Äì sampling\nWhen you make soup, there is no need to drink the whole pot (population) to know if the it is seasoned enough.\n\nTasting a spoonful (sample) is sufficient.\nIf the soup is well mixed, a spoonful is a representative sample of the population"
  },
  {
    "objectID": "slides/lec-2.html#section",
    "href": "slides/lec-2.html#section",
    "title": "Introduction to Data",
    "section": "",
    "text": "Group exercise - sampling\n\n\nBack to the study on the effect of diet on sleep among Duke students. How would you obtain a sample of student for your study if you had (i) 1 hour, (ii) 1 week to collect your data?\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-2.html#section-1",
    "href": "slides/lec-2.html#section-1",
    "title": "Introduction to Data",
    "section": "",
    "text": "Are all samples created equal? No!\n\nWhat can go wrong?\n\nsmall samples,\nconvenience sampling, e.g.¬†students on campus,\nblind spots, e.g.¬†voters with no phone,\n‚Ä¶\n\n\n\nSampling is an art."
  },
  {
    "objectID": "slides/lec-2.html#random-sample",
    "href": "slides/lec-2.html#random-sample",
    "title": "Introduction to Data",
    "section": "Random sample",
    "text": "Random sample\nThe gold standard is a random sample\n\nbut even then, we can have non-response bias\n\n\nüõë Obtaining a representative sample is difficult.\n\n\n‚úÖ But surprisingly small representative samples can do the job!\n\ne.g.¬†1,500 voters (later in class)"
  },
  {
    "objectID": "slides/lec-2.html#numerical-variables",
    "href": "slides/lec-2.html#numerical-variables",
    "title": "Introduction to Data",
    "section": "Numerical variables",
    "text": "Numerical variables\n\nTakes a numerical value\nExamples: age, height, number of children\n\n\n\n\n\n\n\n\nWarning\n\n\nNot all numbers are numerical variables, e.g.¬†zip code, phone number.\nHeuristic: is the average meaningful? Yes!\n\n\n\n\n\nNumerical variables are either\n\ndiscrete, e.g.¬†number of siblings\nor continuous, e.g.¬†a person‚Äôs height\n\n\n\n\nnot always clear cut, e.g.¬†GPA"
  },
  {
    "objectID": "slides/lec-2.html#categorical-variables",
    "href": "slides/lec-2.html#categorical-variables",
    "title": "Introduction to Data",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nTakes a level (a category)\nExamples: eye color, place of birth, education level\n\n\n\n\n\n\n\nWarning\n\n\nSome numbers are categorical variables, e.g.¬†zip code, phone number.\nHeuristic: is the average meaningful? No!\n\n\n\n\nNumerical variables are either\n\nnominal, e.g.¬†eye color\nor ordinal, e.g.¬†education level"
  },
  {
    "objectID": "slides/lec-2.html#section-2",
    "href": "slides/lec-2.html#section-2",
    "title": "Introduction to Data",
    "section": "",
    "text": "Group exercise - types of variables\n\n\n\nExercise 1.13 b\nConsider the study you used in the previous group exercise. Can you identify a numerical and a categorical variable? What are they?\nDoes the study consider a variable of all four types? Can you come up with a variable of each type?\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-2.html#relationship-between-variables",
    "href": "slides/lec-2.html#relationship-between-variables",
    "title": "Introduction to Data",
    "section": "Relationship between variables",
    "text": "Relationship between variables\nTwo variables can either be independent or associated.\n\nIf two variables are associated, the association can be\n\nlinear (positive, or negative)\nor it can take any form, e.g.¬†U-shape, inverted-J-shape (like a square root)"
  },
  {
    "objectID": "slides/lec-2.html#section-3",
    "href": "slides/lec-2.html#section-3",
    "title": "Introduction to Data",
    "section": "",
    "text": "Group exercise - types of associations\n\n\nProvide two numerical variables which you expect to be\n\nlinearly associated; is the association positive or negative?\nassociated in a non linear way.\n\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-2.html#explanatory-and-response-variable",
    "href": "slides/lec-2.html#explanatory-and-response-variable",
    "title": "Introduction to Data",
    "section": "Explanatory and response variable",
    "text": "Explanatory and response variable\n\nWhen two variables are associated, we sometimes hypothesize that changes in one cause changes in the other.\n\n\n\nExplanatory variable \\(\\Rightarrow\\) response variable\n\n\n\n\n‚Ä¶but association \\(\\neq\\) causation; examples:\n\nice-cream and shark attacks; fire damage and firemen\ncounties and kidney cancer death rate; the best classrooms are small classrooms, but so are the worst classrooms."
  },
  {
    "objectID": "slides/lec-2.html#group-exercise---counties-and-kidney-cancer-death-rate",
    "href": "slides/lec-2.html#group-exercise---counties-and-kidney-cancer-death-rate",
    "title": "Introduction to Data",
    "section": "Group exercise - counties and kidney cancer death rate",
    "text": "Group exercise - counties and kidney cancer death rate\n\nInstructionsGroup 1Group 2Small sample effect\n\n\nWhy are most of the shaded counties in the middle of the country?\n\n\n\n04:00\n\n\n\n\n\nSource: Bayesian Data Analysis\n\n\n\n\n\nThe counties of the United Sates with the lowest 10% age-standardized death rates for cancer of kidney/ureter for U.S. white males, 1980‚Äì1989.\n\n\nSource: Bayesian Data Analysis\n\n\n\n\n\nThere is more variation in small counties.\n\n\nSource: Bayesian Data Analysis"
  },
  {
    "objectID": "slides/lec-2.html#experiments",
    "href": "slides/lec-2.html#experiments",
    "title": "Introduction to Data",
    "section": "Experiments",
    "text": "Experiments\n\nThe value of the explanatory variable is assigned by the researcher\nRandomized experiment: the value of the explanatory variable is randomly assigned\n\nremoves any counfounding (lurking) variable, e.g.¬†air temperature\n\nBlind, or even double-blind, to avoid biases\n\nplacebo\ncan go wrong, e.g.¬†vitamins in prison"
  },
  {
    "objectID": "slides/lec-2.html#observational-studies",
    "href": "slides/lec-2.html#observational-studies",
    "title": "Introduction to Data",
    "section": "Observational Studies",
    "text": "Observational Studies\n\nThe value of the explanatory variable is not assigned by the researcher\n\nthere is no interference\n\nExample: survey"
  },
  {
    "objectID": "slides/lec-2.html#section-4",
    "href": "slides/lec-2.html#section-4",
    "title": "Introduction to Data",
    "section": "",
    "text": "üõë Does not easily lead to causal claims due to the potential presence of counfounding variables\n\nSource: IMS\n\n‚Ä¶but they can lead to causal claims in certain cases!\n\nE.g. smoking causes cancer."
  },
  {
    "objectID": "slides/lec-2.html#section-5",
    "href": "slides/lec-2.html#section-5",
    "title": "Introduction to Data",
    "section": "",
    "text": "Group exercise - experiment and observational study\n\n\nYou want to investigate the effect of caffeine on class participation among Duke students\n\nDesign an observational study.\nDesign an experiment.\nIs your experiment double-blind? Can you make it double-blind?\nDo you have any ethical or practical concern with the experiment?\n\nProvide an example of an observational study that you would not turn into an experiment due to:\n\npractical considerations\nethical considerations\n\nExercise 2.12\n\n\n\n\n\n\n06:00"
  },
  {
    "objectID": "slides/lec-2.html#recap-1",
    "href": "slides/lec-2.html#recap-1",
    "title": "Introduction to Data",
    "section": "Recap",
    "text": "Recap\n\n\nobservations (row) and variables (column)\npopulation parameters and sample statistics\nstatistical inference\nsampling\nfour types of variables\n\nnumerical: continuous, discrete\ncategorical: nominal, ordinal\n\nexperiments, observational studies and causal claims\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-3.html#announcements---oh",
    "href": "slides/lec-3.html#announcements---oh",
    "title": "Data Summary and Visualization",
    "section": "Announcements - OH",
    "text": "Announcements - OH\n\nOH:\n\nRaphael: Mon, Wed 10-11am (virtual)\nRoy: Wed, Fri 4:45-5:45pm (hybrid)\n\nThis week (exceptional):\n\nRoy: Fri 2:30-3:30pm (hybrid)\nRaphael: Sun 9:00-10:00am (virtual)"
  },
  {
    "objectID": "slides/lec-3.html#announcements---hw",
    "href": "slides/lec-3.html#announcements---hw",
    "title": "Data Summary and Visualization",
    "section": "Announcements - HW",
    "text": "Announcements - HW\n\nHW due on Sunday 9:00pm and Wednesday 9:00pm\nHW 1 is due Sunday, May 15 at 9:00pm"
  },
  {
    "objectID": "slides/lec-3.html#announcements---general",
    "href": "slides/lec-3.html#announcements---general",
    "title": "Data Summary and Visualization",
    "section": "Announcements - general",
    "text": "Announcements - general\n\nLectures will closely follow IMS, but\n\nsome topics will be skipped, e.g.¬†2.1.5, dot plots, etc.\nsome topics will be added, e.g.¬†AIC and BIC\n\nDrop/Add for Term 1 ends tomorrow (Friday, May 13)."
  },
  {
    "objectID": "slides/lec-3.html#franklin-albino-and-gillman-have-read-the-syllabus.-have-you",
    "href": "slides/lec-3.html#franklin-albino-and-gillman-have-read-the-syllabus.-have-you",
    "title": "Data Summary and Visualization",
    "section": "Franklin (albino) and Gillman have read the syllabus. Have you?",
    "text": "Franklin (albino) and Gillman have read the syllabus. Have you?"
  },
  {
    "objectID": "slides/lec-3.html#recap-of-last-lecture",
    "href": "slides/lec-3.html#recap-of-last-lecture",
    "title": "Data Summary and Visualization",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nobservations (row) and variables (column)\npopulation parameters and sample statistics\nstatistical inference\nsampling\nfour types of variables\nexperiments, observational studies and causal claims"
  },
  {
    "objectID": "slides/lec-3.html#outline",
    "href": "slides/lec-3.html#outline",
    "title": "Data Summary and Visualization",
    "section": "Outline",
    "text": "Outline\n\nVisualization for numerical data\nSummary for numerical data\nVisualization for categorical data\nSummary for categorical data\nMore visualizations\n\n\n‚ÄúThe greatest value of a picture is when it forces us to notice what we never expected to see.‚Äù ‚Äî John Tukey"
  },
  {
    "objectID": "slides/lec-3.html#billion-are-you-typical",
    "href": "slides/lec-3.html#billion-are-you-typical",
    "title": "Data Summary and Visualization",
    "section": "7 Billion: Are You Typical?",
    "text": "7 Billion: Are You Typical?\nNational Geographic\n\n\n\n\nGroup exercise - data summaries?\n\n\n\nWhat variables are mentioned in the video?\nWhat are their types?\nHow were they summarized and/or visualized?\n\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-3.html#us-birth-data",
    "href": "slides/lec-3.html#us-birth-data",
    "title": "Data Summary and Visualization",
    "section": "US birth data",
    "text": "US birth data\n\nlibrary(fivethirtyeight) # for the USbirth dataset\nd_birth <- fivethirtyeight::US_births_2000_2014\n\nThere are 5479 observations (rows)\n\nnrow(d_birth) # number of rows\n\n[1] 5479\n\n\nand 6 variables (columns)\n\nncol(d_birth) # number of columns\n\n[1] 6"
  },
  {
    "objectID": "slides/lec-3.html#section",
    "href": "slides/lec-3.html#section",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "head(d_birth)\n\n# A tibble: 6 x 6\n   year month date_of_month date       day_of_week births\n  <int> <int>         <int> <date>     <ord>        <int>\n1  2000     1             1 2000-01-01 Sat           9083\n2  2000     1             2 2000-01-02 Sun           8006\n3  2000     1             3 2000-01-03 Mon          11363\n4  2000     1             4 2000-01-04 Tues         13032\n5  2000     1             5 2000-01-05 Wed          12558\n6  2000     1             6 2000-01-06 Thurs        12466\n\nlibrary(tidyverse)       # for data wrangling\nglimpse(d_birth)\n\nRows: 5,479\nColumns: 6\n$ year          <int> 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20~\n$ month         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\n$ date_of_month <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~\n$ date          <date> 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01~\n$ day_of_week   <ord> Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, Mon, Tue~\n$ births        <int> 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794~"
  },
  {
    "objectID": "slides/lec-3.html#histogram",
    "href": "slides/lec-3.html#histogram",
    "title": "Data Summary and Visualization",
    "section": "Histogram",
    "text": "Histogram\n\nggplot(d_birth) +\n  geom_histogram(aes(births)) +\n  labs(title = \"Daily number of natural births in the US between 2000 and 2014\")"
  },
  {
    "objectID": "slides/lec-3.html#section-1",
    "href": "slides/lec-3.html#section-1",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Higher bars indicate where the data are relatively more common\nMore days with around 8,000 births or with around 12,500 births\nFew days with less than 7,000 or more than 14,000 births.\nAlso few days with around 10,000 births\n\nWe can change the number of bins to have a rougher or more detailed histogram."
  },
  {
    "objectID": "slides/lec-3.html#section-2",
    "href": "slides/lec-3.html#section-2",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "bins = 10bins = 100\n\n\n\nggplot(d_birth) +\n  geom_histogram(aes(births), bins = 10)\n\n\n\n\n\n\n\n\n\n\n\nggplot(d_birth) +\n  geom_histogram(aes(births), bins = 100)"
  },
  {
    "objectID": "slides/lec-3.html#statistics-as-an-art---describing-a-distribution",
    "href": "slides/lec-3.html#statistics-as-an-art---describing-a-distribution",
    "title": "Data Summary and Visualization",
    "section": "Statistics as an art - describing a distribution",
    "text": "Statistics as an art - describing a distribution\n\n\n\n\n\n\nTip\n\n\nTo explore a numerical variable, always start with a histogram\n\n\n\nTo describe the distribution of a numerical variable, we comment on\n\nthe mode(s): unimodal, bimodal, multimodal\nthe shape of each mode: flat, bell-shape, bounded\nthe symmetry: symmetric, left skewed, right skewed\nthe outliers: presence of extreme values\nany other surprising feature.\n\n\n\n\n\n\n\nDescribing a distribution is an art\n\n\nNote that some distributions will not fit nicely in these categories."
  },
  {
    "objectID": "slides/lec-3.html#describing-the-us-birth-data",
    "href": "slides/lec-3.html#describing-the-us-birth-data",
    "title": "Data Summary and Visualization",
    "section": "Describing the US birth data",
    "text": "Describing the US birth data\nThe distribution of the daily number of births in the US is bimodal with each mode being bell-shaped and symmetric. We observe no extreme value."
  },
  {
    "objectID": "slides/lec-3.html#section-3",
    "href": "slides/lec-3.html#section-3",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Group exercise - describing a distribution\n\n\nDescribe the distributions in exercises 5.6, 5.13, 5.24 and 5.26 (only consider the histograms)\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-3.html#scatterplots",
    "href": "slides/lec-3.html#scatterplots",
    "title": "Data Summary and Visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\nHistograms: visualize the distribution of a single numerical variable.\n\nScatterplots: visualize the relation between two numerical variables."
  },
  {
    "objectID": "slides/lec-3.html#the-mpg-dataset",
    "href": "slides/lec-3.html#the-mpg-dataset",
    "title": "Data Summary and Visualization",
    "section": "The mpg dataset",
    "text": "The mpg dataset\n\nd_car <- ggplot2::mpg\nhead(d_car)\n\n# A tibble: 6 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa~\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa~\n\n\nWe will look at the relation between engine size (disp) and fuel efficiency (hwy)."
  },
  {
    "objectID": "slides/lec-3.html#section-4",
    "href": "slides/lec-3.html#section-4",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "ggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(title = \"Relation between fuel consumption on the highway and engine size\")"
  },
  {
    "objectID": "slides/lec-3.html#section-5",
    "href": "slides/lec-3.html#section-5",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Note\n\n\nTo add an additional variable to your visualization, you can use color or symbols."
  },
  {
    "objectID": "slides/lec-3.html#section-6",
    "href": "slides/lec-3.html#section-6",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Colored pointsSymbols\n\n\n\nggplot(d_car) +\n  geom_point(aes(displ, hwy, col = drv))\n\n\n\n\n\n\n\n\n\n\n\nggplot(d_car) +\n  geom_point(aes(displ, hwy, shape = drv))"
  },
  {
    "objectID": "slides/lec-3.html#measures-of-centrality",
    "href": "slides/lec-3.html#measures-of-centrality",
    "title": "Data Summary and Visualization",
    "section": "Measures of centrality",
    "text": "Measures of centrality\n\nThe average: \\(\\bar{x} = \\dfrac{x_1 + \\dots + x_n}{n}\\)\n\nTo compute the average age in the class, we would calculate\n\n\n\\[\n\\overline{\\text{age}} = \\dfrac{\\text{age}_{Hayden} + \\text{age}_{Janice} + \\text{age}_{Kenndy} + \\text{age}_{Maggie} + \\text{age}_{Melissa} + \\text{age}_{Yuanzhi}}{6}\n\\]\n\nThe median: the middle value\n\n50% of the sample is large than the median, and 50% is smaller.\n\n\n\n\nmean(d_birth$births)   # average\n\n[1] 11350.07\n\nmedian(d_birth$births) # median\n\n[1] 12343"
  },
  {
    "objectID": "slides/lec-3.html#percentiles",
    "href": "slides/lec-3.html#percentiles",
    "title": "Data Summary and Visualization",
    "section": "Percentiles",
    "text": "Percentiles\nPercentiles are a generalization of the median.\n\nThe value that is larger than p% of the data and smaller than the rest is called the p-th percentile.\n\n\nThe median is the 50th percentile.\n\n\nWe will soon make use of the 25th and 75th percentiles.\nLater in the course, the 95th and 97.5th percentiles will also be useful."
  },
  {
    "objectID": "slides/lec-3.html#measures-of-variation",
    "href": "slides/lec-3.html#measures-of-variation",
    "title": "Data Summary and Visualization",
    "section": "Measures of variation",
    "text": "Measures of variation\n\nVariance: average squared distance from the mean\n\nStandard deviation (sd): square root of the variance (roughly speaking, the average distance to the mean)\nMost (+- 95%) of the data is within 2 sd of the mean.\n\nInter-quartile range (IQR): distance between the 25th and the 75th percentiles.\n\n\n\nvar(d_birth$births) # variance\n\n[1] 5409444\n\nsd(d_birth$births) # sd\n\n[1] 2325.821\n\nIQR(d_birth$births) # iqr\n\n[1] 4342"
  },
  {
    "objectID": "slides/lec-3.html#robustness",
    "href": "slides/lec-3.html#robustness",
    "title": "Data Summary and Visualization",
    "section": "Robustness",
    "text": "Robustness\nReal-world data often contain extreme values\n\nmeasurement errors,\ntypos,\nextreme observations,\n‚Ä¶\n\nThe average, median, variance, sd and iqr are not equally robust to the presence of extreme values."
  },
  {
    "objectID": "slides/lec-3.html#section-7",
    "href": "slides/lec-3.html#section-7",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Let us contaminate the birth data with an extreme value of 1 billion‚Ä¶\n\nx_uncontaminated <- d_birth$births  \nx_contaminated   <- c(x_uncontaminated, 1e9) # 1e9 = 10^9 (scientific notation)\n\n\n‚Ä¶and compare the mean, median, variance, sd and iqr of these two variables."
  },
  {
    "objectID": "slides/lec-3.html#section-8",
    "href": "slides/lec-3.html#section-8",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "summary(x_uncontaminated)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5728    8740   12343   11350   13082   16081 \n\nsummary(x_contaminated)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n5.728e+03 8.740e+03 1.234e+04 1.938e+05 1.308e+04 1.000e+09 \n\n\n\n\nvar(x_uncontaminated); var(x_contaminated)\n\n[1] 5409444\n\n\n[1] 1.824776e+14\n\nsd(x_uncontaminated); sd(x_contaminated)\n\n[1] 2325.821\n\n\n[1] 13508428\n\nIQR(x_uncontaminated); IQR(x_contaminated)\n\n[1] 4342\n\n\n[1] 4342.25"
  },
  {
    "objectID": "slides/lec-3.html#section-9",
    "href": "slides/lec-3.html#section-9",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Robustness of the median and the iqr\n\n\nWhile the median and iqr are robust to the presence of extreme values, the mean, variance and sd are not."
  },
  {
    "objectID": "slides/lec-3.html#section-10",
    "href": "slides/lec-3.html#section-10",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Group exercise - summary statistics\n\n\nExercises 5.8, 5.11, 5.15 (replace part \\(c\\) by height of all adults)\nBonus: 5.17, 5.19\nNote: Q1 is first the 25th percentile (larger than one quarter of the data), Q3 is the 75th percentile.\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-3.html#frequency-table-1d",
    "href": "slides/lec-3.html#frequency-table-1d",
    "title": "Data Summary and Visualization",
    "section": "Frequency table (1d)",
    "text": "Frequency table (1d)\n\nhead(d_car)\n\n# A tibble: 6 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa~\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa~\n\n\n\ntable(d_car$drv)\n\n\n  4   f   r \n103 106  25"
  },
  {
    "objectID": "slides/lec-3.html#contigency-table-2d",
    "href": "slides/lec-3.html#contigency-table-2d",
    "title": "Data Summary and Visualization",
    "section": "Contigency table (2d)",
    "text": "Contigency table (2d)\n\ntable(d_car$class, d_car$drv)\n\n            \n              4  f  r\n  2seater     0  0  5\n  compact    12 35  0\n  midsize     3 38  0\n  minivan     0 11  0\n  pickup     33  0  0\n  subcompact  4 22  9\n  suv        51  0 11"
  },
  {
    "objectID": "slides/lec-3.html#proportion-table-2d",
    "href": "slides/lec-3.html#proportion-table-2d",
    "title": "Data Summary and Visualization",
    "section": "Proportion table (2d)",
    "text": "Proportion table (2d)\n\ntable proportionrow proportioncolumn proportion\n\n\n\ntable(d_car$class, d_car$drv) %>%\n  prop.table() %>%\n  round(2)\n\n            \n                4    f    r\n  2seater    0.00 0.00 0.02\n  compact    0.05 0.15 0.00\n  midsize    0.01 0.16 0.00\n  minivan    0.00 0.05 0.00\n  pickup     0.14 0.00 0.00\n  subcompact 0.02 0.09 0.04\n  suv        0.22 0.00 0.05\n\n\n\n\n\ntable(d_car$class, d_car$drv) %>%\n  prop.table(1) %>%\n  round(2)\n\n            \n                4    f    r\n  2seater    0.00 0.00 1.00\n  compact    0.26 0.74 0.00\n  midsize    0.07 0.93 0.00\n  minivan    0.00 1.00 0.00\n  pickup     1.00 0.00 0.00\n  subcompact 0.11 0.63 0.26\n  suv        0.82 0.00 0.18\n\n\n\n\n\ntable(d_car$class, d_car$drv) %>%\n  prop.table(2) %>%\n  round(2)\n\n            \n                4    f    r\n  2seater    0.00 0.00 0.20\n  compact    0.12 0.33 0.00\n  midsize    0.03 0.36 0.00\n  minivan    0.00 0.10 0.00\n  pickup     0.32 0.00 0.00\n  subcompact 0.04 0.21 0.36\n  suv        0.50 0.00 0.44"
  },
  {
    "objectID": "slides/lec-3.html#section-11",
    "href": "slides/lec-3.html#section-11",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Group exercise - contigency and proportion table\n\n\n\nWhat does the number \\(12\\) (2nd row, 1st column) represent in the contigency table?\nWhat does the number \\(0.05\\) (2nd row, 1st column) represent in the first proportion table?\nWhat does the number \\(0.25\\) (2nd row, 1st column) represent in the row proportion table?\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-3.html#barplot",
    "href": "slides/lec-3.html#barplot",
    "title": "Data Summary and Visualization",
    "section": "Barplot",
    "text": "Barplot\n\nggplot(d_car) +\n  geom_bar(aes(drv))\n\n\n\nWe can add a second categorical variable using colors."
  },
  {
    "objectID": "slides/lec-3.html#advanced-barplots",
    "href": "slides/lec-3.html#advanced-barplots",
    "title": "Data Summary and Visualization",
    "section": "Advanced barplots",
    "text": "Advanced barplots\n\nstackeddodgedstandardized\n\n\n\nggplot(d_car) +\n  geom_bar(aes(drv, fill = class))\n\n\n\n\n\n\n\n\n\n\n\nggplot(d_car) +\n  geom_bar(aes(drv, fill = class), position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(d_car) +\n  geom_bar(aes(drv, fill = class), position = \"fill\")"
  },
  {
    "objectID": "slides/lec-3.html#section-12",
    "href": "slides/lec-3.html#section-12",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Group exercise - pros and cons of barplots\n\n\nExercise 4.5\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-3.html#faceted-histograms",
    "href": "slides/lec-3.html#faceted-histograms",
    "title": "Data Summary and Visualization",
    "section": "Faceted histograms",
    "text": "Faceted histograms\n\nd_birth_small <- filter(d_birth, year %in% c(2000, 2004, 2009, 2014))\nggplot(d_birth_small) +\n  geom_histogram(aes(births)) + \n  facet_grid(year~.)"
  },
  {
    "objectID": "slides/lec-3.html#section-13",
    "href": "slides/lec-3.html#section-13",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Beyond 2 variables\n\n\nFaceted figures are a great way to include \\(\\ge\\) 3 variables! See exercise 1.13"
  },
  {
    "objectID": "slides/lec-3.html#mosaic-plot",
    "href": "slides/lec-3.html#mosaic-plot",
    "title": "Data Summary and Visualization",
    "section": "Mosaic plot",
    "text": "Mosaic plot\n\nggplot(d_car) +\n  geom_mosaic(aes(x = product(drv), fill = class))\n\n\n\n‚úÖ Combines the strengths of the various barplots.\nüõë Not in the tool box of every data scientist"
  },
  {
    "objectID": "slides/lec-3.html#boxplots",
    "href": "slides/lec-3.html#boxplots",
    "title": "Data Summary and Visualization",
    "section": "Boxplots",
    "text": "Boxplots\n\nFrom raw data to boxplotSource: R 4 Data Science"
  },
  {
    "objectID": "slides/lec-3.html#section-14",
    "href": "slides/lec-3.html#section-14",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "The thick line in the middle of the box indicates the median;\nthe box stretches from the 25th percentile (Q1) to the 75th percentile (Q3); it covers 50% of the data;\nthe length of the whiskers are at most 1.5 iqr;\nany observation more than 1.5 iqr away from the box is labelled as an outlier;\n\n\n\nmore compact than histograms\n\n\n\n\n\n\n\n\n\nOutliers\n\n\nOutliers have an extreme value. How to deal with an outlier depends on why the observation stands out. Outliers can be\n\nremoved\ncorrected\nignored"
  },
  {
    "objectID": "slides/lec-3.html#section-15",
    "href": "slides/lec-3.html#section-15",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "Group exercise - limitation of boxplots\n\n\nExercise 5.13\n\n\n\n\n\n\n01:00"
  },
  {
    "objectID": "slides/lec-3.html#section-16",
    "href": "slides/lec-3.html#section-16",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "BoxplotSide-by-side boxplots\n\n\n\nggplot(d_birth) +\n  geom_boxplot(aes(y = births))\n\n\n\n\n\n\n\n\n\n\n\nggplot(d_birth) +\n  geom_boxplot(aes(y=births, x=day_of_week))"
  },
  {
    "objectID": "slides/lec-3.html#figure-title",
    "href": "slides/lec-3.html#figure-title",
    "title": "Data Summary and Visualization",
    "section": "Figure title",
    "text": "Figure title\n\nggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(title = \"Fuel consumption on the highway per engine size\")"
  },
  {
    "objectID": "slides/lec-3.html#axis-labels",
    "href": "slides/lec-3.html#axis-labels",
    "title": "Data Summary and Visualization",
    "section": "Axis labels",
    "text": "Axis labels\n\nggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(\n    title = \"Fuel consumption on the highway per engine size\",\n    x = \"Engine size (engine displaced in litres)\",\n    y = \"Fuel efficiency on the highway (mpg)\"\n    )"
  },
  {
    "objectID": "slides/lec-3.html#section-17",
    "href": "slides/lec-3.html#section-17",
    "title": "Data Summary and Visualization",
    "section": "",
    "text": "theme_bwtheme_classictheme_dark\n\n\n\n\nShow the code\nggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(\n    title = \"Fuel consumption on the highway per engine size\",\n    x = \"Engine size (engine displaced in litres)\",\n    y = \"Fuel efficiency on the highway (mpg)\"\n    ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(\n    title = \"Fuel consumption on the highway per engine size\",\n    x = \"Engine size (engine displaced in litres)\",\n    y = \"Fuel efficiency on the highway (mpg)\"\n    ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(d_car) +\n  geom_point(aes(displ, hwy)) +\n  labs(\n    title = \"Fuel consumption on the highway per engine size\",\n    x = \"Engine size (engine displaced in litres)\",\n    y = \"Fuel efficiency on the highway (mpg)\"\n    ) +\n  theme_dark()"
  },
  {
    "objectID": "slides/lec-3.html#editing-tables",
    "href": "slides/lec-3.html#editing-tables",
    "title": "Data Summary and Visualization",
    "section": "Editing tables",
    "text": "Editing tables\n\ntable(d_car$class, d_car$drv) %>%\n  prop.table(1) %>%\n  round(2) %>%\n  kbl(caption = \"Distribution of drive type per class of car\") %>%\n  kable_classic(full_width = FALSE, c(\"striped\", \"hover\"))\n\n\nDistribution of drive type per class of car\n \n  \n      \n    4 \n    f \n    r \n  \n \n\n  \n    2seater \n    0.00 \n    0.00 \n    1.00 \n  \n  \n    compact \n    0.26 \n    0.74 \n    0.00 \n  \n  \n    midsize \n    0.07 \n    0.93 \n    0.00 \n  \n  \n    minivan \n    0.00 \n    1.00 \n    0.00 \n  \n  \n    pickup \n    1.00 \n    0.00 \n    0.00 \n  \n  \n    subcompact \n    0.11 \n    0.63 \n    0.26 \n  \n  \n    suv \n    0.82 \n    0.00 \n    0.18 \n  \n\n\n\n\n\nüìã See this vignette for more details on editing tables"
  },
  {
    "objectID": "slides/lec-3.html#statistics-as-an-art---figures",
    "href": "slides/lec-3.html#statistics-as-an-art---figures",
    "title": "Data Summary and Visualization",
    "section": "Statistics as an art - figures",
    "text": "Statistics as an art - figures\n\nHave a purpose: is the figure necessary?\n\n\n\nParsimony: keep it simple and avoid distractions\n\n\n\n\nTell a story: provide context and interpret the figure\n\n\n\n\nAt least 3 variables as often as possible: color, shape, facets, etc.\n\n\n\n\nEdit your figure: title, axes, theme, etc.\n\n\n\nüìã See R for Data Science - chapters 3 and 7 for more on data visualization in R."
  },
  {
    "objectID": "slides/lec-3.html#recap-1",
    "href": "slides/lec-3.html#recap-1",
    "title": "Data Summary and Visualization",
    "section": "Recap",
    "text": "Recap\n\n\nHistogram, scatterplot, boxplot\nAverage, median, variance, sd and IQR; robustness\nFrequency, contigency and proportion tables\nBarplot, mosaic plot\nEffective communication: well-edited figures, \\(\\ge3\\) variables (symbols, colors, facets), tell a story\nR for Data Science - chapters 3 and 7\n\n\n\n\n‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device.‚Äù ‚Äî John Tukey\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-4.html#announcements",
    "href": "slides/lec-4.html#announcements",
    "title": "Simple Linear Regression Models",
    "section": "Announcements",
    "text": "Announcements\n\nFind someone you have not worked with yet.\nHomework 1 has been released\n\nAsk us if anything is unclear\nregrade window of 48 hours.\n\nHomework 2 has been published\n\nDue on Wednesday (tomorrow)\nFeel free to re-use the template from HW 1\nThere should be no need for external code ‚Äì look in the lab!"
  },
  {
    "objectID": "slides/lec-4.html#recap-of-last-lecture",
    "href": "slides/lec-4.html#recap-of-last-lecture",
    "title": "Simple Linear Regression Models",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nHistogram, scatterplot, boxplot\nAverage, median, variance, sd and IQR; robustness\nFrequency, contigency and proportion tables\nBarplot, mosaic plot\nEffective communication: well-edited figures, \\(\\ge3\\) variables (symbols, colors, facets)\nR for Data Science - chapters 3 and 7"
  },
  {
    "objectID": "slides/lec-4.html#outline",
    "href": "slides/lec-4.html#outline",
    "title": "Simple Linear Regression Models",
    "section": "Outline",
    "text": "Outline\n\nsimple linear regression model\nleast-square estimates\nmodel comparison\noutliers"
  },
  {
    "objectID": "slides/lec-4.html#regression-model",
    "href": "slides/lec-4.html#regression-model",
    "title": "Simple Linear Regression Models",
    "section": "Regression model",
    "text": "Regression model\n\nd <- ggplot2::mpg\nhead(d, n = 4)\n\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n\n\n\nSuppose the variable hwy (fuel efficiency on highway) is very expensive to measure.\nWe decide to estimate it using the other variables. To do so, we will fit a regression model.\n\\[\n\\text{hwy} \\approx \\text{model}(\\text{other variables})\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#simple-regression",
    "href": "slides/lec-4.html#simple-regression",
    "title": "Simple Linear Regression Models",
    "section": "Simple regression",
    "text": "Simple regression\nWe expect the variable cty to be a good proxy for hwy.\nAfter all, if a car is efficient in the city, we expect it to also be efficient on the highway! We will therefore consider a simple regression model\n\\[\n\\text{hwy} \\approx \\text{model}(\\text{cty})\n\\] Simple here means that we include a single predictor in the model."
  },
  {
    "objectID": "slides/lec-4.html#simple-linear-regression-1",
    "href": "slides/lec-4.html#simple-linear-regression-1",
    "title": "Simple Linear Regression Models",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy))\n\n\nThe variables cty and hwy are linearly associated.\nWe therefore opt for a simple linear regression model\n\\[\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#parameters",
    "href": "slides/lec-4.html#parameters",
    "title": "Simple Linear Regression Models",
    "section": "Parameters",
    "text": "Parameters\nThe numbers \\(\\beta_0\\) and \\(\\beta_1\\) are the two parameters of the model.\n\nWe now want to find good values for these unknown parameters.\n\n\nLet us look at two sets of values:\n\\[\\begin{align*}\n\\beta_0 = 1, \\beta_1 = 1.3 \\qquad & \\Rightarrow \\qquad \\text{hwy} \\approx 1 + 1.3 \\text{cty} \\\\\n\n\\beta_0 = 15, \\beta_1 = 0.5 \\qquad & \\Rightarrow \\qquad \\text{hwy} \\approx 15 + 0.5 \\text{cty}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lec-4.html#section",
    "href": "slides/lec-4.html#section",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Good modelBad model\n\n\n\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 1, slope = 1.3)\n\n\n\n\n\n\n\n\n\n\n\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 15, slope = 0.5)"
  },
  {
    "objectID": "slides/lec-4.html#prediction",
    "href": "slides/lec-4.html#prediction",
    "title": "Simple Linear Regression Models",
    "section": "Prediction",
    "text": "Prediction\nWe can use our models to estimate hwy for new vehicles.\n\nImagine there is a new vehicle with \\(\\text{cty} = 30\\). Instead of measuring its hwy (expensive), we use our model to estimate it. Using the ‚Äúgood‚Äù model gives the following estimate\n\\[\n\\begin{align*}\n\\text{hwy}\n& \\approx \\beta_0 + \\beta_1 \\text{cty} \\\\\n& = 1 + 1.3 * 30\\\\\n& = 40\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#section-1",
    "href": "slides/lec-4.html#section-1",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Is this the true hwy of the new vehicle? No!\n\n\nthis is only an estimate based on the value of the variable cty and our ‚Äúgood‚Äù model.\n\n\n\nCan we do better? Yes!\n\n\n\nTake additional variables into account in the model (e.g.¬†engine size, vehicle age, etc)\n\n\n\n\nUse better values for \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "slides/lec-4.html#section-2",
    "href": "slides/lec-4.html#section-2",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - parameters\n\n\n\nWhat is the prediction for the new vehicle (\\(\\text{cty} = 30\\)) if we use the bad model (\\(\\beta_0 = 15, \\beta_1=0.5\\))?\nCopy and paste the following piece of code and try different values for the parameters to find a good set of values.\n\n\nlibrary(tidyverse)\nd <- ggplot2::mpg\nbeta_0 <- 15\nbeta_1 <- 0.5\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = beta_0, slope = beta_1)\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-4.html#linear-association",
    "href": "slides/lec-4.html#linear-association",
    "title": "Simple Linear Regression Models",
    "section": "Linear association",
    "text": "Linear association\nA simple linear regression model is only applicable if the relation between the predictor and the response is linear.\n\nIf the relation is not linear, the simple linear regression is not suitable.\nIn this case, we need to model the non-linearity (next lecture)."
  },
  {
    "objectID": "slides/lec-4.html#section-3",
    "href": "slides/lec-4.html#section-3",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - linear association\n\n\nExercise 7.3\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-4.html#residuals",
    "href": "slides/lec-4.html#residuals",
    "title": "Simple Linear Regression Models",
    "section": "Residuals",
    "text": "Residuals\nOur predictions are only approximate.\n\nLet us represent our prediction with \\(\\widehat{\\text{hwy}}\\) and the true value with \\(\\text{hwy}\\)\nthe error we make is \\(\\text{hw} - \\widehat{\\text{hwy}}\\)\nthis is called the residual\n\n\\[e = \\text{hwy} - \\widehat{\\text{hwy}}\\]"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-residuals",
    "href": "slides/lec-4.html#visualizing-residuals",
    "title": "Simple Linear Regression Models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals\n\n\n\n\nBlack circles: Observed values (\\(\\text{hwy}\\))\nPink solid line: Least-squares regression line\nMaroon triangles: Predicted values (\\(\\widehat{\\text{hwy}}\\))\nGray lines: Residuals"
  },
  {
    "objectID": "slides/lec-4.html#residual-plot",
    "href": "slides/lec-4.html#residual-plot",
    "title": "Simple Linear Regression Models",
    "section": "Residual plot",
    "text": "Residual plot\n\nd %>%\n  mutate(\n    hwy_hat = 1 + 1.3 * cty, # prediction\n    resid   = hwy - hwy_hat  # residual\n    ) %>%\n  ggplot() +\n  geom_point(aes(cty, resid)) +\n  geom_abline(intercept = 0, slope = 0, col = \"red\")"
  },
  {
    "objectID": "slides/lec-4.html#assessing-linearity-with-residual-plots",
    "href": "slides/lec-4.html#assessing-linearity-with-residual-plots",
    "title": "Simple Linear Regression Models",
    "section": "Assessing linearity with residual plots",
    "text": "Assessing linearity with residual plots\n\nSource: IMS"
  },
  {
    "objectID": "slides/lec-4.html#section-4",
    "href": "slides/lec-4.html#section-4",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - residuals\n\n\nExercises 7.1, 7.17, 7.19\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-4.html#good-estimates",
    "href": "slides/lec-4.html#good-estimates",
    "title": "Simple Linear Regression Models",
    "section": "Good estimates",
    "text": "Good estimates\nWe want to choose estimates that give a model that fits the data well.\n\na model with a regression line that is close to the data\n\n\nWe want to minimize the residuals."
  },
  {
    "objectID": "slides/lec-4.html#minimizing-residuals",
    "href": "slides/lec-4.html#minimizing-residuals",
    "title": "Simple Linear Regression Models",
    "section": "Minimizing residuals",
    "text": "Minimizing residuals\nPerhaps the most natural thing to do is to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the sum of absolute residuals\n\\[\n|e_1|+|e_2|+\\dots+|e_n|\n\\]\n\nFor practical reasons, the sum of squared residuals (SSR) is a more common criterion\n\\[\ne_1^2+e_2^2+\\dots+e_n^2\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#why-squaring-the-residuals",
    "href": "slides/lec-4.html#why-squaring-the-residuals",
    "title": "Simple Linear Regression Models",
    "section": "Why squaring the residuals?",
    "text": "Why squaring the residuals?\n\nnice mathematical properties\n\ncan work by hand (pre-computer era; first derived by Gauss in the late 1700s (Stigler, 1981))\n\nreflects the assumptions that being off by \\(4\\) is more than twice as bad as being off by \\(2\\)\nmainstream, e.g.¬†the command lm in R."
  },
  {
    "objectID": "slides/lec-4.html#least-square-estimates",
    "href": "slides/lec-4.html#least-square-estimates",
    "title": "Simple Linear Regression Models",
    "section": "Least-square estimates",
    "text": "Least-square estimates\nWe simply find the values for \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the SSR with the R command lm\n\nm <- lm(hwy ~ cty, data = d) # find least-square estimates\nm\n\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\n\nThe symbols \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) denote least-square estimates.\nIn our model, we therefore have \\[\\hat{\\beta}_0 = 0.892, \\qquad \\hat{\\beta}_1 = 1.337\\]"
  },
  {
    "objectID": "slides/lec-4.html#visualizing-the-least-square-regression-line",
    "href": "slides/lec-4.html#visualizing-the-least-square-regression-line",
    "title": "Simple Linear Regression Models",
    "section": "Visualizing the least square regression line",
    "text": "Visualizing the least square regression line\n\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 0.892, slope = 1.337, col = \"red\")"
  },
  {
    "objectID": "slides/lec-4.html#reading-r-output",
    "href": "slides/lec-4.html#reading-r-output",
    "title": "Simple Linear Regression Models",
    "section": "Reading R output",
    "text": "Reading R output\nR can provide the results of a model in different formats.\n\nm <- lm(hwy ~ cty, data = d)\n\n\nEstimates onlyRaw R outputTidy R output\n\n\n\nm\n\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\n\nPrinting the model itself provides the least-square estimates. This is sufficient for now.\n\n\n\nsummary(m)\n\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,    Adjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\nThis format contains information that will be useful when we do inference. It is, however, difficult to read.\n\n\n\nsummary(m) %>%\n  tidy() %>%\n  kable(digits = 2)\n\n\n \n  \n    term \n    estimate \n    std.error \n    statistic \n    p.value \n  \n \n\n  \n    (Intercept) \n    0.89 \n    0.47 \n    1.90 \n    0.06 \n  \n  \n    cty \n    1.34 \n    0.03 \n    49.58 \n    0.00 \n  \n\n\n\n\n\nThis format contains the information necessary for doing inference and is easy to read. This is the format used the textbook."
  },
  {
    "objectID": "slides/lec-4.html#exploring-the-model",
    "href": "slides/lec-4.html#exploring-the-model",
    "title": "Simple Linear Regression Models",
    "section": "Exploring the model",
    "text": "Exploring the model\nThe command augment from the R package broom gives us the residuals (.resid) and predictions (.fitted).\n\nlibrary(broom)\nm <- lm(hwy ~ cty, data = d)\naugment(m)[,1:4]\n\n# A tibble: 234 x 4\n     hwy   cty .fitted .resid\n   <int> <int>   <dbl>  <dbl>\n 1    29    18    25.0 4.03  \n 2    29    21    29.0 0.0214\n 3    31    20    27.6 3.36  \n 4    30    21    29.0 1.02  \n 5    26    16    22.3 3.71  \n 6    26    18    25.0 1.03  \n 7    27    18    25.0 2.03  \n 8    26    18    25.0 1.03  \n 9    25    16    22.3 2.71  \n10    28    20    27.6 0.359 \n# ... with 224 more rows"
  },
  {
    "objectID": "slides/lec-4.html#section-5",
    "href": "slides/lec-4.html#section-5",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - exploring a model\n\n\nConsider the third observation in the sample. What is its\n\nvalue of cty?\npredicted value for hwy based on the model?\nactual value for hwy?\nresidual?\nDoes the model over- or under-predict?\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-4.html#intepreting-the-parameters",
    "href": "slides/lec-4.html#intepreting-the-parameters",
    "title": "Simple Linear Regression Models",
    "section": "Intepreting the parameters",
    "text": "Intepreting the parameters\n\nthe intercept estimate \\(\\hat{\\beta}_0\\) is the prediction for a car with \\(\\text{cty} = 0\\)\n\nmeaningless in our case\n\n\n\n\nthe slope estimate \\(\\hat{\\beta}_1\\) is the increase in a our prediction for hwy for each additional unit of cty\n\n‚Äúfor each additional unit of cty, we expect hwy to increase by 1.337‚Äù\n\n\n\n\n\n\n\n\n\n\nExtrapolation\n\n\n\nStick to the range of the data\n\nif you extrapolate, do so with care, not like this.\n\nThe intercept will not always be meaningful"
  },
  {
    "objectID": "slides/lec-4.html#section-6",
    "href": "slides/lec-4.html#section-6",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - least-square estimates\n\n\nExercise 7.21\n\nskip part c\nstart by fitting the model in R with the following command,\n\n\nd <- openintro::coast_starlight\nm <- lm(travel_time ~ dist, data = d)\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-4.html#special-case-categorical-predictor",
    "href": "slides/lec-4.html#special-case-categorical-predictor",
    "title": "Simple Linear Regression Models",
    "section": "Special case: categorical predictor",
    "text": "Special case: categorical predictor\nLet us create a binary predictor indicating if the car is from \\(1999\\).\n\nd_binary <- mutate(d, year_binary = if_else(year == 1999, 1, 0))\nhead(select(d_binary, hwy, year, year_binary))\n\n# A tibble: 6 x 3\n    hwy  year year_binary\n  <int> <int>       <dbl>\n1    29  1999           1\n2    29  1999           1\n3    31  2008           0\n4    30  2008           0\n5    26  1999           1\n6    26  1999           1\n\n\nThe variable year_binary takes the value \\(1\\) if the car is from \\(1999\\) and \\(0\\) otherwise."
  },
  {
    "objectID": "slides/lec-4.html#section-7",
    "href": "slides/lec-4.html#section-7",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "m_binary <- lm(hwy ~ year_binary, data = d_binary)\nm_binary\n\n\nCall:\nlm(formula = hwy ~ year_binary, data = d_binary)\n\nCoefficients:\n(Intercept)  year_binary  \n   23.45299     -0.02564  \n\n\nThe model equation is\n\\[\n\\text{hwy} \\approx 23.46 - 0.026 \\text{year_binary}\n\\]\n\nFor a new car from \\(1999\\), the variable year_binary takes the value \\(1\\) and our prediction is\n\\[\n\\widehat{\\text{hwy}} = 23.46 - 0.026*1 = 23.46 - 0.026 = 23.434\n\\]\n\n\nWhile for a car that not from \\(1999\\), the variable year_binary takes the value \\(0\\) and our prediction is\n\\[\n\\widehat{\\text{hwy}} = 23.46 - 0.026*0 = 23.46 - 0 = 23.46\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#alternative-model",
    "href": "slides/lec-4.html#alternative-model",
    "title": "Simple Linear Regression Models",
    "section": "Alternative model",
    "text": "Alternative model\nLet us fit an alternative model using engine size (disp) as a predictor\n\\[\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{displ}\n\\]\n\nggplot(d) +\n  geom_point(aes(displ, hwy))"
  },
  {
    "objectID": "slides/lec-4.html#section-8",
    "href": "slides/lec-4.html#section-8",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "The least-square estimates for the coefficients are\n\nlm(hwy ~ displ, data = d)\n\n\nCall:\nlm(formula = hwy ~ displ, data = d)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\n\nNote that the slope coefficient is negative; which makes sense since we would expect cars with larger engines to be less efficient."
  },
  {
    "objectID": "slides/lec-4.html#section-9",
    "href": "slides/lec-4.html#section-9",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "We now have two models. Which is the best?\n\n\nWe could start by looking at the residuals"
  },
  {
    "objectID": "slides/lec-4.html#comparing-residuals",
    "href": "slides/lec-4.html#comparing-residuals",
    "title": "Simple Linear Regression Models",
    "section": "Comparing residuals",
    "text": "Comparing residuals\n\n\n\nm1 <- lm(hwy ~ cty, data = d)\nm_augment <- augment(m1)\nggplot(m_augment) +\n  geom_histogram(aes(.resid)) +\n  xlim(-15, 15)\n\n\n\n\n\n\n\n\n\n\nlm(hwy ~ displ, data = d) %>%\n  augment %>%\n  ggplot() +\n  geom_histogram(aes(.resid)) +\n  xlim(-15, 15)\n\n\n\n\n\n\n\n\n\n\n\nThe first model seems to have smaller residuals.\n\\(\\Rightarrow\\) choose the first model!"
  },
  {
    "objectID": "slides/lec-4.html#comparing-models-in-a-systematic-way",
    "href": "slides/lec-4.html#comparing-models-in-a-systematic-way",
    "title": "Simple Linear Regression Models",
    "section": "Comparing models in a systematic way",
    "text": "Comparing models in a systematic way\nBut looking at a plot can be misleading\n\nillusions\ndifficult to compare models with similar residuals\n\n\nWe need a more systematic approach for comparing models."
  },
  {
    "objectID": "slides/lec-4.html#ssr",
    "href": "slides/lec-4.html#ssr",
    "title": "Simple Linear Regression Models",
    "section": "SSR",
    "text": "SSR\nInstead of comparing histograms of residuals, we can compute the SSR (sum of squared residuals!)\n\\[\\begin{align*}\nSSR\n& = r_1^2+r_2^2+\\dots+r^2_n \\\\\n& = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_n - \\hat{y}_n)^2\n\\end{align*}\\]\n\nsmall residuals will give a small SSR\nlarge residuals will give a large SSR\n\n\n\\(\\Rightarrow\\) choose the model with the smaller SSR!\n\n\nüìã The textbook uses the term SSE (sum of squared errors)."
  },
  {
    "objectID": "slides/lec-4.html#section-10",
    "href": "slides/lec-4.html#section-10",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "SSR of the first model:\n\nm1 <- lm(hwy ~ cty, data = d)\nm_augment <- augment(m1)\nresid <- m_augment$.resid\nsum(resid^2)\n\n[1] 712.3599\n\n\nSSR of the second model:\n\nlm(hwy ~ displ, data = d) %>%\n  augment %>% \n  .[[\".resid\"]] %>% \n  .^2 %>% \n  sum\n\n[1] 3413.829\n\n\n\nWe opt for the first model (smaller SSR)."
  },
  {
    "objectID": "slides/lec-4.html#r2",
    "href": "slides/lec-4.html#r2",
    "title": "Simple Linear Regression Models",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nWhile the SSR is useful for comparing models, it can also be used to describe the goodness of fit of the model.\n\nThe SST (total sum of squares) is the sum of squared distance to the mean.\n\\[\nSST = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2\n\\]\nIt measures the total amount of variability in the data.\n\n\nRemember the formula for SSR\n\\[\nSSR = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_n - \\hat{y}_n)^2\n\\]\nIt measures the amount of variability in the data left unexplained by the model.\n\n\n\\(SST - SSR\\) (total - residual) is therefore the amount of variation explained by the model:\n\\[\n\\text{data} = SST = (SST-SSR) + SSR = \\text{model} + \\text{residuals}\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#section-11",
    "href": "slides/lec-4.html#section-11",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "The statistic \\(R^2\\) measures the proportion of variation in the data that is explained by the model.\n\\[\n\\begin{align*}\nR^2\n& = 1-\\dfrac{SSR}{SST}\\\\\n& = \\dfrac{SST-SSR}{SST} \\\\\n& = \\dfrac{\\text{var. explained by model}}{\\text{total var.}}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lec-4.html#section-12",
    "href": "slides/lec-4.html#section-12",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Note that \\(0\\le R^2 \\le 1\\).\n\ngood model \\(\\Rightarrow\\) small residuals \\(\\Rightarrow\\) small SSR \\(\\Rightarrow\\) large \\(R^2\\).\ngreat model \\(\\Rightarrow\\) tiny residuals \\(\\Rightarrow\\) tiny SSR \\(\\Rightarrow\\) \\(R^2\\) close to 1.\npoor model \\(\\Rightarrow\\) large residuals \\(\\Rightarrow\\) SSR almost as large as SST \\(\\Rightarrow\\) \\(R^2\\) close to 0."
  },
  {
    "objectID": "slides/lec-4.html#computing-r2-in-r",
    "href": "slides/lec-4.html#computing-r2-in-r",
    "title": "Simple Linear Regression Models",
    "section": "Computing \\(R^2\\) in R",
    "text": "Computing \\(R^2\\) in R\nTo compute \\(R^2\\) in R, simply use the command glance.\n\nm1 <- lm(hwy ~ cty, data = d)\nm_glance <- glance(m1)\nm_glance$r.squared\n\n[1] 0.9137752\n\nlm(hwy ~ displ, data = d) %>% \n  glance %>% \n  .[[\"r.squared\"]]\n\n[1] 0.5867867\n\n\nThe model with cty as a predictor has a \\(R^2\\) value of \\(0.914\\), and the model that uses displ has a \\(R^2\\) of \\(0.59\\) (worse).\n\nThe first model is better!"
  },
  {
    "objectID": "slides/lec-4.html#section-13",
    "href": "slides/lec-4.html#section-13",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - Interpretation\n\n\n\nExercise 7.23\n\nfit the model in R; do you obtain the same estimates?\ndo parts a-d\ncompute the SST, SSR and \\(R^2\\) ‚Äúby hand‚Äù in R (do not use glance). You can use the command augment to compute the residuals.\n\n\n\nd <- usdata::county_2019\nm <- lm(. . .)\nm_augment <- augment(m)\nSST <- ...\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-4.html#outliers-in-regression",
    "href": "slides/lec-4.html#outliers-in-regression",
    "title": "Simple Linear Regression Models",
    "section": "Outliers in regression",
    "text": "Outliers in regression\nRemember, in a boxplot, outliers are observations far from the bulk of the data\n\nggplot(d) +\n  geom_boxplot(aes(hwy))\n\n\n\nIn the context of regression models, an outlier is an observation that falls far from the cloud of points"
  },
  {
    "objectID": "slides/lec-4.html#identifying-outliers",
    "href": "slides/lec-4.html#identifying-outliers",
    "title": "Simple Linear Regression Models",
    "section": "Identifying outliers",
    "text": "Identifying outliers\nIn the following scatterplot, we see two outliers\n\nggplot(d) +\n  geom_point(aes(cty, hwy))"
  },
  {
    "objectID": "slides/lec-4.html#section-14",
    "href": "slides/lec-4.html#section-14",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - outlier in regression\n\n\nExercise 7.25\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-4.html#outliers-leverage-and-influential-points",
    "href": "slides/lec-4.html#outliers-leverage-and-influential-points",
    "title": "Simple Linear Regression Models",
    "section": "Outliers, leverage and influential points",
    "text": "Outliers, leverage and influential points\n\noutliers: observations that fall far from the cloud of points;\nhigh leverage points: observations that fall horizontally away from the cloud of points;\ninfluential points: observations that influence the slope of the regression line;\n\n\n\nAll influential points are high leverage points.\nAll leverage points are outliers.\n(Venn Diagram)"
  },
  {
    "objectID": "slides/lec-4.html#section-15",
    "href": "slides/lec-4.html#section-15",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-4.html#section-16",
    "href": "slides/lec-4.html#section-16",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "Group exercise - outlier\n\n\nExercise 7.27\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-4.html#least-square-estimates-are-not-robust",
    "href": "slides/lec-4.html#least-square-estimates-are-not-robust",
    "title": "Simple Linear Regression Models",
    "section": "Least-square estimates are not robust",
    "text": "Least-square estimates are not robust\nIn regression, outliers have the potential to highly influence the least-square estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\n\n\n\n\n\nWarning\n\n\nLeast-square estimates are not robust to the presence of outliers.\n\n\n\n\nEstimates that are robust include (not covered in this class)\n\nleast absolute deviation estimates, which minimize the SAR (sum of absolute residuals) instead of the SSR\n\n\\[\nSAR = |e_1| + |e_2| + \\dots + |e_n|\n\\]\n\nBayesian estimates (STA360)"
  },
  {
    "objectID": "slides/lec-4.html#impact-of-outliers",
    "href": "slides/lec-4.html#impact-of-outliers",
    "title": "Simple Linear Regression Models",
    "section": "Impact of outliers",
    "text": "Impact of outliers\nLet us contaminate the data with an outlier (cty \\(=10\\) and hwy \\(=1000\\))\n\nd_contaminated <- select(d, cty, hwy) %>%\n  add_row(cty = 40, hwy = 500) # add outlier\narrange(d_contaminated, desc(hwy)) %>% slice(1:5)\n\n# A tibble: 5 x 2\n    cty   hwy\n  <dbl> <dbl>\n1    40   500\n2    33    44\n3    35    44\n4    29    41\n5    28    37"
  },
  {
    "objectID": "slides/lec-4.html#section-17",
    "href": "slides/lec-4.html#section-17",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "‚Ä¶and compare the two regression models.\n\nlm(hwy ~ cty, data = d)\n\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n\nlm(hwy ~ cty, data = d_contaminated)\n\n\nCall:\nlm(formula = hwy ~ cty, data = d_contaminated)\n\nCoefficients:\n(Intercept)          cty  \n    -33.841        3.498  \n\n\n\nThe slope estimate has almost tripled!"
  },
  {
    "objectID": "slides/lec-4.html#section-18",
    "href": "slides/lec-4.html#section-18",
    "title": "Simple Linear Regression Models",
    "section": "",
    "text": "ggplot(d_contaminated) +\n  geom_point(aes(cty, hwy)) +\n  geom_abline(intercept = -33.841, slope = 3.498, col = \"purple\")\n\n\nThe regression line not longer fits the data well."
  },
  {
    "objectID": "slides/lec-4.html#statistics-as-an-art---dealing-with-outliers",
    "href": "slides/lec-4.html#statistics-as-an-art---dealing-with-outliers",
    "title": "Simple Linear Regression Models",
    "section": "Statistics as an art - dealing with outliers",
    "text": "Statistics as an art - dealing with outliers\n\n\n\n\n\n\nDealing with outliers\n\n\nOutliers can unduly influence parameter estimates. How to deal with an outlier depends on why the observation stands out. Outliers can either be\n\nremoved\ncorrected\nignored"
  },
  {
    "objectID": "slides/lec-4.html#recap-1",
    "href": "slides/lec-4.html#recap-1",
    "title": "Simple Linear Regression Models",
    "section": "Recap",
    "text": "Recap\n\nsimple linear regression model\n\n\\[\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n\\]\n\nresiduals\nleast-square estimates\nparameter interpretation\nmodel comparison with \\(R^2\\)\noutliers\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-5.html#announcements",
    "href": "slides/lec-5.html#announcements",
    "title": "Multiple Linear Regression Models",
    "section": "Announcements",
    "text": "Announcements\n\nFind the last person you have not worked with\nPrediction project\n\nteams\ndeadlines: May 31 (model and presentation) and June 1 (report)"
  },
  {
    "objectID": "slides/lec-5.html#project---reading-in-the-data",
    "href": "slides/lec-5.html#project---reading-in-the-data",
    "title": "Multiple Linear Regression Models",
    "section": "Project - reading in the data",
    "text": "Project - reading in the data\n\nlibrary(readr)\nd <- read_csv(\"https://rmorsomme.github.io/website/projects/training_set.csv\")"
  },
  {
    "objectID": "slides/lec-5.html#recap",
    "href": "slides/lec-5.html#recap",
    "title": "Multiple Linear Regression Models",
    "section": "Recap",
    "text": "Recap\n\nsimple linear regression model\n\n\\[\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n\\]\n\nresiduals\nleast-square estimates\nparameter interpretation\nmodel comparison with \\(R^2\\)\noutliers"
  },
  {
    "objectID": "slides/lec-5.html#outline",
    "href": "slides/lec-5.html#outline",
    "title": "Multiple Linear Regression Models",
    "section": "Outline",
    "text": "Outline\n\nmultiple linear regression\ncategorical predictor\nfeature engineering\n\ntransformation\ncombination\ninteraction"
  },
  {
    "objectID": "slides/lec-5.html#the-mpg-data-set",
    "href": "slides/lec-5.html#the-mpg-data-set",
    "title": "Multiple Linear Regression Models",
    "section": "The mpg data set",
    "text": "The mpg data set\n\nd <- ggplot2::mpg\nhead(d, n = 4)\n\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n\n\nInstead of fitting a model with only cty or only displ, we could fit a linear regression model with both predictors!"
  },
  {
    "objectID": "slides/lec-5.html#linear-regression-with-2-predictors",
    "href": "slides/lec-5.html#linear-regression-with-2-predictors",
    "title": "Multiple Linear Regression Models",
    "section": "Linear regression with 2 predictors",
    "text": "Linear regression with 2 predictors\nThe model equation is\n\\[\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty} + \\beta_2 \\text{displ}\n\\]\nWe can find the least-square estimates (minimizing the SSR) with the command lm\n\nlm(hwy ~ cty + displ, data = d)\n\n\nCall:\nlm(formula = hwy ~ cty + displ, data = d)\n\nCoefficients:\n(Intercept)          cty        displ  \n    1.15145      1.32914     -0.03432  \n\n\nwhich give the following regression equation\n\\[\n\\text{hwy} \\approx 1.15 + 1.33 \\text{cty} - 0.034  \\text{displ}\n\\]"
  },
  {
    "objectID": "slides/lec-5.html#interpretation",
    "href": "slides/lec-5.html#interpretation",
    "title": "Multiple Linear Regression Models",
    "section": "Interpretation",
    "text": "Interpretation\n\nInterpreting \\(\\hat{\\beta}_1 = 1.33\\):\n\n‚ÄúKeeping displ constant, for each additional unit in cty, we would expect hwy to be higher, on average, by 1.33 units.‚Äù\n\n\n\n\nInterpreting \\(\\hat{\\beta}_2 = -0.034\\):\n\n‚ÄúKeeping cty constant, for each additional unit in displ, we would expect hwy to be lower, on average, by 0.034 unit.‚Äù\n\n\n\n\n\nInterpreting \\(\\hat{\\beta}_0 = 1.15\\):\n\n‚ÄúFor a car with cty and displ equal to 0, we would expect hwy to be 1.15.‚Äù\nmeaningless in this context."
  },
  {
    "objectID": "slides/lec-5.html#special-case-categorical-predictor",
    "href": "slides/lec-5.html#special-case-categorical-predictor",
    "title": "Multiple Linear Regression Models",
    "section": "Special case: categorical predictor",
    "text": "Special case: categorical predictor\nIn a regression model, categorical predictors are represented using indicator variables.\nTo represent a categorical predictor with \\(k\\) levels (categories), we use \\((k-1)\\) indicator variables1.\nWe cannot use \\(k\\) indicator variables for identifiability reasons (advanced topic)."
  },
  {
    "objectID": "slides/lec-5.html#including-drv",
    "href": "slides/lec-5.html#including-drv",
    "title": "Multiple Linear Regression Models",
    "section": "Including drv",
    "text": "Including drv\nFor instance, the categorical variable drv has \\(k=3\\) levels (4, f and r), so we can represent it with \\(3-1=2\\) indicator variables with the following model equation\n\\[\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{drv_f} + \\beta_2 \\text{drv_r}\n\\]\n\n\n\n\n\n\n\nTip\n\n\nFor a binary variable \\(k=2\\), so we only need \\(k-1=1\\) indicator variable.\nFor instance, to include the binary variable year_binary, we only added a single indicator variable to the model."
  },
  {
    "objectID": "slides/lec-5.html#using-categorical-predictors-in-r",
    "href": "slides/lec-5.html#using-categorical-predictors-in-r",
    "title": "Multiple Linear Regression Models",
    "section": "Using categorical predictors in R",
    "text": "Using categorical predictors in R\n\nlm(hwy ~ drv, data = d)\n\n\nCall:\nlm(formula = hwy ~ drv, data = d)\n\nCoefficients:\n(Intercept)         drvf         drvr  \n     19.175        8.986        1.825  \n\n\nThe model equation with the least-square estimates is\n\\[\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n\\]"
  },
  {
    "objectID": "slides/lec-5.html#interpreting-the-output",
    "href": "slides/lec-5.html#interpreting-the-output",
    "title": "Multiple Linear Regression Models",
    "section": "Interpreting the output",
    "text": "Interpreting the output\n\\[\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n\\]\nIf a new vehicle has a drv that is:\n\n4, then the two indicator variables drv_f and drv_r take the value 0, and the prediction is \\(\\widehat{hwy} = 19.175\\)\n\n\n\nf, then the indicator variables drv_f takes the value 1 and drv_r the value 0, and the prediction is \\(\\widehat{hwy} = 19.175 + 8.986 = 28.161\\)\n\n\n\n\nr, then the indicator variables drv_f takes the value 0 and drv_r the value 1, and the prediction is \\(\\widehat{hwy} = 19.175 + 1.825 = 21\\)\n\n\n\nThe level (category) 4 is called the reference (baseline) level."
  },
  {
    "objectID": "slides/lec-5.html#section",
    "href": "slides/lec-5.html#section",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "Group exercise - categorical predictor\n\n\n\nExercises 8.5, 8.7\nIn addition\n\nidentify the baseline level of the categorical variable in each model.\n\n\nlibrary(openintro)\nd <- openintro::births14\n\n\nfit the first model in R\n\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-5.html#fitting-a-larger-model",
    "href": "slides/lec-5.html#fitting-a-larger-model",
    "title": "Multiple Linear Regression Models",
    "section": "Fitting a larger model",
    "text": "Fitting a larger model\nLet us fit a model with cty, drv and disp\n\nm_larger <- lm(hwy ~ cty + drv + displ, data = d)\nm_larger\n\n\nCall:\nlm(formula = hwy ~ cty + drv + displ, data = d)\n\nCoefficients:\n(Intercept)          cty         drvf         drvr        displ  \n      3.424        1.157        2.158        2.360       -0.208  \n\n\nIts \\(R^2\\) is 0.938.\n\nglance(m_larger)$r.squared\n\n[1] 0.9384357\n\n\n\nUnsurprisingly, including additional predictors makes the regression line closer to the points \\(\\Rightarrow\\) residuals are smaller \\(\\Rightarrow\\) SSR is smaller \\(\\Rightarrow\\) \\(R^2\\) is larger."
  },
  {
    "objectID": "slides/lec-5.html#section-1",
    "href": "slides/lec-5.html#section-1",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "Group exercise - computing \\(R^2\\) by hand\n\n\n\nWhat is the formula for \\(R^2\\)?\nCompute \\(R^2\\) by hand in R (do not use glance). You can use the following code.\n\n\nd <- ggplot2::mpg\nm_larger <- lm(hwy ~ cty + drv + displ, data = d)\nm_augment <- augment(m_larger)\n\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-5.html#fitting-the-full2-model",
    "href": "slides/lec-5.html#fitting-the-full2-model",
    "title": "Multiple Linear Regression Models",
    "section": "Fitting the full1 model",
    "text": "Fitting the full1 model\n\nm_full <- lm(\n  hwy ~ manufacturer + displ + year + cyl + trans + drv + cty + fl + class,\n  data = d\n  )\n\nThanks to the additional predictors, the residuals are very small, making \\(R^2\\) close to \\(1\\).\n\nglance(m_full)$r.squared\n\n[1] 0.9773386\n\n\n\nLarge \\(R^2\\)\n\nseems great!\n‚Ä¶but we will see in the next lecture that this is not always a good sign.\n\n\nThis not exactly the full model since I do not include the variable model due to identifiability issues (technical point)."
  },
  {
    "objectID": "slides/lec-5.html#section-2",
    "href": "slides/lec-5.html#section-2",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "Group exercise - multiple linear regression\n\n\nExercise 8.9\n\nfit the model in R\nidentify the type of each variable\nidentify the baseline level of the categorical predictors\ndo parts a-d\n\n\nlibrary(openintro)\nd_birth <- openintro::births14\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-5.html#feature-engineering",
    "href": "slides/lec-5.html#feature-engineering",
    "title": "Multiple Linear Regression Models",
    "section": "Feature engineering",
    "text": "Feature engineering\nWe saw that adding predictors to the model seems to help.\nHowever, the variables included in the data set, e.g.¬†displ, year, etc, may not be the most useful predictors for hwy.\n\nFeature engineering refers to the creation of new predictors from the raw variables.\n\n\n\n\n\n\n\n\nTip\n\n\nThis is where your understanding of the data, scientific knowledge and experience make a big difference."
  },
  {
    "objectID": "slides/lec-5.html#nonlinearity",
    "href": "slides/lec-5.html#nonlinearity",
    "title": "Multiple Linear Regression Models",
    "section": "Nonlinearity",
    "text": "Nonlinearity\nConsider the relation between the predictor displ and the response hwy\n\nggplot(d) +\n  geom_point(aes(displ, hwy))\n\n\n\nThe relationis not exactly linear."
  },
  {
    "objectID": "slides/lec-5.html#modeling-nonlinearity",
    "href": "slides/lec-5.html#modeling-nonlinearity",
    "title": "Multiple Linear Regression Models",
    "section": "Modeling nonlinearity",
    "text": "Modeling nonlinearity\nLet us include the predictor \\(\\dfrac{1}{\\text{displ}}\\) to capture this nonlinear relation.\nHere is the model equation\n\\[\n\\text{hwy} \\approx \\beta_0+ \\beta_1 \\text{displ} + \\beta_2 \\dfrac{1}{\\text{displ}}\n\\]\nThe least-square coefficient estimates are\n\nd_transf <- mutate(d, displ_inv = 1/displ)\nm <- lm(hwy ~ displ + displ_inv, data = d_transf)\nm\n\n\nCall:\nlm(formula = hwy ~ displ + displ_inv, data = d_transf)\n\nCoefficients:\n(Intercept)        displ    displ_inv  \n    12.2601      -0.2332      36.1456"
  },
  {
    "objectID": "slides/lec-5.html#section-3",
    "href": "slides/lec-5.html#section-3",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "And the regression lines captures the nonlinear relation.\n\naugment(m) %>%\n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))"
  },
  {
    "objectID": "slides/lec-5.html#the-trees-data-set",
    "href": "slides/lec-5.html#the-trees-data-set",
    "title": "Multiple Linear Regression Models",
    "section": "The trees data set",
    "text": "The trees data set\n\n\n\nMeasurements of the diameter, height and volume of timber in 31 felled black cherry trees.\nNote that the diameter (in inches) is erroneously labelled Girth in the data\nThe diameter (Girth) is measured at 4 ft 6 in above the ground.\nSource: Atkinson, A. C. (1985) Plots, Transformations and Regression. Oxford University Press."
  },
  {
    "objectID": "slides/lec-5.html#combining-variables-1",
    "href": "slides/lec-5.html#combining-variables-1",
    "title": "Multiple Linear Regression Models",
    "section": "Combining variables",
    "text": "Combining variables\nTransforming a variable may be helpful ‚Ä¶but we can go a step further!\n\nWe can construct new predictors by combining existing variables.\n\n\nConsider the trees dataset\n\nd_tree <- datasets::trees\nhead(d_tree, n = 4)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n\n\nwhere Girth indicates the tree diameter (twice the radius) in inches."
  },
  {
    "objectID": "slides/lec-5.html#geomtric-considerations",
    "href": "slides/lec-5.html#geomtric-considerations",
    "title": "Multiple Linear Regression Models",
    "section": "Geomtric considerations",
    "text": "Geomtric considerations\nSuppose we want to estimate Volume (expensive to measure) from Girth and Height (cheap to measure).\n\nYou might decide to approximate the shape of a tree with a shape that is between a cylinder and a cone.\nFrom geometry, we know that the volume of a cylinder is\n\\[\nV = \\pi r^2 h\n\\]\nand that of a cone is\n\\[\nV = \\frac{1}{3} \\pi r^2 h\n\\]"
  },
  {
    "objectID": "slides/lec-5.html#section-4",
    "href": "slides/lec-5.html#section-4",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "This suggests approximating the volume of a tree with the following model\n\\[\n\\text{Volume} = \\beta_1 \\left[ \\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right] = \\beta_1X\n\\]\nwhere\n\n\\(\\beta_1\\) is an unknown parameter that we expect to be between \\(\\pi\\) (pure cylinder) and \\(\\frac{1}{3}\\pi\\) (pure cone)\n\\(X = \\left[\\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right]\\) is our new predictor."
  },
  {
    "objectID": "slides/lec-5.html#section-5",
    "href": "slides/lec-5.html#section-5",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "To accomplish this, we simply create a new variable corresponding to \\(\\left[\\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right]\\).\nBefore doing that, we just need to transform Girth into feet to ensure that all variables have the same units.\n\nd_tree_comb <- d_tree %>%\n  mutate(\n    Girth_ft    = Girth / 12,\n    radius      = Girth_ft / 2,\n    r_squared   = radius^2,\n    r2h = r_squared * Height\n  )\nhead(d_tree_comb, n = 3)\n\n  Girth Height Volume  Girth_ft    radius r_squared      r2h\n1   8.3     70   10.3 0.6916667 0.3458333 0.1196007 8.372049\n2   8.6     65   10.3 0.7166667 0.3583333 0.1284028 8.346181\n3   8.8     63   10.2 0.7333333 0.3666667 0.1344444 8.470000"
  },
  {
    "objectID": "slides/lec-5.html#section-6",
    "href": "slides/lec-5.html#section-6",
    "title": "Multiple Linear Regression Models",
    "section": "",
    "text": "Our model does not include an intercept. To exclude the intercept from the model, I use -1 in the lm command.\n\nm_engineered <- lm(Volume ~ r2h - 1, data = d_tree_comb)\nm_engineered\n\n\nCall:\nlm(formula = Volume ~ r2h - 1, data = d_tree_comb)\n\nCoefficients:\n  r2h  \n1.214  \n\n\n\nThe coefficient estimate is between the two anticipated bounds \\(\\pi=3.14\\) and \\(\\frac{\\pi}{3}=1.047\\)!\n\n\n\n\n\n\n\n\nWarning\n\n\nWhen we do not include the intercept in a model, \\(R^2\\) measures something different and should therefore not be interpreted in the usual way.\n\n\n\n\nglance(m_engineered)$r.squared\n\n[1] 0.9950219"
  },
  {
    "objectID": "slides/lec-5.html#comparison-with-the-full-model",
    "href": "slides/lec-5.html#comparison-with-the-full-model",
    "title": "Multiple Linear Regression Models",
    "section": "Comparison with the full model",
    "text": "Comparison with the full model\nAlthough \\(R^2\\) has a different meaning when there is no intercept, it can be still used for comparison1.\n\n\nm_full <- lm(Volume ~ Girth + Height -1, data = d_tree)\nglance(m_full)$r.squared\n\n[1] 0.9696913\n\nglance(m_engineered)$r.squared\n\n[1] 0.9950219\n\n\nOur new variable \\((\\dfrac{\\text{Girth}}{2})^2 * \\text{Height}\\) improves the model!\n\n\nNote that the full model has two predictors, while our geometry-based model has only a single predictor!\n\n\n\n\n\n\nFeature engineering\n\n\nA carefully constructed predictor can do a better job than multiple raw predictors!\n\n\n\n\nas long as both models exclude the intercept"
  },
  {
    "objectID": "slides/lec-5.html#predicting-amateur-jogging-races-duration",
    "href": "slides/lec-5.html#predicting-amateur-jogging-races-duration",
    "title": "Multiple Linear Regression Models",
    "section": "Predicting amateur jogging races duration",
    "text": "Predicting amateur jogging races duration\n\n\nSuppose you are interested in predicting the average run time (duration) of amateur jogging races.\nTwo variables that impact the duration are (i) the distance of the race, and (ii) the weather.\nFor simplicity, we measure the weather as either good (nice weather) or bad (rain, heat wave, etc)."
  },
  {
    "objectID": "slides/lec-5.html#fixed-effect-weather",
    "href": "slides/lec-5.html#fixed-effect-weather",
    "title": "Multiple Linear Regression Models",
    "section": "Fixed effect weather?",
    "text": "Fixed effect weather?\nTe full model is\n\\[\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad}\n\\]\nwhere \\(\\beta_1\\) indicate the effect of an additional miles on the expected duration and \\(\\beta_2\\) the effect of bad weather.\nNote that the effect of weather is fixed in this model, say ‚Äú\\(+5\\) minutes‚Äù if \\(\\hat{\\beta}_2 = 5\\).\n\nIs this reasonable? No!\n\n\n\\(\\Rightarrow\\) the effect of weather should vary with distance. For shorter races, bad weather may add only 2 or 3 minutes, while for longer races, bad weather may increase the average duration by 10 or 15 minutes."
  },
  {
    "objectID": "slides/lec-5.html#model-equation-with-interaction",
    "href": "slides/lec-5.html#model-equation-with-interaction",
    "title": "Multiple Linear Regression Models",
    "section": "Model equation with interaction",
    "text": "Model equation with interaction\nWe capture such pattern using an interaction term.\n\\[\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad} + \\beta_3 \\text{weather_bad}*\\text{distance}\n\\]\n\nWhen the weather is good, the equation simplifies to\n\n\\[\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 0 + \\beta_3 0*\\text{distance} = \\beta_0 + \\beta_1 \\text{distance}\n\\]\n\nWhen the weather is bad, the equation simplifies to\n\n\\[\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 1 + \\beta_3 1*\\text{distance} = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3) \\text{distance}\n\\]"
  },
  {
    "objectID": "slides/lec-5.html#interpreting-interactions",
    "href": "slides/lec-5.html#interpreting-interactions",
    "title": "Multiple Linear Regression Models",
    "section": "Interpreting interactions",
    "text": "Interpreting interactions\n\nWhen the weather is good, the slope estimate is \\(\\hat{\\beta}_1\\), meaning that the effect of an additional miles on the average duration is \\(\\hat{\\beta}_1\\).\nWhen the weather is bad, the slope estimate is \\(\\hat{\\beta}_1+\\hat{\\beta}_3\\), meaning that the effect of an additional miles on the average duration is \\(\\hat{\\beta}_1+\\hat{\\beta}_3\\) (not \\(\\hat{\\beta}_1\\)).\n\n\n\n\n\n\n\nInteractions\n\n\nThe effect of the distance depends on the weather. Similarly, the effect of the weather depends on the distance.\n\\(\\Rightarrow\\) the two variables interact."
  },
  {
    "objectID": "slides/lec-5.html#recap-2",
    "href": "slides/lec-5.html#recap-2",
    "title": "Multiple Linear Regression Models",
    "section": "Recap",
    "text": "Recap\n\n\nsimple linear regression model \\[\nY \\approx \\beta_0 + \\beta_1 X\n\\]\nmultiple linear regression model \\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p\n\\]\ncategorical predictor\n\n\\((k-1)\\) indicator variables\n\nfeature engineering\n\ntransforming variables\ncombining variables\ninteractions\n\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-6.html#announcements",
    "href": "slides/lec-6.html#announcements",
    "title": "Model Selection",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 4 is due on Thursday.\nHomework 5 will be due after the prediction project on Sunday June 5.\nPrediction project\n\nFind your teammate here.\nIf you have not started yet, start now. You can do already do 80% of the project.\nYou will have the tools to do the remaining 20% after this lecture."
  },
  {
    "objectID": "slides/lec-6.html#recap",
    "href": "slides/lec-6.html#recap",
    "title": "Model Selection",
    "section": "Recap",
    "text": "Recap\n\n\nsimple linear regression model \\[\nY \\approx \\beta_0 + \\beta_1 X\n\\]\nmultiple linear regression model \\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p\n\\]\ncategorical predictor\n\n\\((k-1)\\) indicator variables\n\nfeature engineering\n\ntransforming variables\ncombining variables\ninteractions"
  },
  {
    "objectID": "slides/lec-6.html#outline",
    "href": "slides/lec-6.html#outline",
    "title": "Model Selection",
    "section": "Outline",
    "text": "Outline\n\nOverfitting\n\nlimitations of \\(R^2\\) for model selection\n\nModel selection through\n\noverall criteria (adjusted-\\(R^2\\), AIC, BIC)\npredictive performance (holdout method, cross-validation)\nstepwise procedure (forward, backward)"
  },
  {
    "objectID": "slides/lec-6.html#competing-models-1",
    "href": "slides/lec-6.html#competing-models-1",
    "title": "Model Selection",
    "section": "Competing models",
    "text": "Competing models\n\nEarlier, we saw that adding the predictor \\(\\dfrac{1}{\\text{displ}}\\) gave a better fit.\nLet us see if the same idea work with the trees dataset."
  },
  {
    "objectID": "slides/lec-6.html#a-nonlinear-association",
    "href": "slides/lec-6.html#a-nonlinear-association",
    "title": "Model Selection",
    "section": "A nonlinear association?",
    "text": "A nonlinear association?\nSuppose we want to predict volume using only the variable girth.\n\nset.seed(0)\nd_tree <- datasets::trees %>% \n  mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5)) # add some random noise to girth for illustration purpose\nggplot(d_tree) +\n  geom_point(aes(Girth, Volume))\n\n\nOne could argue that there is a slight nonlinear association"
  },
  {
    "objectID": "slides/lec-6.html#function-to-compute-r2",
    "href": "slides/lec-6.html#function-to-compute-r2",
    "title": "Model Selection",
    "section": "Function to compute \\(R^2\\)",
    "text": "Function to compute \\(R^2\\)\n\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}"
  },
  {
    "objectID": "slides/lec-6.html#starting-simple",
    "href": "slides/lec-6.html#starting-simple",
    "title": "Model Selection",
    "section": "Starting simple‚Ä¶",
    "text": "Starting simple‚Ä¶\nWe start with the simple model\n\\[\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth}\n\\]\n\nm1 <- lm(Volume ~ Girth, data = d_tree)\ncompute_R2(m1)\n\n[1] 0.933"
  },
  {
    "objectID": "slides/lec-6.html#section",
    "href": "slides/lec-6.html#section",
    "title": "Model Selection",
    "section": "",
    "text": "R codeRegression line\n\n\n\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m1, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.44        5.51\n2  8.44        5.52\n3  8.44        5.52\n4  8.44        5.53\n5  8.44        5.53\n6  8.44        5.54\n\n\n\n\n\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))"
  },
  {
    "objectID": "slides/lec-6.html#taking-it-up-a-notch",
    "href": "slides/lec-6.html#taking-it-up-a-notch",
    "title": "Model Selection",
    "section": "‚Ä¶taking it up a notch‚Ä¶",
    "text": "‚Ä¶taking it up a notch‚Ä¶\nTo capture the nonlinear association between Girth and Volume, we consider the predictor \\(\\text{Girth}^2\\).\n\\[\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2\n\\]\nThe command to fit this model is\n\nd_tree2 <- mutate(d_tree, Girth2 = Girth^2)\nm2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)\ncompute_R2(m2)\n\n[1] 0.958\n\n\n\n\\(R^2\\) has increased! It went from 0.933 (model 1) to 0.958."
  },
  {
    "objectID": "slides/lec-6.html#section-1",
    "href": "slides/lec-6.html#section-1",
    "title": "Model Selection",
    "section": "",
    "text": "R codeRegression line\n\n\n\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2)\nVolume_pred <- predict(m2, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n\n# A tibble: 6 x 3\n  Girth Girth2 Volume_pred\n  <dbl>  <dbl>       <dbl>\n1  8.44   71.2        11.5\n2  8.44   71.2        11.5\n3  8.44   71.2        11.5\n4  8.44   71.2        11.5\n5  8.44   71.2        11.5\n6  8.44   71.3        11.5\n\n\n\n\n\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))"
  },
  {
    "objectID": "slides/lec-6.html#taking-it-up-another-notch",
    "href": "slides/lec-6.html#taking-it-up-another-notch",
    "title": "Model Selection",
    "section": "‚Ä¶taking it up another notch‚Ä¶",
    "text": "‚Ä¶taking it up another notch‚Ä¶\nLet us consider the predictor \\(\\text{Girth}^3\\).\n\\[\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\beta_3 \\text{girth}^3\n\\]\n\nd_tree3 <- mutate(d_tree, Girth2 = Girth^2, Girth3 = Girth^3)\nm3 <- lm(Volume ~ Girth + Girth2 + Girth3, data = d_tree3)\ncompute_R2(m3)\n\n[1] 0.96\n\n\n\n\\(R^2\\) has increased! It went from 0.958 (model 2) to 0.96."
  },
  {
    "objectID": "slides/lec-6.html#section-2",
    "href": "slides/lec-6.html#section-2",
    "title": "Model Selection",
    "section": "",
    "text": "R codeRegression line\n\n\n\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2, Girth3 = Girth^3)\nVolume_pred <- predict(m3, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n\n# A tibble: 6 x 4\n  Girth Girth2 Girth3 Volume_pred\n  <dbl>  <dbl>  <dbl>       <dbl>\n1  8.44   71.2   601.        9.83\n2  8.44   71.2   601.        9.83\n3  8.44   71.2   601.        9.83\n4  8.44   71.2   601.        9.84\n5  8.44   71.2   601.        9.84\n6  8.44   71.3   602.        9.84\n\n\n\n\n\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))"
  },
  {
    "objectID": "slides/lec-6.html#before-taking-things-to-the-extreme",
    "href": "slides/lec-6.html#before-taking-things-to-the-extreme",
    "title": "Model Selection",
    "section": "‚Ä¶before taking things to the extreme",
    "text": "‚Ä¶before taking things to the extreme\nWhat if we also include the predictors \\(\\text{Girth}^4, \\text{Girth}^5, \\dots, \\text{Girth}^{k}\\) for some larger number \\(k\\)?\nThe following R command fits the model with \\(k=81\\), that is,\n\\[\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\dots + \\beta_{81} \\text{girth}^{81}\n\\]\n\nm_extreme <- lm(Volume ~ poly(Girth, degree = 81, raw = TRUE), data = d_tree)\ncompute_R2(m_extreme)\n\n[1] 0.981\n\n\n\\(R^2\\) has again increased! It went from 0.96 (model 3) to 0.981."
  },
  {
    "objectID": "slides/lec-6.html#a-limitation-of-r2",
    "href": "slides/lec-6.html#a-limitation-of-r2",
    "title": "Model Selection",
    "section": "A limitation of \\(R^2\\)",
    "text": "A limitation of \\(R^2\\)\nAs we keep adding predictors, \\(R^2\\) will always increase.\n\nadditional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.\n\n\nIs the model m_extreme a good model?\n\n\n\n\n\n\nNote\n\n\nWe want to learn about the relation between Volume and Girth present in the population (not the sample).\nA good model should accurately represent the population."
  },
  {
    "objectID": "slides/lec-6.html#section-3",
    "href": "slides/lec-6.html#section-3",
    "title": "Model Selection",
    "section": "",
    "text": "R codeRegression lineRegression line (zoomed)\n\n\n\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.0001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m_extreme, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.44        10.3\n2  8.44        10.4\n3  8.44        10.4\n4  8.44        10.4\n5  8.44        10.5\n6  8.44        10.5\n\n\n\n\n\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred)) +\n  ylim(0, 90)"
  },
  {
    "objectID": "slides/lec-6.html#overfitting",
    "href": "slides/lec-6.html#overfitting",
    "title": "Model Selection",
    "section": "Overfitting",
    "text": "Overfitting\nThe model m_extreme overfits the data.\nA model overfits the data when it fits the sample extremely well but does a poor job for new data.\n\nRemember that we want to learn about the population, not the sample!"
  },
  {
    "objectID": "slides/lec-6.html#r2",
    "href": "slides/lec-6.html#r2",
    "title": "Model Selection",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nWe saw that \\(R^2\\) keeps increasing as we add predictors.\n\\[\nR^2 = 1 - \\dfrac{SSR}{SST}\n\\]\n\n\\(R^2\\) can therefore not be used to identify models that overfit the data."
  },
  {
    "objectID": "slides/lec-6.html#adjusted-r2",
    "href": "slides/lec-6.html#adjusted-r2",
    "title": "Model Selection",
    "section": "Adjusted-\\(R^2\\)",
    "text": "Adjusted-\\(R^2\\)\nThe adjusted-\\(R^2\\) is similar to \\(R^2\\), but penalizes large models:\n\\[\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\\dfrac{n-1}{n-p-1}\n\\]\nwhere \\(p\\) corresponds to the number of predictors (model size)."
  },
  {
    "objectID": "slides/lec-6.html#section-4",
    "href": "slides/lec-6.html#section-4",
    "title": "Model Selection",
    "section": "",
    "text": "The adjusted-\\(R^2\\) therefore balances goodness of fit and parsimony:\n\nThe ratio \\(\\dfrac{SSR}{SST}\\) favors model with good fit (like \\(R^2\\))\nThe ratio \\(\\dfrac{n-1}{n-k-1}\\) favors parsimonious models\n\n\nThe model with the highest adjusted-\\(R^2\\) typically provides a good fit without overfitting."
  },
  {
    "objectID": "slides/lec-6.html#section-5",
    "href": "slides/lec-6.html#section-5",
    "title": "Model Selection",
    "section": "",
    "text": "Group exercise - a function for the adjusted-\\(R^2\\)\n\n\nConsider the function compute_R2 which takes a model as input and returns its \\(R^2\\) rounded to the 3rd decimal.\n\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}\n\nAdapt the function so that it computes the adjusted-\\(R^2\\) instead of \\(R^2\\). Give an appropriate name to the new function.\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-6.html#aic-and-bic",
    "href": "slides/lec-6.html#aic-and-bic",
    "title": "Model Selection",
    "section": "AIC and BIC",
    "text": "AIC and BIC\nTwo other popular overall criteria that balance goodness of fit (small SSR) and parsimony (small \\(p\\)) are\n\nthe Akaike Information Criterion (AIC)\nthe Bayesian Information Criterion (BIC)\n\n\nThe formula for AIC and BIC are respectively\n\\[\nAIC = 2p - \\text{GoF}, \\qquad BIC = \\ln(n)p- \\text{GoF}\n\\]\nwhere \\(\\text{GoF}\\) measures the goodness of fit of the model1.\n\nThe exact formula for \\(\\text{GoF}\\) is beyond the scope of this class."
  },
  {
    "objectID": "slides/lec-6.html#aic-and-bic-in-practice",
    "href": "slides/lec-6.html#aic-and-bic-in-practice",
    "title": "Model Selection",
    "section": "AIC and BIC in practice",
    "text": "AIC and BIC in practice\n\n\n\n\n\n\nWarning\n\n\nUnlike the adjusted-\\(R^2\\), smaller is better for the AIC and BIC.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe BIC penalizes the number of parameters \\(p\\) more strongly than AIC. BIC will therefore tend to favor smaller models (models with fewer parameters)."
  },
  {
    "objectID": "slides/lec-6.html#computing-aic-and-bic",
    "href": "slides/lec-6.html#computing-aic-and-bic",
    "title": "Model Selection",
    "section": "Computing AIC and BIC",
    "text": "Computing AIC and BIC\nAIC and BIC are easily accessible in R with the command glance.\n\nrbind(glance(m1), glance(m2), glance(m3), glance(m_extreme))\n\n# A tibble: 4 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.933         0.930  4.34     401.  1.57e-18     1  -88.5  183.  187.\n2     0.958         0.955  3.50     317.  5.93e-20     2  -81.3  171.  176.\n3     0.960         0.955  3.48     214.  6.39e-19     3  -80.5  171.  178.\n4     0.981         0.960  3.28      46.3 2.32e- 9    16  -68.5  173.  199.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\nIn this case, BIC favor m2, AIC cannot decide between m2 and m3 and the adjusted-\\(R^2\\) (wrongly) favors m_extreme.\n\nFor AIC and BIC, smaller is better!\nBIC favors more parsimonious models than AIC."
  },
  {
    "objectID": "slides/lec-6.html#limitations-of-the-previous-criteria",
    "href": "slides/lec-6.html#limitations-of-the-previous-criteria",
    "title": "Model Selection",
    "section": "Limitations of the previous criteria",
    "text": "Limitations of the previous criteria\nThe adjusted-\\(R^2\\), AIC and BIC all try to balance\n\ngoodness of fit\nparsimony\n\n\nThey achieve this balance by favoring models with small SSR while penalizing models with larger \\(p\\).\n\n\n‚Ä¶But\n\nThe form of the penalty for \\(p\\) is somewhat arbitrary, e.g.¬†AIC versus BIC.\nThe adjusted-\\(R^2\\) failed to penalize m_extreme, although it was clearly overfitting the data."
  },
  {
    "objectID": "slides/lec-6.html#predictive-performance",
    "href": "slides/lec-6.html#predictive-performance",
    "title": "Model Selection",
    "section": "Predictive performance",
    "text": "Predictive performance\nInstead of using these criteria, we could look for the model with the best predictions performance.\nThat is, the model that makes predictions for new observations that are the closest to the true values.\n\nWe will learn two approaches to accomplish this\n\nthe holdout method\ncross-validation"
  },
  {
    "objectID": "slides/lec-6.html#the-holdout-method",
    "href": "slides/lec-6.html#the-holdout-method",
    "title": "Model Selection",
    "section": "The holdout method",
    "text": "The holdout method\nThe holdout method is a simple method to evaluate the predictive performance of a model.\n\n\nRandomly partition your sample into two sets: a training set (typically 2/3 of the sample) and a test set (the remaining 1/3).\n\n\n\n\nFit your model to the training set.\n\n\n\n\nEvaluate the prediction accuracy of the model on the test set.\n\n\n\nNote that the test set consists of new observations for the model.\n\n\n\\(\\Rightarrow\\) Select the model with the best prediction accuracy in step 3."
  },
  {
    "objectID": "slides/lec-6.html#section-6",
    "href": "slides/lec-6.html#section-6",
    "title": "Model Selection",
    "section": "",
    "text": "The holdout method.Source: IMS"
  },
  {
    "objectID": "slides/lec-6.html#step-1-training-and-test-sets",
    "href": "slides/lec-6.html#step-1-training-and-test-sets",
    "title": "Model Selection",
    "section": "Step 1: training and test sets",
    "text": "Step 1: training and test sets\nThe following R function splits a sample into a training and a test set\n\nconstruct_training_test <- function(sample, prop_training = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n * prop_training)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,    1 : n_training )\n  sample_test     <- slice(sample_random, - (1 : n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}"
  },
  {
    "objectID": "slides/lec-6.html#section-7",
    "href": "slides/lec-6.html#section-7",
    "title": "Model Selection",
    "section": "",
    "text": "SampleTraining setTest set\n\n\n\nd_tree # entire sample\n\n       Girth Height Volume\n1   8.931477     70   10.3\n2   8.436883     65   10.3\n3   9.464900     63   10.2\n4  11.136215     72   16.4\n5  10.907321     81   18.8\n6  10.030025     83   19.7\n7  10.535716     66   15.6\n8  10.852640     75   18.2\n9  11.097116     80   22.6\n10 12.402327     75   19.9\n11 11.681797     79   24.2\n12 11.000495     76   21.0\n13 10.826171     76   21.4\n14 11.555269     69   21.3\n15 11.850392     75   19.1\n16 12.694245     74   22.2\n17 13.026112     85   33.8\n18 12.854039     86   27.4\n19 13.917842     71   25.7\n20 13.181231     64   24.9\n21 13.887866     78   34.5\n22 14.388698     80   31.7\n23 14.566668     74   36.3\n24 16.402095     72   38.3\n25 16.271447     77   42.6\n26 17.551804     81   55.4\n27 18.042885     82   55.7\n28 17.554523     80   58.3\n29 17.357700     80   51.5\n30 18.023363     80   51.0\n31 20.482147     87   77.0\n\n\n\n\n\nset.seed(0) # set the seed of the random number generator to 0\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntraining_set\n\n       Girth Height Volume\n1  11.555269     69   21.3\n2  16.271447     77   42.6\n3  11.136215     72   16.4\n4  10.535716     66   15.6\n5   8.931477     70   10.3\n6   8.436883     65   10.3\n7  14.566668     74   36.3\n8  11.681797     79   24.2\n9  20.482147     87   77.0\n10 12.854039     86   27.4\n11 13.917842     71   25.7\n12 18.042885     82   55.7\n13 12.402327     75   19.9\n14 18.023363     80   51.0\n15 13.887866     78   34.5\n16 17.554523     80   58.3\n17 11.097116     80   22.6\n18 10.907321     81   18.8\n19 14.388698     80   31.7\n20 11.850392     75   19.1\n21 11.000495     76   21.0\n\n\n\n\n\ntest_set <- training_test_sets[[\"test\"]]\ntest_set\n\n      Girth Height Volume\n1  10.82617     76   21.4\n2  13.02611     85   33.8\n3  17.55180     81   55.4\n4  10.85264     75   18.2\n5  10.03002     83   19.7\n6  13.18123     64   24.9\n7  17.35770     80   51.5\n8   9.46490     63   10.2\n9  16.40209     72   38.3\n10 12.69424     74   22.2"
  },
  {
    "objectID": "slides/lec-6.html#step-2-fit-the-model-to-the-training-set",
    "href": "slides/lec-6.html#step-2-fit-the-model-to-the-training-set",
    "title": "Model Selection",
    "section": "Step 2: fit the model to the training set",
    "text": "Step 2: fit the model to the training set\nWe simply fit our regression model to the training set.\n\nm1 <- lm(Volume ~ Girth, data = training_set)"
  },
  {
    "objectID": "slides/lec-6.html#step-3-evaluate-the-prediction-accuracy-on-the-test-set",
    "href": "slides/lec-6.html#step-3-evaluate-the-prediction-accuracy-on-the-test-set",
    "title": "Model Selection",
    "section": "Step 3: Evaluate the prediction accuracy on the test set",
    "text": "Step 3: Evaluate the prediction accuracy on the test set\nTo evaluate the prediction accuracy, we start by computing the predictions for the observations in the test set.\n\nVolume_hat <- predict(m1, test_set)\n\n\nA good model will make predictions that are closed to the true values of the response variable."
  },
  {
    "objectID": "slides/lec-6.html#sum-of-squared-errors",
    "href": "slides/lec-6.html#sum-of-squared-errors",
    "title": "Model Selection",
    "section": "Sum of squared errors",
    "text": "Sum of squared errors\nA good measure of prediction accuracy is the sum of squared errors (SSE).\n\\[\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2\n\\]\nSmall SSE is better.\n\nVolume <- test_set$Volume\nsum((Volume - Volume_hat)^2)\n\n[1] 211.4543"
  },
  {
    "objectID": "slides/lec-6.html#mean-squared-error",
    "href": "slides/lec-6.html#mean-squared-error",
    "title": "Model Selection",
    "section": "Mean squared error",
    "text": "Mean squared error\nIn practice, the (root) mean sum of squared errors ((R)MSE) is often used.\n\\[\nMSE = \\dfrac{SSE}{m} = \\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}\n\\]\n\\[\nRMSE = \\sqrt{\\dfrac{SSE}{m}} = \\sqrt{\\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}}\n\\] Again, small (R)MSE is better."
  },
  {
    "objectID": "slides/lec-6.html#section-8",
    "href": "slides/lec-6.html#section-8",
    "title": "Model Selection",
    "section": "",
    "text": "SSE <- sum ((Volume - Volume_hat)^2)\nMSE <- mean((Volume - Volume_hat)^2)\nRMSE <- sqrt(MSE)\nSSE; MSE; RMSE\n\n[1] 211.4543\n\n\n[1] 21.14543\n\n\n[1] 4.598416\n\n\n\n\n\n\n\n\n(R)MSE\n\n\nThe advantage of (R)MSE over SSE is that we can compare models that are evaluated with test sets of different sizes."
  },
  {
    "objectID": "slides/lec-6.html#selecting-a-model",
    "href": "slides/lec-6.html#selecting-a-model",
    "title": "Model Selection",
    "section": "Selecting a model",
    "text": "Selecting a model\nApply steps 2 and 3 on different models.\n\nuse the same training and test sets from step 1 for the different models\n\n\n\\(\\Rightarrow\\) Choose the model with the lowest (R)MSE!\nThis model has the best predictive accuracy."
  },
  {
    "objectID": "slides/lec-6.html#function-to-compute-the-rmse",
    "href": "slides/lec-6.html#function-to-compute-the-rmse",
    "title": "Model Selection",
    "section": "Function to compute the RMSE",
    "text": "Function to compute the RMSE\n\ncompute_RMSE <- function(test_set, model){\n  \n  y     <- test_set$Volume # truth\n  y_hat <- predict(model, test_set) # predicted value\n  \n  E    <- y - y_hat\n  MSE  <- mean(E^2)\n  RMSE <- sqrt(MSE)\n  \n  return(RMSE)\n  \n}"
  },
  {
    "objectID": "slides/lec-6.html#section-9",
    "href": "slides/lec-6.html#section-9",
    "title": "Model Selection",
    "section": "",
    "text": "# Step 1 - partition sample into a training set and a test set\n# already done\n\n# Step 2 - fit models to training set\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3 - evaluate models on test set\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2)\ncompute_RMSE(test_set, m3) # best\ncompute_RMSE(test_set, m48)\n\n[1] 4.598416\n[1] 4.07333\n[1] 3.957379\n[1] 29.97533\n\n\nWe select m3."
  },
  {
    "objectID": "slides/lec-6.html#limitation-of-the-holdout-method",
    "href": "slides/lec-6.html#limitation-of-the-holdout-method",
    "title": "Model Selection",
    "section": "Limitation of the holdout method",
    "text": "Limitation of the holdout method\nThe previous analysis was conducted with set.seed(0).\n\nNote that models m2 and m3 were pretty close.\nCould we have obtained a different result with a different seed?"
  },
  {
    "objectID": "slides/lec-6.html#section-10",
    "href": "slides/lec-6.html#section-10",
    "title": "Model Selection",
    "section": "",
    "text": "# Step 1\nset.seed(1) # new seed\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntest_set <- training_test_sets[[\"test\"]]\n\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2) # best\ncompute_RMSE(test_set, m3)\ncompute_RMSE(test_set, m48)\n\n[1] 6.097541\n[1] 4.248172\n[1] 10.37698\n[1] 103753769"
  },
  {
    "objectID": "slides/lec-6.html#the-test-set-matters",
    "href": "slides/lec-6.html#the-test-set-matters",
    "title": "Model Selection",
    "section": "The test set matters!",
    "text": "The test set matters!\nWith set.seed(1), the holdout method indicates that m2 is the best model.\n\nm3, our previous best model, is worse than m1!\n\n\nA drawback of the holdout method is that the test set matters a lot.\n\nRepeating steps 2 and 3 with a different partition in step 1 may give different results."
  },
  {
    "objectID": "slides/lec-6.html#section-11",
    "href": "slides/lec-6.html#section-11",
    "title": "Model Selection",
    "section": "",
    "text": "Group exercise - limitation of the holdout method\n\n\nCopy and paste the following code in your R session. Try a few different seed numbers for step 1. Keep track of which model has the lowest RMSE. Is m1 ever the best model?\nYou can safely ignore the warning messages.\n\n# Setup\nset.seed(0) # do not change this seed number\nd_tree <- datasets::trees %>% mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5))\ncompute_RMSE <- function(test_set, model){\n  \n  y     <- test_set$Volume\n  y_hat <- predict(model, test_set)\n  \n  E    <- y - y_hat\n  MSE  <- mean(E^2)\n  RMSE <- sqrt(MSE)\n  \n  return(RMSE)\n  \n}\nconstruct_training_test <- function(sample, prop_training = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n * prop_training)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,    1 : n_training )\n  sample_test     <- slice(sample_random, - (1 : n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}\n\n# Step 1\nset.seed(2) # try a few different seed numbers\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntest_set <- training_test_sets[[\"test\"]]\n\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2)\ncompute_RMSE(test_set, m3)\ncompute_RMSE(test_set, m48)\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-6.html#cross-validation",
    "href": "slides/lec-6.html#cross-validation",
    "title": "Model Selection",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation (CV) is a natural generalization of the holdout method:\n\nrepeat the holdout method many times with different partitions in step 1."
  },
  {
    "objectID": "slides/lec-6.html#section-12",
    "href": "slides/lec-6.html#section-12",
    "title": "Model Selection",
    "section": "",
    "text": "randomly partition the sample into \\(k\\) folds of equal size. \\(k\\) is typically \\(5\\) or \\(10\\).\n\n\nRepeat the following steps \\(k\\) times\n\nLet fold 1 be the test set and the remaining folds be the training set.\nFit the model to the training set (like the holdout method)\nEvaluate the prediction accuracy of the model on the test set (like the holdout method)\nGo back to step 1, let the next fold be the test set.\n\n\n\n\\(\\Rightarrow\\) Select the model with the best overall prediction accuracy in step 3."
  },
  {
    "objectID": "slides/lec-6.html#section-13",
    "href": "slides/lec-6.html#section-13",
    "title": "Model Selection",
    "section": "",
    "text": "Source: towardsdatascience"
  },
  {
    "objectID": "slides/lec-6.html#step-0",
    "href": "slides/lec-6.html#step-0",
    "title": "Model Selection",
    "section": "Step 0",
    "text": "Step 0\n\nset.seed(0)\n\n# Setup\nn <- nrow(d_tree)\nn_folds <- 10\n\n# Fold assignment\nfolds <- c(rep(1:n_folds, n %/% n_folds), seq_along(n %% n_folds))\n# you do not need to understand the details of the previous line\n# just note that it creates folds of (almost) the same size\nfolds\n\n [1]  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5\n[26]  6  7  8  9 10  1\n\n# Step 1\nd_tree_fold <- d_tree %>%\n  slice_sample(n = n) %>% # shuffle the rows\n  mutate(fold = folds)\nhead(d_tree_fold)\n\n      Girth Height Volume fold\n1 11.555269     69   21.3    1\n2 16.271447     77   42.6    2\n3 11.136215     72   16.4    3\n4 10.535716     66   15.6    4\n5  8.931477     70   10.3    5\n6  8.436883     65   10.3    6"
  },
  {
    "objectID": "slides/lec-6.html#steps-1-4-setup",
    "href": "slides/lec-6.html#steps-1-4-setup",
    "title": "Model Selection",
    "section": "Steps 1-4 ‚Äì Setup",
    "text": "Steps 1-4 ‚Äì Setup\n\ncreate_empty_RMSE <- function(){\n  tibble(\n    fold     = numeric(),\n    rmse_m1  = numeric(), \n    rmse_m2  = numeric(), \n    rmse_m3  = numeric(), \n    rmse_m48 = numeric()\n  )\n}\n\nRMSE <- create_empty_RMSE()\nRMSE\n\n# A tibble: 0 x 5\n# ... with 5 variables: fold <dbl>, rmse_m1 <dbl>, rmse_m2 <dbl>,\n#   rmse_m3 <dbl>, rmse_m48 <dbl>"
  },
  {
    "objectID": "slides/lec-6.html#steps-1-4",
    "href": "slides/lec-6.html#steps-1-4",
    "title": "Model Selection",
    "section": "Steps 1-4",
    "text": "Steps 1-4\n\nRMSE <- create_empty_RMSE()\n\nfor(i in 1 : n_folds){\n  \n  # Step 1\n  training_set <- filter(d_tree_fold, fold != i)\n  test_set     <- filter(d_tree_fold, fold == i)\n  \n  # Step 2\n  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\n  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\n  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\n  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n  \n  # Step 3\n  rmse_m1  <- compute_RMSE(test_set, m1 )\n  rmse_m2  <- compute_RMSE(test_set, m2 )\n  rmse_m3  <- compute_RMSE(test_set, m3 )\n  rmse_m48 <- compute_RMSE(test_set, m48)\n  \n  RMSE <- RMSE %>% \n    add_row(fold = i, rmse_m1, rmse_m2, rmse_m3, rmse_m48)\n  \n}"
  },
  {
    "objectID": "slides/lec-6.html#model-selection-with-rmse",
    "href": "slides/lec-6.html#model-selection-with-rmse",
    "title": "Model Selection",
    "section": "Model selection with RMSE",
    "text": "Model selection with RMSE\nWhich model has the lowest RMSE across all folds?\n\nRMSE\n\n# A tibble: 10 x 5\n    fold rmse_m1 rmse_m2 rmse_m3   rmse_m48\n   <dbl>   <dbl>   <dbl>   <dbl>      <dbl>\n 1     1    5.07    3.50    3.54       4.25\n 2     2    2.89    2.34    1.95      10.8 \n 3     3    4.78    5.02    5.11       5.54\n 4     4    2.82    3.49    3.45       3.15\n 5     5    1.67    2.99    2.67      72.5 \n 6     6    6.38    4.45    5.38     412.  \n 7     7    3.52    2.89    2.83       4.73\n 8     8    1.37    2.01    1.77       3.90\n 9     9    7.49    3.14    3.92 2481852.  \n10    10    5.60    4.53    4.27       4.47\n\n\n\nsummarise_all(RMSE, mean)\n\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1   5.5    4.16    3.44    3.49  248237.\n\nsummarise_all(RMSE, sum ) # what the book uses\n\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1    55    41.6    34.4    34.9 2482374.\n\n\nm2 has slightly better overall RMSE."
  },
  {
    "objectID": "slides/lec-6.html#section-14",
    "href": "slides/lec-6.html#section-14",
    "title": "Model Selection",
    "section": "",
    "text": "Group exercise - cross-validation\n\n\nExercise 25.9, 25.11, 25.12\nIn part c (25.9 and 25.11) , replace left and right, by top and bottom.\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-6.html#short-podcast-on-cross-validation",
    "href": "slides/lec-6.html#short-podcast-on-cross-validation",
    "title": "Model Selection",
    "section": "Short podcast on cross-validation",
    "text": "Short podcast on cross-validation\nCross-validation is a central technique in machine learning.\nStill not totally clear how it works? Check out this podcast to hear more about this important topic.\n\nThese two other podcasts on muliple linear regression and \\(R^2\\) are also excellent."
  },
  {
    "objectID": "slides/lec-6.html#taking-things-to-the-extreme-loocv",
    "href": "slides/lec-6.html#taking-things-to-the-extreme-loocv",
    "title": "Model Selection",
    "section": "Taking things to the extreme: LOOCV",
    "text": "Taking things to the extreme: LOOCV\nSet \\(k=n\\).\n\nthat is, use \\(n\\) folds, each of size \\(1\\).\nEach test set will therefore consist of a single observation and the corresponding training set of the remaining \\(n-1\\) observations."
  },
  {
    "objectID": "slides/lec-6.html#loocv",
    "href": "slides/lec-6.html#loocv",
    "title": "Model Selection",
    "section": "LOOCV",
    "text": "LOOCV\n\nRMSE <- create_empty_RMSE()\n\n# Step 0\nn <- nrow(d_tree)\nd_tree_loocv <- mutate(d_tree, folds = 1 : n) # every observation belongs to a different fold \n\nfor(i in 1:n){\n  \n  # Step 1\n  training_set <- filter(d_tree_loocv, folds != i)\n  test_set     <- filter(d_tree_loocv, folds == i)\n  \n  # Step 2\n  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\n  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\n  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\n  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n  \n  # Step 3\n  rmse_m1  <- compute_RMSE(test_set, m1 )\n  rmse_m2  <- compute_RMSE(test_set, m2 )\n  rmse_m3  <- compute_RMSE(test_set, m3 )\n  rmse_m48 <- compute_RMSE(test_set, m48)\n  \n  RMSE <- RMSE %>%\n    add_row(fold = i, rmse_m1, rmse_m2, rmse_m3, rmse_m48)\n  \n}\n\nsummarize_all(RMSE, mean)\n\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1    16    3.64    3.11    3.12  142697."
  },
  {
    "objectID": "slides/lec-6.html#stepwise-procedures",
    "href": "slides/lec-6.html#stepwise-procedures",
    "title": "Model Selection",
    "section": "Stepwise procedures",
    "text": "Stepwise procedures\nNot my favorite method, but it is widely used.\n\n\nAkin the ‚Äúthrow cooked spaghetti to the wall and see what sticks‚Äù technique.\nThis is what you do when you are clueless about what variables may be good predictors\n\n‚Ä¶but then you should probably collaborate with a scientific expert.\n\n\n\n\nTwo types of stepwise procedures: (i) forward selection and (ii) backward elimination."
  },
  {
    "objectID": "slides/lec-6.html#forward-selection",
    "href": "slides/lec-6.html#forward-selection",
    "title": "Model Selection",
    "section": "Forward selection",
    "text": "Forward selection\n\nChoose an overall criterion that balances GoF (small SSR) and parsimony (small \\(p\\))\n\\(\\Rightarrow\\) adjusted-\\(R^2\\), AIC or BIC\n\n\n\nStart with the empty model \\(Y \\approx \\beta_0\\), i.e.¬†the model with no predictor. This is our current model\nFit all possible models with one additional predictor.\n\n\n\n\nCompute the criterion, say the AIC, of each of these models.\nIdentify the model with the smallest AIC. This is our candidate model.\n\n\n\n\nIf the AIC of the candidate model is smaller (better) than the AIC of the current model, the candidate model becomes the current model. Go back to step 3.\nIf the AIC of the candidate model is larger than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model."
  },
  {
    "objectID": "slides/lec-6.html#section-15",
    "href": "slides/lec-6.html#section-15",
    "title": "Model Selection",
    "section": "",
    "text": "Group exercise - forward selection\n\n\nExercise 8.13. In addition, fit the candidate model (a single model) in R with the command lm.\n\nlibrary(openintro)\nd <- openintro::births14\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-6.html#backward-elimination",
    "href": "slides/lec-6.html#backward-elimination",
    "title": "Model Selection",
    "section": "Backward elimination",
    "text": "Backward elimination\nSimilar to forward selection, except that\n\nWe start with the full model.\nWe remove one predictor at a time,\nuntil removing any predictor makes the model worse."
  },
  {
    "objectID": "slides/lec-6.html#section-16",
    "href": "slides/lec-6.html#section-16",
    "title": "Model Selection",
    "section": "",
    "text": "Group exercise - backward elimination\n\n\nExercise 8.11. In addition, fit the current and candidate models in R with the command lm.\n\nlibrary(openintro)\nd <- openintro::births14\n\n\n\n\n\n\n\n05:00"
  },
  {
    "objectID": "slides/lec-6.html#section-17",
    "href": "slides/lec-6.html#section-17",
    "title": "Model Selection",
    "section": "",
    "text": "Forward selection versus backward elimination\n\n\nNote that forward selection and backward elimination need not agree; they may select different models!\n. . .\n\nWhat to do in that case? Nobody knows."
  },
  {
    "objectID": "slides/lec-6.html#parsimony",
    "href": "slides/lec-6.html#parsimony",
    "title": "Model Selection",
    "section": "Parsimony",
    "text": "Parsimony\nTo obtain a parsimonious model\n\nuse your knowledge of the subject to carefuly choose which variables to consider and construct new ones, and\nimplement a model selection procedure."
  },
  {
    "objectID": "slides/lec-6.html#recap-2",
    "href": "slides/lec-6.html#recap-2",
    "title": "Model Selection",
    "section": "Recap",
    "text": "Recap\n\n\nOverfitting\n\nlimitations of \\(R^2\\) for model selection\n\nModel selection through\n\n\noverall criteria (adjusted-\\(R^2\\), AIC, BIC)\npredictive performance (holdout method, cross-validation)\nstepwise procedure (forward, backward)\n\n\n\n\n\nYou are now fully equipped for the prediction project!\nüçÄ Good luck! üçÄ\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-7.html#announcements",
    "href": "slides/lec-7.html#announcements",
    "title": "Logistic Regression",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "slides/lec-7.html#recap",
    "href": "slides/lec-7.html#recap",
    "title": "Logistic Regression",
    "section": "Recap",
    "text": "Recap\n\n\nSimple and multiple lrm\n\nnumerical response\n\nmodel selection\n\noverall criterion\npredictive performance\nstepwise procedures"
  },
  {
    "objectID": "slides/lec-7.html#outline",
    "href": "slides/lec-7.html#outline",
    "title": "Logistic Regression",
    "section": "Outline",
    "text": "Outline\n\nModeling a binary response\nLogistic regression\nOne predictor\nMultiple predictors"
  },
  {
    "objectID": "slides/lec-7.html#example-stanford-university-heart-transplant-study",
    "href": "slides/lec-7.html#example-stanford-university-heart-transplant-study",
    "title": "Logistic Regression",
    "section": "Example ‚Äì Stanford University Heart Transplant Study",
    "text": "Example ‚Äì Stanford University Heart Transplant Study\n\n\n\nGoals: to determine whether an experimental heart transplant program increases lifespan\nobservations: patients\nresponse: survival after 5 years (binary)\npredictors: age, prior surgery, waiting time for transplant."
  },
  {
    "objectID": "slides/lec-7.html#data",
    "href": "slides/lec-7.html#data",
    "title": "Logistic Regression",
    "section": "Data",
    "text": "Data\n\nd <- heart_transplant\nd\n\n# A tibble: 103 x 8\n      id acceptyear   age survived survtime prior transplant  wait\n   <int>      <int> <int> <fct>       <int> <fct> <fct>      <int>\n 1    15         68    53 dead            1 no    control       NA\n 2    43         70    43 dead            2 no    control       NA\n 3    61         71    52 dead            2 no    control       NA\n 4    75         72    52 dead            2 no    control       NA\n 5     6         68    54 dead            3 no    control       NA\n 6    42         70    36 dead            3 no    control       NA\n 7    54         71    47 dead            3 no    control       NA\n 8    38         70    41 dead            5 no    treatment      5\n 9    85         73    47 dead            5 no    control       NA\n10     2         68    51 dead            6 no    control       NA\n# ... with 93 more rows"
  },
  {
    "objectID": "slides/lec-7.html#effect-of-age-on-survival",
    "href": "slides/lec-7.html#effect-of-age-on-survival",
    "title": "Logistic Regression",
    "section": "Effect of age on survival",
    "text": "Effect of age on survival\n\ngeom_pointgeom_jitter\n\n\n\nd %>%\n  ggplot(aes(x = age, y = survived)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nd %>%\n  ggplot(aes(x = age, y = survived)) +\n  geom_jitter(     # add random noise to point location\n    width = 0,     # horizontal jitter\n    height = 0.05, # vertical jitter\n    alpha = 0.5    # transparency\n    )"
  },
  {
    "objectID": "slides/lec-7.html#binary-response",
    "href": "slides/lec-7.html#binary-response",
    "title": "Logistic Regression",
    "section": "Binary response",
    "text": "Binary response\n\nTechnical problem: levels of the response are labels\n\ncan‚Äôt fit a regression model to words, only to numbers\n\nuse mutate to create a binary variable (either 0 or 1)\n\n\nd <- d %>%\n  mutate(is_alive = if_else(survived == \"alive\", 1, 0))"
  },
  {
    "objectID": "slides/lec-7.html#section",
    "href": "slides/lec-7.html#section",
    "title": "Logistic Regression",
    "section": "",
    "text": "ggplot(data = d, aes(x = age, y = is_alive)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.5)"
  },
  {
    "objectID": "slides/lec-7.html#linear-regression",
    "href": "slides/lec-7.html#linear-regression",
    "title": "Logistic Regression",
    "section": "Linear regression?",
    "text": "Linear regression?\nIntuitively, age should be a good predictor of survival.\n\nLet us fit a linear regression model\n\\[\n\\text{is_alive} \\approx \\beta_0 + \\beta_1 age\n\\]\n\nm_lrm <- lm(is_alive ~ age, data = d)"
  },
  {
    "objectID": "slides/lec-7.html#section-1",
    "href": "slides/lec-7.html#section-1",
    "title": "Logistic Regression",
    "section": "",
    "text": "ggplot(data = d, aes(x = age, y = is_alive)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.5) +\n  geom_abline(intercept = 0.80955, slope = -0.01205)"
  },
  {
    "objectID": "slides/lec-7.html#problems-with-linear-regression",
    "href": "slides/lec-7.html#problems-with-linear-regression",
    "title": "Logistic Regression",
    "section": "Problems with linear regression",
    "text": "Problems with linear regression\n\nThe response is not continuous\n\nunlike fuel consumption, tree volume or newborn weight\n\nnonsensical predictions, even for reasonable value of age."
  },
  {
    "objectID": "slides/lec-7.html#section-2",
    "href": "slides/lec-7.html#section-2",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - limitation of linear regression\n\n\nWhat is the prediction for a 70-year-old patient?\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-7.html#glms",
    "href": "slides/lec-7.html#glms",
    "title": "Logistic Regression",
    "section": "GLMs",
    "text": "GLMs\nGeneralized linear models (GLMs ‚Äì STA310) extend the linear regression framework to settings where the response variable is restricted:\n\n\nlogistic regression with binary response\nmultinomial regression with nominal (categorical) response, e.g.¬†voting in multiparty systems\nordinal regression with ordinal (categorical) response, e.g.¬†course letter grade\npoisson regression with count response, e.g.¬†number of children\ngamma regression with positive response variable, e.g.¬†mpg, insurance costs\nbeta regression with continuous response between 0 and 1 (percentage), e.g.¬†cancer rates in counties"
  },
  {
    "objectID": "slides/lec-7.html#logistic-regression-1",
    "href": "slides/lec-7.html#logistic-regression-1",
    "title": "Logistic Regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nExample of a generalized linear model\nWe‚Äôll focus on implementation and interpretation\nUseful for inference project (not the prediction project)\n\nLogistic regression: model the probability of success \\(p\\) based on a set of predictors \\(X_1, \\dots, X_p\\)."
  },
  {
    "objectID": "slides/lec-7.html#binary-outcome-and-probability",
    "href": "slides/lec-7.html#binary-outcome-and-probability",
    "title": "Logistic Regression",
    "section": "Binary outcome and probability",
    "text": "Binary outcome and probability\nLet \\(Y_i\\) denote the response of person \\(i\\).\nThe logistic regression model assumes that\n\\[\nY_i =\n\\begin{cases}\n1 \\text{ (success)}, \\text{with probability } p_i, \\\\\n0 \\text{ (failure)}, \\text{with probability } 1-p_i.\n\\end{cases}\n\\]\nwhere \\(p_i\\) denotes the probability of success (survival) of person \\(i\\)."
  },
  {
    "objectID": "slides/lec-7.html#modeling-a-probability-with-linear-regression",
    "href": "slides/lec-7.html#modeling-a-probability-with-linear-regression",
    "title": "Logistic Regression",
    "section": "Modeling a probability with linear regression?",
    "text": "Modeling a probability with linear regression?\nWe saw that we cannot model \\(p\\) with a lrm (\\(p \\approx \\beta_0 + \\beta_1\\text{age}\\))\n\n\\(0\\le p_i\\le1\\)\n‚Ä¶but \\(\\beta_0 + \\beta_1 \\text{age}\\) may be negative or larger than 1!\n\nin fact, the regression line extends infinitely in either either direction\n\n\n\n\n\n\n\n\nWarning\n\n\nBinary variables should not be modeled using linear regression!"
  },
  {
    "objectID": "slides/lec-7.html#modeling-a-probability-with-logistic-regression",
    "href": "slides/lec-7.html#modeling-a-probability-with-logistic-regression",
    "title": "Logistic Regression",
    "section": "Modeling a probability with logistic regression",
    "text": "Modeling a probability with logistic regression\nIntuitively, an older patient should have a smaller probability of surviving.\n\\(\\Rightarrow\\) We want a model that associates a smaller probability \\(p\\) for a patient with a larger year variable.\n\n\\(\\Rightarrow\\) We need to find a way to go from age (could be any number) to \\(p\\) (between 0 and 1)"
  },
  {
    "objectID": "slides/lec-7.html#the-logit-transformation",
    "href": "slides/lec-7.html#the-logit-transformation",
    "title": "Logistic Regression",
    "section": "The logit transformation",
    "text": "The logit transformation\nConsider the logit transformation\n\\[\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n\\]\nwhere \\(\\mu\\) can be any number."
  },
  {
    "objectID": "slides/lec-7.html#section-3",
    "href": "slides/lec-7.html#section-3",
    "title": "Logistic Regression",
    "section": "",
    "text": "tibble(mu = seq(-7.5, 7.5, by = 0.01)) %>%\n  mutate(p = exp(mu)/(1+exp(mu))) %>%\n  ggplot() + \n  geom_line(aes(x = mu, y = p))\n\n\nNote that \\(\\dfrac{e^{\\mu}}{1+e^{\\mu}}\\) is bounded between 0 and 1.\nMoreover, larger values of \\(\\mu\\) will give larger \\(p\\), and smaller \\(\\mu\\) will give smaller \\(p\\)."
  },
  {
    "objectID": "slides/lec-7.html#section-4",
    "href": "slides/lec-7.html#section-4",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - logit transformation\n\n\n\nWhat value of \\(p\\) corresponds to\n\n\\(\\mu = 0.5\\)?\n\\(\\mu = 2\\)?\n\\(\\mu = -2\\)?\n\nWhat value of \\(\\mu\\) gives\n\n\\(p=0.5\\)?\n\\(p=0.9\\)?\n\\(p=0.1\\)?\n\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-7.html#modeling-mu",
    "href": "slides/lec-7.html#modeling-mu",
    "title": "Logistic Regression",
    "section": "Modeling \\(\\mu\\)",
    "text": "Modeling \\(\\mu\\)\nWe can now simply model \\(\\mu\\) using a simple lrm with age\n\\[\n\\mu \\approx \\beta_0 + \\beta_1 \\text{age}\n\\]\nand then transform \\(\\mu\\) into \\(p\\) using the logit transformation\n\\[\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n\\]\n\nPutting everything together gives the logistic regression model\n\\[\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}} \\approx \\dfrac{e^{\\beta_0 + \\beta_1 \\text{age}}}{1+e^{\\beta_0 + \\beta_1 \\text{age}}}\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#section-5",
    "href": "slides/lec-7.html#section-5",
    "title": "Logistic Regression",
    "section": "",
    "text": "Alternative formulation\n\n\nThe formulation\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) \\approx \\beta_0 +\\beta_1 \\text{age}\n\\]\nis also widely to describe the logistic regression model. This formulation is equivalent to that used on the previous slide."
  },
  {
    "objectID": "slides/lec-7.html#maximum-likelihood-estimates",
    "href": "slides/lec-7.html#maximum-likelihood-estimates",
    "title": "Logistic Regression",
    "section": "Maximum likelihood estimates",
    "text": "Maximum likelihood estimates\nHow are the unknown coefficients \\(\\beta_0\\) and \\(\\beta_1\\) estimated?\nWhen we fit a logistic regression model with R, the so-called maximum likelihood estimates (MLE1) are returned.\nMLE are extremely popular in statistics."
  },
  {
    "objectID": "slides/lec-7.html#glm",
    "href": "slides/lec-7.html#glm",
    "title": "Logistic Regression",
    "section": "glm",
    "text": "glm\nWe fit a logistic regression model in R with the command glm (not lm)\n\nm <- glm( # glm, not lm\n  is_alive ~ age, \n  family = binomial, # logistic regression\n  data = d\n  )\nm\n\n\nCall:  glm(formula = is_alive ~ age, family = binomial, data = d)\n\nCoefficients:\n(Intercept)          age  \n    1.56438     -0.05847  \n\nDegrees of Freedom: 102 Total (i.e. Null);  101 Residual\nNull Deviance:      120.5 \nResidual Deviance: 113.7    AIC: 117.7"
  },
  {
    "objectID": "slides/lec-7.html#r-output",
    "href": "slides/lec-7.html#r-output",
    "title": "Logistic Regression",
    "section": "R output",
    "text": "R output\nFor the moment simply focus on\n\ncoefficient estimates\nAIC (no (adjusted-) \\(R^2\\)) for model selection\n\n\n\n\n\n\n\nglm not lm\n\n\nTo fit a logistic regression model in R, use the command glm (not lm) with the argument family = binomial."
  },
  {
    "objectID": "slides/lec-7.html#interpretation",
    "href": "slides/lec-7.html#interpretation",
    "title": "Logistic Regression",
    "section": "Interpretation",
    "text": "Interpretation\nA positive coefficient estimate indicates that a higher value of the predictor is associated with a higher probability of success; and vice-versa for a smaller value.\nIn our case, the coefficient estimate for age (-0.058) is negative, so the model indicates that older participants have a smaller probability of surviving."
  },
  {
    "objectID": "slides/lec-7.html#visualizing-the-regression-curve",
    "href": "slides/lec-7.html#visualizing-the-regression-curve",
    "title": "Logistic Regression",
    "section": "Visualizing the regression curve",
    "text": "Visualizing the regression curve\n\nd_augm <- augment(m, type.predict = \"response\")\nd_augm # .fitted is equivalent to p_hat \n\n# A tibble: 103 x 8\n   is_alive   age .fitted .resid .std.resid   .hat .sigma .cooksd\n      <dbl> <int>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>   <dbl>\n 1        0    53   0.177 -0.625     -0.630 0.0156   1.06 0.00174\n 2        0    43   0.279 -0.809     -0.813 0.0106   1.06 0.00210\n 3        0    52   0.186 -0.642     -0.646 0.0147   1.06 0.00173\n 4        0    52   0.186 -0.642     -0.646 0.0147   1.06 0.00173\n 5        0    54   0.169 -0.608     -0.614 0.0166   1.06 0.00175\n 6        0    36   0.368 -0.958     -0.967 0.0181   1.06 0.00548\n 7        0    47   0.234 -0.731     -0.735 0.0111   1.06 0.00174\n 8        0    41   0.303 -0.850     -0.855 0.0115   1.06 0.00257\n 9        0    47   0.234 -0.731     -0.735 0.0111   1.06 0.00174\n10        0    51   0.195 -0.659     -0.663 0.0138   1.06 0.00172\n# ... with 93 more rows"
  },
  {
    "objectID": "slides/lec-7.html#section-6",
    "href": "slides/lec-7.html#section-6",
    "title": "Logistic Regression",
    "section": "",
    "text": "d_augm %>%\n  ggplot(aes(x = age)) +\n  geom_jitter(aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +\n  geom_line(aes(y = .fitted), col = \"maroon\", size = 2) # .fitted is equivalent to p_hat"
  },
  {
    "objectID": "slides/lec-7.html#extending-the-regression-curve",
    "href": "slides/lec-7.html#extending-the-regression-curve",
    "title": "Logistic Regression",
    "section": "Extending the regression curve",
    "text": "Extending the regression curve\n\n#artificial data with larger range (0 to 100)\nd_artificial <- tibble(age = seq(0, 100, by = 0.1))\np_hat <- predict(m, newdata = d_artificial, type = \"response\")\n\nd_artificial <- mutate(d_artificial, p_hat = p_hat)\nd_artificial\n\n# A tibble: 1,001 x 2\n     age p_hat\n   <dbl> <dbl>\n 1   0   0.827\n 2   0.1 0.826\n 3   0.2 0.825\n 4   0.3 0.824\n 5   0.4 0.824\n 6   0.5 0.823\n 7   0.6 0.822\n 8   0.7 0.821\n 9   0.8 0.820\n10   0.9 0.819\n# ... with 991 more rows"
  },
  {
    "objectID": "slides/lec-7.html#section-7",
    "href": "slides/lec-7.html#section-7",
    "title": "Logistic Regression",
    "section": "",
    "text": "ggplot(mapping = aes(x = age)) +\n  geom_jitter(data = d, aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +\n  geom_line(data = d_artificial, aes(y = p_hat), col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-7.html#multiple-logistic-regression",
    "href": "slides/lec-7.html#multiple-logistic-regression",
    "title": "Logistic Regression",
    "section": "Multiple logistic regression",
    "text": "Multiple logistic regression\nBased on what we know about multiple lrm, it is easy to extend the previous framework to multiple logistic regression:\n\\[\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n\\]\nwhere\n\\[\n\\mu \\approx \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#implementation-in-r",
    "href": "slides/lec-7.html#implementation-in-r",
    "title": "Logistic Regression",
    "section": "Implementation in R",
    "text": "Implementation in R\n\nm2 <- glm( # glm, not lm\n  is_alive ~ age + transplant, \n  family = binomial, # logistic regression\n  data = d\n  )\nm2\n\n\nCall:  glm(formula = is_alive ~ age + transplant, family = binomial, \n    data = d)\n\nCoefficients:\n        (Intercept)                  age  transplanttreatment  \n            0.97311             -0.07632              1.82316  \n\nDegrees of Freedom: 102 Total (i.e. Null);  100 Residual\nNull Deviance:      120.5 \nResidual Deviance: 103.9    AIC: 109.9"
  },
  {
    "objectID": "slides/lec-7.html#section-8",
    "href": "slides/lec-7.html#section-8",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - interpretation\n\n\nWhat is the interpretation of the coefficient estimates for age and transplant?\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-7.html#section-9",
    "href": "slides/lec-7.html#section-9",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - interpretation\n\n\nExercise 9.5\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-7.html#section-10",
    "href": "slides/lec-7.html#section-10",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - implementation\n\n\nExercise 9.3\nYou do not need to do parts a and b.\nSimply fit the two models in R. The possum data used in this exercise can be found in the openintro R package. You should obtain the same coefficient estimates as in the book, though with an opposite sign.\nSave these models; you will need them later!\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-7.html#artificial-data",
    "href": "slides/lec-7.html#artificial-data",
    "title": "Logistic Regression",
    "section": "Artificial data",
    "text": "Artificial data\n\nd_artificial <- expand.grid(\n  age = seq(0, 100, by = 0.1),\n  transplant = c(\"treatment\", \"control\")\n  ) %>%\n  as_tibble() %>%\n  arrange(age)\nd_artificial\n\n# A tibble: 2,002 x 2\n     age transplant\n   <dbl> <fct>     \n 1   0   treatment \n 2   0   control   \n 3   0.1 treatment \n 4   0.1 control   \n 5   0.2 treatment \n 6   0.2 control   \n 7   0.3 treatment \n 8   0.3 control   \n 9   0.4 treatment \n10   0.4 control   \n# ... with 1,992 more rows"
  },
  {
    "objectID": "slides/lec-7.html#visualization",
    "href": "slides/lec-7.html#visualization",
    "title": "Logistic Regression",
    "section": "Visualization",
    "text": "Visualization\n\np_hat <- predict(m2, d_artificial, type = \"response\")\nd_artificial %>%\n  mutate(p_hat = p_hat) %>%\n  ggplot() +\n  geom_line(aes(x = age, y = p_hat, col = transplant))"
  },
  {
    "objectID": "slides/lec-7.html#prediction",
    "href": "slides/lec-7.html#prediction",
    "title": "Logistic Regression",
    "section": "Prediction",
    "text": "Prediction\n\nd_augm <- augment(m2, type.predict = \"response\") %>%\n  mutate(is_alive_hat = if_else(.fitted < 0.5, 0, 1)) %>%\n  select(is_alive, age, transplant, .fitted, is_alive_hat)\nd_augm\n\n# A tibble: 103 x 5\n   is_alive   age transplant .fitted is_alive_hat\n      <dbl> <int> <fct>        <dbl>        <dbl>\n 1        0    53 control     0.0443            0\n 2        0    43 control     0.0904            0\n 3        0    52 control     0.0476            0\n 4        0    52 control     0.0476            0\n 5        0    54 control     0.0412            0\n 6        0    36 control     0.145             0\n 7        0    47 control     0.0682            0\n 8        0    41 treatment   0.418             0\n 9        0    47 control     0.0682            0\n10        0    51 control     0.0512            0\n# ... with 93 more rows"
  },
  {
    "objectID": "slides/lec-7.html#confusion-matrix",
    "href": "slides/lec-7.html#confusion-matrix",
    "title": "Logistic Regression",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nd_augm %>%\n  select(is_alive, is_alive_hat) %>%\n  table()\n\n        is_alive_hat\nis_alive  0  1\n       0 71  4\n       1 20  8\n\n\nThe model got \\(71+8=79\\) observations right out of \\(71+4+20+8=103\\); so its accuracy is\n\\[\n\\frac{79}{103} \\approx 77\\%\n\\]\nTo estimate the prediction accuracy on new data, simply use the holdout method or cross-validation."
  },
  {
    "objectID": "slides/lec-7.html#model-selection",
    "href": "slides/lec-7.html#model-selection",
    "title": "Logistic Regression",
    "section": "Model selection",
    "text": "Model selection\n\nAIC, BIC (not adjusted-\\(R^2\\))\nthe holdout method using prediction accuracy (not RMSE)\ncross-validation using prediction accuracy (not RMSE)\n\n\n\n\n\n\n\nAIC and BIC\n\n\nWith AIC and BIC, lower is better!\n\\[\nAIC = 2p - \\text{Goodness of fit}, \\qquad BIC = p\\ln(n) - \\text{Goodness of fit}\n\\]"
  },
  {
    "objectID": "slides/lec-7.html#section-11",
    "href": "slides/lec-7.html#section-11",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - model comparison with AIC\n\n\nGo back to the two models you fitted for exercise 9.3. What are their respective AIC? Which model is better?\nSimilarly, compare the AIC of the simple (age) and multiple (age + implant) logistic regression models for the heart transplant study. Which model is better?"
  },
  {
    "objectID": "slides/lec-7.html#section-12",
    "href": "slides/lec-7.html#section-12",
    "title": "Logistic Regression",
    "section": "",
    "text": "Group exercise - stepwise model selection with AIC\n\n\nExercise 9.7\n\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-8.html#announcements-presentations",
    "href": "slides/lec-8.html#announcements-presentations",
    "title": "Principles of Inference",
    "section": "Announcements ‚Äì presentations",
    "text": "Announcements ‚Äì presentations\n\nPresentations are < 5 minutes\nQA ‚Äì ask a question!\nDiscussion"
  },
  {
    "objectID": "slides/lec-8.html#presentations",
    "href": "slides/lec-8.html#presentations",
    "title": "Principles of Inference",
    "section": "Presentations",
    "text": "Presentations"
  },
  {
    "objectID": "slides/lec-8.html#announcements",
    "href": "slides/lec-8.html#announcements",
    "title": "Principles of Inference",
    "section": "Announcements",
    "text": "Announcements\n\nHomework 5 due Sunday\nWednesday‚Äôs schedule: 3:30-4:15 lecture; 4:15-5:45 (Roy‚Äôs OH)"
  },
  {
    "objectID": "slides/lec-8.html#recap-1st-half-of-course",
    "href": "slides/lec-8.html#recap-1st-half-of-course",
    "title": "Principles of Inference",
    "section": "Recap ‚Äì 1st half of course",
    "text": "Recap ‚Äì 1st half of course\n\nTypes of data and studies\nVisualization and numerical summaries\nRegression models\n\nlinear regression\nlogistic regression\nmodel selection"
  },
  {
    "objectID": "slides/lec-8.html#types-of-data",
    "href": "slides/lec-8.html#types-of-data",
    "title": "Principles of Inference",
    "section": "Types of data",
    "text": "Types of data\n\nBreakdown of variables into their respective types.Source: IMS"
  },
  {
    "objectID": "slides/lec-8.html#outline-2nd-half-of-course",
    "href": "slides/lec-8.html#outline-2nd-half-of-course",
    "title": "Principles of Inference",
    "section": "Outline ‚Äì 2nd half of course",
    "text": "Outline ‚Äì 2nd half of course\n\nStatistical inference\n\nproportions\nmeans\nlinear and logistic regression"
  },
  {
    "objectID": "slides/lec-8.html#outline",
    "href": "slides/lec-8.html#outline",
    "title": "Principles of Inference",
    "section": "Outline",
    "text": "Outline\n\nStatistical inference\nFive cases\nHypothesis test\nConfidence interval\nA first glimpse of modern statistics"
  },
  {
    "objectID": "slides/lec-8.html#population-parameters",
    "href": "slides/lec-8.html#population-parameters",
    "title": "Principles of Inference",
    "section": "Population parameters",
    "text": "Population parameters\nWe want to learn about some (unknown) parameter of some population of interest from a (small) sample of observations\n\n\nExamples of parameters: proportion of vegetarian among Duke students, average weight gained by US adults during the Covid-19 pandemic, etc.\n\n\n\n\nIn the remainder of the course, we will always assume that we have a random sample of the population."
  },
  {
    "objectID": "slides/lec-8.html#section",
    "href": "slides/lec-8.html#section",
    "title": "Principles of Inference",
    "section": "",
    "text": "Group exercise - identify the parameter\n\n\nExercise 11.1\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-8.html#statistical-inference-1",
    "href": "slides/lec-8.html#statistical-inference-1",
    "title": "Principles of Inference",
    "section": "Statistical inference",
    "text": "Statistical inference\nInference: estimating the population parameter from the sample.\nStatistical inference: estimating the population parameter from the sample and rigorously quantify our certainty in the estimate."
  },
  {
    "objectID": "slides/lec-8.html#statistic",
    "href": "slides/lec-8.html#statistic",
    "title": "Principles of Inference",
    "section": "Statistic",
    "text": "Statistic\nStatistic: any function of some data\n\ne.g., average, median, iqr, maximum, variance, etc.\n\nSample statistic: a statistic computed on the sample\nSummary statistic: a statistic used to summarize a sample\nTest statistic: statistic used for statistical inference"
  },
  {
    "objectID": "slides/lec-8.html#estimating-a-parameter",
    "href": "slides/lec-8.html#estimating-a-parameter",
    "title": "Principles of Inference",
    "section": "Estimating a parameter",
    "text": "Estimating a parameter\nTo estimate a population parameter, we can simply\n\nobtain a representative sample, and\nuse the corresponding sample statistic as an estimate\n\nto estimate the median age of Duke students, simply collect a sample of students and compute their median age."
  },
  {
    "objectID": "slides/lec-8.html#certainty-matters",
    "href": "slides/lec-8.html#certainty-matters",
    "title": "Principles of Inference",
    "section": "Certainty matters",
    "text": "Certainty matters\nA single point (the sample statistic) does not indicate how certain we are in our estimate.\n\nIf we have a large sample, we can be pretty confident that our estimates will be close to the true value of the parameter,\nif we have a small sample, we know that our estimate may be far from the truth.\ne.g.¬†free throws in basketball, penalty kicks in soccer."
  },
  {
    "objectID": "slides/lec-8.html#statistical-inference-2",
    "href": "slides/lec-8.html#statistical-inference-2",
    "title": "Principles of Inference",
    "section": "Statistical inference",
    "text": "Statistical inference\nFramework to make rigorous statements about uncertainty in our estimates\n\nconfidence intervals\n\nrange of plausible values for the population parameter\n\nhypothesis tests\n\nevaluate competing claims"
  },
  {
    "objectID": "slides/lec-8.html#case-1-single-proportion",
    "href": "slides/lec-8.html#case-1-single-proportion",
    "title": "Principles of Inference",
    "section": "Case 1 ‚Äì Single proportion",
    "text": "Case 1 ‚Äì Single proportion\n\n\nWhat is the proportion of vegetarians among Duke students?\nPopulation parameter: proportion of vegetarians among Duke students \\((p)\\)\nSample statistic: proportion of vegetarian in the sample \\((\\hat{p})\\)\n\n\n\n\n\nConfidence interval: a range a plausible values for the population parameter \\(p\\)\n\nfor example, \\((0.31, 0.43)\\)\n\n\n\nHypothesis test: is the proportion of vegetarians among Duke students \\(0.5\\)?\n\n\\(H_0:p=0.5, \\qquad H_a:p\\neq0.5\\)"
  },
  {
    "objectID": "slides/lec-8.html#case-2-difference-between-two-proportions",
    "href": "slides/lec-8.html#case-2-difference-between-two-proportions",
    "title": "Principles of Inference",
    "section": "Case 2 ‚Äì Difference between two proportions",
    "text": "Case 2 ‚Äì Difference between two proportions\nIs the proportion of vegetarians the same among Duke undergraduate and graduate students?\nPopulation parameter: difference between the proportion of vegetarians among Duke undergraduate and graduate students \\((p_{diff} = p_{undergrad}-p_{grad})\\)\nSample statistic: difference in proportion of vegetarians in the sample \\((\\hat{p}_{diff} = \\hat{p}_{undergrad} - \\hat{p}_{grad})\\)\n\nConfidence interval for \\(p_{diff}\\): \\((-0.05, 0.08)\\)\n\n\nHypothesis test: is the proportion of vegetarians the same among Duke undergraduate and graduate students?\n\n\\(H_0:p_{diff}=0, \\qquad H_a:p_{diff}\\neq0\\)"
  },
  {
    "objectID": "slides/lec-8.html#case-3-single-mean",
    "href": "slides/lec-8.html#case-3-single-mean",
    "title": "Principles of Inference",
    "section": "Case 3 ‚Äì Single mean",
    "text": "Case 3 ‚Äì Single mean\n\n\nHow much time do Duke students sleep on average per night?\nPopulation parameter: mean amount of time that Duke students sleep per night \\((\\mu)\\)\nSample statistic: average amount of time slept in the sample \\((\\bar{x})\\)\n\n\n\n\n\nConfidence interval for \\(\\mu\\): \\((5.5, 7.5)\\)\n\n\nHypothesis test: Do Duke students sleep on average \\(8\\) hours per night?\n\n\\(H_0:\\mu=8, H_a:\\mu\\neq8\\)"
  },
  {
    "objectID": "slides/lec-8.html#case-4-difference-between-two-means",
    "href": "slides/lec-8.html#case-4-difference-between-two-means",
    "title": "Principles of Inference",
    "section": "Case 4 ‚Äì Difference between two means",
    "text": "Case 4 ‚Äì Difference between two means\nDo Duke undergraduate and graduate students sleep on average the same amount of time per night?\n\nPopulation parameter: difference between the mean amount of time that Duke undergraduate and graduate students sleep per night \\((\\mu_{diff} = \\mu_{undergrad}-\\mu_{grad})\\)\nSample statistic: difference between the two sample averages \\((\\bar{x}_{diff} = \\bar{x}_{undergrad} - \\bar{x}_{grad})\\)\n\n\nConfidence interval for \\(\\mu_{diff}\\) : \\((-0.5, 1)\\)\n\n\nHypothesis test: Do Duke undergraduate and graduate students sleep on average the same amount of time per night?\n\n\\(H_0:\\mu_{diff}=0, H_a:\\mu_{diff}\\neq0\\)"
  },
  {
    "objectID": "slides/lec-8.html#case-5-linear-regression",
    "href": "slides/lec-8.html#case-5-linear-regression",
    "title": "Principles of Inference",
    "section": "Case 5 ‚Äì Linear regression",
    "text": "Case 5 ‚Äì Linear regression\nWhat is the relation between fuel consumption in the city and on the highway?\n\nPopulation parameter: the coefficient \\(\\beta_1\\) is the equation \\(\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\\).\nSample statistic: the least-square estimate \\(\\hat{\\beta}_1\\).\n\n\nConfidence interval for \\(\\beta_1\\): \\((1.05, 1.2)\\)\n\n\nHypothesis test: are the variables \\(\\text{cty}\\) and \\(\\text{hwy}\\) independent?\n\n\\(H_0:\\beta_1=0, H_a:\\beta_1\\neq0\\)"
  },
  {
    "objectID": "slides/lec-8.html#the-null-hypothesis-and-the-alternative-hypothesis",
    "href": "slides/lec-8.html#the-null-hypothesis-and-the-alternative-hypothesis",
    "title": "Principles of Inference",
    "section": "The null hypothesis and the alternative hypothesis",
    "text": "The null hypothesis and the alternative hypothesis\nTwo competing hypotheses:\n\nthe null hypothesis \\(H_0\\)\n\n‚Äúnothing is going on‚Äù: there is no effect, no difference\n\nthe alternative hypothesis \\(H_a\\)\n\n‚Äúsomething is going on‚Äù: there is an effect, there is a difference"
  },
  {
    "objectID": "slides/lec-8.html#example",
    "href": "slides/lec-8.html#example",
    "title": "Principles of Inference",
    "section": "Example",
    "text": "Example\nConsider the 2nd case (difference in proportion of vegetarians between undergrad and grad students).\n\n\\(H_0:\\) the proportion of vegetarian is the same among undergraduate and graduate students (‚Äúnothing is going on‚Äù) \\[\nH_0: p_{diff}=p_{undegrad}-p_{grad}=0\n\\] \\[\nH_a: p_{undegrad}=p_{grad}\n\\]\n\\(H_a:\\) the proportion of vegetarians among undergraduate and graduate students is not the same (‚Äúsomething is going on‚Äù). \\[\nH_0: p_{diff}=p_{undegrad}-p_{grad}\\neq0\n\\] \\[\nH_a: p_{undegrad}\\neq p_{grad}\n\\]"
  },
  {
    "objectID": "slides/lec-8.html#more-examples",
    "href": "slides/lec-8.html#more-examples",
    "title": "Principles of Inference",
    "section": "More examples",
    "text": "More examples\n\nAre the Covid-19 vaccines equally effective?\n\n\\(H_0\\): the vaccines are all equally effective; \\(H_a\\): the vaccines are not all equally effective.\n\nDoes caffeine consumption affect student participation in class\n\n\\(H_0\\): caffeine consumption does not affect student participation; \\(H_a\\): caffeine consumption affects student participation.\n\nAre men and women paid equally in the workplace?\n\n\\(H_0\\): men and women are paid equally; \\(H_a\\): men and women are not paid equally.\n\nHave Duke students gained weight since the start of the Covid-19 pandemic?\n\n\\(H_0\\): Duke student have not gained weight; \\(H_a\\): Duke students have gained weight."
  },
  {
    "objectID": "slides/lec-8.html#section-1",
    "href": "slides/lec-8.html#section-1",
    "title": "Principles of Inference",
    "section": "",
    "text": "Group exercise - hypotheses\n\n\nExercises 11.3, 11.5\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-8.html#natural-variability",
    "href": "slides/lec-8.html#natural-variability",
    "title": "Principles of Inference",
    "section": "Natural variability",
    "text": "Natural variability\nGo back to the 2nd case and suppose that \\(H_0\\) is true.\n\nWe‚Äôll probably still observe a small difference between undergrad and grad students in the sample.\n\n\nNow suppose that \\(H_a\\) is true.\n\nWe‚Äôll probably observe a larger difference in the sample,\nbut we might also observe no difference at all,\nor observe a different in the wrong direction!"
  },
  {
    "objectID": "slides/lec-8.html#natural-variability-and-hypothesis-tests",
    "href": "slides/lec-8.html#natural-variability-and-hypothesis-tests",
    "title": "Principles of Inference",
    "section": "Natural variability and hypothesis tests",
    "text": "Natural variability and hypothesis tests\n\n\n\n\n\n\nWe‚Äôll always observe a difference\n\n\nObserving a difference in the sample is not sufficient to reject \\(H_0\\). When we collect a sample, there always is some natural variability inherent to the data.\nDetermining whether the observed difference is due to natural variation or provides convincing evidence of a true difference between the two groups is at the heart of hypothesis tests."
  },
  {
    "objectID": "slides/lec-8.html#section-2",
    "href": "slides/lec-8.html#section-2",
    "title": "Principles of Inference",
    "section": "",
    "text": "Group exercise - convincing evidence\n\n\nSuppose we have a sample of Duke students with the same number of undergraduates and graduates, and \\(\\hat{p}_{diff} = \\hat{p}_{undergrad} - \\hat{p}_{grad} = 0.6 - 0.4 = 0.2\\). For what sample sizes \\(n\\) do you intuitively feel that the observed difference provides convincing evidence that the two groups are different?\n\n\\(n=10\\) (5 undergrad and 5 grad students)\n\\(n=20\\) (10 undergrad and 10 grad students)\n\\(n=50\\) (‚Ä¶)\n\\(n=100\\)\n\\(n=250\\)\n\\(n=500\\)\n\\(n=1000\\)\n\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-8.html#confidence-interval-1",
    "href": "slides/lec-8.html#confidence-interval-1",
    "title": "Principles of Inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nCI: range of plausible values for the population parameter.\nThere always is natural variability in the data.\n\nIf we draw a second sample from the population, the two samples will differ and the sample statistics will not be the same (e.g., \\(\\hat{p}_1\\neq\\hat{p}_2\\), \\(\\bar{x}_1\\neq\\bar{x}_2\\) and \\(\\hat{\\beta}^{(1)}_1\\neq\\hat{\\beta}^{(2)}_1\\)).\nThere is thus no reason to believe that the sample statistic in the first sample is exactly equal to the population parameter (e.g.¬†\\(\\hat{p}_1 = p\\) and \\(\\bar{x}_1=\\mu\\))."
  },
  {
    "objectID": "slides/lec-8.html#examples",
    "href": "slides/lec-8.html#examples",
    "title": "Principles of Inference",
    "section": "Examples",
    "text": "Examples\n\nWhat is the approval rate of the US president?\nWhat proportion of Duke students are vegetarian?\nHow much weight have US adults gained since the start of the Covid-19 pandemic?\n\nCI: range of plausible values based on a sample."
  },
  {
    "objectID": "slides/lec-8.html#classical-and-modern-approach",
    "href": "slides/lec-8.html#classical-and-modern-approach",
    "title": "Principles of Inference",
    "section": "Classical and modern approach",
    "text": "Classical and modern approach\nWe will learn two approaches to statistical inference\n\nclassical\n\npen and paper, pre-computer era\nbased on simple mathematical formula\nrequires the data to satisfy certain conditions\n\nmodern\n\ncomputer-intensive\nmodels the variability in the data by repeating a procedure many times (for-loop)\nalways applicable"
  },
  {
    "objectID": "slides/lec-8.html#section-3",
    "href": "slides/lec-8.html#section-3",
    "title": "Principles of Inference",
    "section": "",
    "text": "Individual exercise ‚Äì simulation\n\n\nSuppose you flip a coin 100 times and count the number of heads. In the first lecture, you were asked to identify what number of heads would make you doubt that the coin is fair.\nWe will run an experiment together to see if your gut feeling was correct.\nUse the following commands to simulate 100 flips of a coin and count the number of heads. Repeat the experiment 5 times, keeping track of the number of heads.\n\nset.seed(0) # change the seed for each run\nflips <- rbernoulli(n = 100, p = 0.5) # 100 flips - TRUE is heads and FALSE is tails\nsum(flips) # number of heads (number of TRUEs)\n\n[1] 48\n\n\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-8.html#for-loop",
    "href": "slides/lec-8.html#for-loop",
    "title": "Principles of Inference",
    "section": "For-loop",
    "text": "For-loop\nThe following for-loop does the previous experiment efficiently!\n\nset.seed(0)\nresults <- tibble(prop_heads = numeric())          # empty data frame to collect the results\nfor(i in 1 : 1e3){                              # repeat the experiment 1,000 times\n  flips      <- rbernoulli(100, p = 0.5)        # flip 100 coins (sample)\n  n_heads    <- sum(flips)                      # count the number of heads\n  prop_heads <- n_heads / 100                   # proportion of heads (sample statistic)\n  results    <- results %>% add_row(prop_heads) # add the sample statistic to the data frame `result`\n}"
  },
  {
    "objectID": "slides/lec-8.html#section-4",
    "href": "slides/lec-8.html#section-4",
    "title": "Principles of Inference",
    "section": "",
    "text": "Distribution of the sample statistic \\(\\hat{p}\\) when \\(H_0\\) is true.\n\nggplot(results) +\n  geom_histogram(aes(prop_heads), binwidth = 0.01)"
  },
  {
    "objectID": "slides/lec-8.html#population-and-parameter",
    "href": "slides/lec-8.html#population-and-parameter",
    "title": "Principles of Inference",
    "section": "Population and parameter",
    "text": "Population and parameter\n\n\n\n\n\n\nPopulation and parameter first\n\n\nAlways start by defining the population and the parameter of interest.\nPopulation: flips of the coin\nParameter: probability that a flip is head"
  },
  {
    "objectID": "slides/lec-8.html#null-and-alternative-hypotheses",
    "href": "slides/lec-8.html#null-and-alternative-hypotheses",
    "title": "Principles of Inference",
    "section": "Null and alternative hypotheses",
    "text": "Null and alternative hypotheses\n\n\\(H_0:p=0.5\\) (the coin is fair)\n\\(H_a:p\\neq0.5\\) (the coin is not fair)"
  },
  {
    "objectID": "slides/lec-8.html#like-jurors-in-the-justice-system",
    "href": "slides/lec-8.html#like-jurors-in-the-justice-system",
    "title": "Principles of Inference",
    "section": "Like jurors in the justice system",
    "text": "Like jurors in the justice system\n\n\\(H_0:\\) innocent (the coin is fair)\n\\(H_a:\\) guilty (the coin is not fair)\n\nQuestion: do the facts (the sample) provide sufficient evidence to reject the claim that the defendant is innocent (that the coin is fair)?\n\nIf so, we reject \\(H_0\\),\notherwise, we fail to reject \\(H_0\\).\n\n\n\n\n\n\n\nNever accept the null hypothesis\n\n\nWe never accept the null hypothesis! We only reject or fail to reject it."
  },
  {
    "objectID": "slides/lec-8.html#section-5",
    "href": "slides/lec-8.html#section-5",
    "title": "Principles of Inference",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(prop_heads), binwidth = 0.01) +\n  geom_vline(xintercept = c(0.55, 0.65), col = \"red\", size = 2)"
  },
  {
    "objectID": "slides/lec-8.html#two-outcomes",
    "href": "slides/lec-8.html#two-outcomes",
    "title": "Principles of Inference",
    "section": "Two outcomes",
    "text": "Two outcomes\nSuppose the sample consists of 55 heads of 100 flips\n\\(\\Rightarrow\\) such sample is plausible under \\(H_0\\); the observed data do not provide strong evidence against the null hypothesis; we fail to reject the claim that the coin is fair\n\nThe coin might be unfair, but the data do not provide strong evidence against fairness.\n\n\nNow suppose that out of 100 flips, you observe 65 heads.\n\\(\\Rightarrow\\) this result is extremely unlikely under \\(H_0\\); the observed data provide strong evidence against the null hypothesis; we reject the claim that the coin is fair.\n\nThe coin might be fair, but a fair coin will rarely give \\(65\\) heads out of \\(100\\) flips."
  },
  {
    "objectID": "slides/lec-8.html#natural-variability-1",
    "href": "slides/lec-8.html#natural-variability-1",
    "title": "Principles of Inference",
    "section": "Natural variability",
    "text": "Natural variability\nDue to the natural variability of the data, each sample is different.\n\nIn practice we only get to observe a single sample. But if we could observe other samples,\n\nthey would all be a bit different\n\\(\\Rightarrow\\) the sample statistics would also be different\n\\(\\Rightarrow\\) the estimates would also be different."
  },
  {
    "objectID": "slides/lec-8.html#sample-to-sample-variability",
    "href": "slides/lec-8.html#sample-to-sample-variability",
    "title": "Principles of Inference",
    "section": "Sample-to-sample variability",
    "text": "Sample-to-sample variability\nIf the samples are very different\n\nwe say that the sample-to-sample variability is large.\nwe would not be very confident in the estimate\n\nIf the samples are all similar\n\nwe say that the sample-to-sample variability is small\nwe would be confident that the estimate is close to the truth"
  },
  {
    "objectID": "slides/lec-8.html#from-a-single-sample-to-bootstrap-samples",
    "href": "slides/lec-8.html#from-a-single-sample-to-bootstrap-samples",
    "title": "Principles of Inference",
    "section": "From a single sample to bootstrap samples",
    "text": "From a single sample to bootstrap samples\nProblem: we only get to observe a single sample!\n\nSolution: Use the sample to approximate the population and take repeated samples from the estimated population to simulate many samples.\n\nEquivalent to sampling with repetition from the sample."
  },
  {
    "objectID": "slides/lec-8.html#section-6",
    "href": "slides/lec-8.html#section-6",
    "title": "Principles of Inference",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-8.html#from-bootstrap-samples-to-ci",
    "href": "slides/lec-8.html#from-bootstrap-samples-to-ci",
    "title": "Principles of Inference",
    "section": "From bootstrap samples to CI",
    "text": "From bootstrap samples to CI\nComputing the sample statistic of each bootstrap sample provides its sampling distribution.\nTo construct a 90% CI of some parameter, we simply identify the 5th and 95th percentiles of the sampling distribution of the corresponding sample statistics.\n\nthe 5th and 95th percentiles of the sampling distribution of the median give the 90% CI for the median.\n\n\nInterpretation: ‚ÄúWe are 90% confident that the interval captures the true value of the population parameter‚Äù."
  },
  {
    "objectID": "slides/lec-8.html#little-variability-rightarrow-narrow-ci",
    "href": "slides/lec-8.html#little-variability-rightarrow-narrow-ci",
    "title": "Principles of Inference",
    "section": "Little variability \\(\\Rightarrow\\) narrow CI",
    "text": "Little variability \\(\\Rightarrow\\) narrow CI\nLittle variability in the data\n\n\\(\\Rightarrow\\) little sample-to-sample variability\n\n\n\\(\\Rightarrow\\) little variability between bootstrap samples\n\n\n\\(\\Rightarrow\\) sampling distribution of mean/proportion is concentrated\n\n\n\\(\\Rightarrow\\) CI is narrow."
  },
  {
    "objectID": "slides/lec-8.html#much-variability-rightarrow-large-ci",
    "href": "slides/lec-8.html#much-variability-rightarrow-large-ci",
    "title": "Principles of Inference",
    "section": "Much variability \\(\\Rightarrow\\) large CI",
    "text": "Much variability \\(\\Rightarrow\\) large CI\nMuch variability in the data\n\n\\(\\Rightarrow\\) much sample-to-sample variability\n\n\n\\(\\Rightarrow\\) much variability between bootstrap samples\n\n\n\\(\\Rightarrow\\) sampling distribution of mean/proportion is diffuse\n\n\n\\(\\Rightarrow\\) CI is large."
  },
  {
    "objectID": "slides/lec-8.html#computing-bootstrap-ci",
    "href": "slides/lec-8.html#computing-bootstrap-ci",
    "title": "Principles of Inference",
    "section": "Computing bootstrap CI",
    "text": "Computing bootstrap CI\n\nd <- ggplot2::mpg\nresults <- tibble(mean = numeric(), sd = numeric())\nfor(i in 1 : 1e3){\n  d_boot <- slice_sample(d, n = nrow(d), replace = TRUE) # sampling from the sample with replacement\n  results <- results %>% \n    add_row(mean = mean(d_boot$cty), sd = sd(d_boot$cty)) # sample statistics of the bootstrap sample\n}"
  },
  {
    "objectID": "slides/lec-8.html#section-7",
    "href": "slides/lec-8.html#section-7",
    "title": "Principles of Inference",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(mean)) +\n  geom_vline(xintercept = quantile(results$mean, c(0.05, 0.95)), col = \"red\", size = 2)"
  },
  {
    "objectID": "slides/lec-8.html#section-8",
    "href": "slides/lec-8.html#section-8",
    "title": "Principles of Inference",
    "section": "",
    "text": "ggplot(results) +\n  geom_histogram(aes(sd))+\n  geom_vline(xintercept = quantile(results$sd, c(0.05, 0.95)), col = \"red\", size = 2)"
  },
  {
    "objectID": "slides/lec-8.html#computing-the-ci",
    "href": "slides/lec-8.html#computing-the-ci",
    "title": "Principles of Inference",
    "section": "Computing the CI",
    "text": "Computing the CI\n\n# 90% bootstrap CI for the mean cty in the population\nquantile(results$mean, c(0.05, 0.95))\n\n      5%      95% \n16.41880 17.32051 \n\n# 90% bootstrap CI for the sd of cty in the population\nquantile(results$sd  , c(0.05, 0.95))\n\n      5%      95% \n3.839925 4.673212"
  },
  {
    "objectID": "slides/lec-8.html#section-9",
    "href": "slides/lec-8.html#section-9",
    "title": "Principles of Inference",
    "section": "",
    "text": "Group exercise - bootstrap CI\n\n\nExercises 12.1, 12.3, 12.7\n\n\n\n\n\n\n06:00"
  },
  {
    "objectID": "slides/lec-8.html#section-10",
    "href": "slides/lec-8.html#section-10",
    "title": "Principles of Inference",
    "section": "",
    "text": "Group exercise - bootstrap CI\n\n\nRe-use the code from the previous slide to compute a 90% bootstrap CI of the variance of cty in the population.\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-8.html#recap-1",
    "href": "slides/lec-8.html#recap-1",
    "title": "Principles of Inference",
    "section": "Recap",
    "text": "Recap\n\n\nStatistical inference\nFive cases\n\nsingle proportion\ndifference between two portions\nsingle mean\ndifference between two means\nregression parameters\n\nHypothesis test\nConfidence interval\nA first glimpse of modern statistics\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "slides/lec-9.html#announcements",
    "href": "slides/lec-9.html#announcements",
    "title": "Inference for proportions",
    "section": "Announcements",
    "text": "Announcements\n\nData analysis in action ‚Äì Inferring ethnicity from X-rays\nAnonymous mid-course feedback"
  },
  {
    "objectID": "slides/lec-9.html#recap",
    "href": "slides/lec-9.html#recap",
    "title": "Inference for proportions",
    "section": "Recap",
    "text": "Recap\n\nStatistical inference\nFive cases\nHypothesis test\nConfidence interval\nA first glimpse of modern statistics"
  },
  {
    "objectID": "slides/lec-9.html#outline",
    "href": "slides/lec-9.html#outline",
    "title": "Inference for proportions",
    "section": "Outline",
    "text": "Outline\n\nOne proportion (case 1)\nTwo proportions (case 2)"
  },
  {
    "objectID": "slides/lec-9.html#setup",
    "href": "slides/lec-9.html#setup",
    "title": "Inference for proportions",
    "section": "Setup",
    "text": "Setup\npopulation parameter: proportion \\(p\\)\nSample statistics: sample proportion \\(\\hat{p}\\)\n\nHypothesis testing:\n\n\\(H_0:p=p_0\\) where \\(p_0\\) is a number between \\(0\\) and \\(1\\)\n\\(H_a:p\\neq p_0\\)\n\nConfidence interval: range of plausible values for \\(p\\)."
  },
  {
    "objectID": "slides/lec-9.html#section",
    "href": "slides/lec-9.html#section",
    "title": "Inference for proportions",
    "section": "",
    "text": "One-sided and two-sided \\(H_a\\)\n\n\nOne-sided) \\(H_a:p>p_0\\) or \\(H_a:p<p_0\\)\nTwo-sided) \\(H_a: p\\neq p_0\\)\nIt is never wrong to use a two-sided \\(H_a\\)."
  },
  {
    "objectID": "slides/lec-9.html#section-1",
    "href": "slides/lec-9.html#section-1",
    "title": "Inference for proportions",
    "section": "",
    "text": "Individual exercise - Hypotheses\n\n\nExercise 16.1\n\n\n\n\n\n\n01:00"
  },
  {
    "objectID": "slides/lec-9.html#example-legalizing-marijuana",
    "href": "slides/lec-9.html#example-legalizing-marijuana",
    "title": "Inference for proportions",
    "section": "Example ‚Äì legalizing marijuana",
    "text": "Example ‚Äì legalizing marijuana\nWhat proportion of US adults support legalizing marijuana?\n\nSample: 900/1500 support it.\n\n\n\\[\n\\hat{p} = \\dfrac{900}{1500}=0.6\n\\]"
  },
  {
    "objectID": "slides/lec-9.html#section-2",
    "href": "slides/lec-9.html#section-2",
    "title": "Inference for proportions",
    "section": "",
    "text": "\\(H_0:p=0.5\\)\n\\(H_a:p\\neq 0.5\\)\n\n\nSimulate many samples under \\(H_0\\).\nDetermine if the observed data could have plausibly arisen under \\(H_0\\).\n\n\n\n\n\n\n\nParametric bootstrap\n\n\nThe textbook uses the term (parametric) bootstrap to refer to this procedure."
  },
  {
    "objectID": "slides/lec-9.html#section-3",
    "href": "slides/lec-9.html#section-3",
    "title": "Inference for proportions",
    "section": "",
    "text": "set.seed(0)\nresults <-  tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- rbernoulli(n = 1500, p = 0.5) # simulate a sample under H0\n  p_hat   <- mean(sim)                     # compute the sample statistic p_hat\n  results <- results %>% add_row(p_hat = p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat))"
  },
  {
    "objectID": "slides/lec-9.html#conclusion",
    "href": "slides/lec-9.html#conclusion",
    "title": "Inference for proportions",
    "section": "Conclusion",
    "text": "Conclusion\n\\(\\hat{p}=0.6\\) is extremely unlikely to happen under \\(H_0:p=0.5\\).\nWe therefore reject the null hypothesis that half of the people support legalizing marijuana."
  },
  {
    "objectID": "slides/lec-9.html#section-4",
    "href": "slides/lec-9.html#section-4",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercise - HT for one proportion\n\n\nExercise 16.3 ‚Äì in part d, you do not need to estimate the p-value.\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-9.html#p-value",
    "href": "slides/lec-9.html#p-value",
    "title": "Inference for proportions",
    "section": "p-value",
    "text": "p-value\nWhat if we do not have a clear-cut case?\n\ne.g., \\(\\hat{p}=0.52 = \\dfrac{780}{1500}\\)\n\n\np-value: probability that the sample statistic of a sample simulated under \\(H_0\\) is at least as extreme as that of the observed sample.\n\nthe probability that \\(\\hat{p}_{sim} > 0.52\\) or \\(\\hat{p}_{sim} < 0.48\\)."
  },
  {
    "objectID": "slides/lec-9.html#p-value-in-r",
    "href": "slides/lec-9.html#p-value-in-r",
    "title": "Inference for proportions",
    "section": "p-value in R",
    "text": "p-value in R\n\nresults <- results %>%\n  mutate(is_more_extreme = p_hat <= 0.48 | p_hat >= 0.52)\nresults\n\n# A tibble: 10,000 x 2\n   p_hat is_more_extreme\n   <dbl> <lgl>          \n 1 0.475 TRUE           \n 2 0.485 FALSE          \n 3 0.483 FALSE          \n 4 0.528 TRUE           \n 5 0.51  FALSE          \n 6 0.493 FALSE          \n 7 0.503 FALSE          \n 8 0.5   FALSE          \n 9 0.495 FALSE          \n10 0.503 FALSE          \n# ... with 9,990 more rows\n\n\n\nresults %>% summarize(p_value = mean(is_more_extreme))\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1   0.127"
  },
  {
    "objectID": "slides/lec-9.html#p-value-in-practice",
    "href": "slides/lec-9.html#p-value-in-practice",
    "title": "Inference for proportions",
    "section": "p-value in practice",
    "text": "p-value in practice\nIn practice, if a p-value is smaller than 0.05 we reject \\(H_0\\)\n\n\\(\\alpha = 0.05\\) is called the significance level.\nIf \\(p<0.05\\), the observed sample is in the top 5% of the most extreme simulated samples.\nIt is highly unlikely that the observed sample could have arisen if \\(H_0\\) were true.\nthe difference \\(\\hat{p}-p_0\\) is statistically significant."
  },
  {
    "objectID": "slides/lec-9.html#section-5",
    "href": "slides/lec-9.html#section-5",
    "title": "Inference for proportions",
    "section": "",
    "text": "Why \\(\\alpha = 0.05\\)?\n\n\nTo know why the significance level 0.05 is widely used in practice, check out this short video."
  },
  {
    "objectID": "slides/lec-9.html#section-6",
    "href": "slides/lec-9.html#section-6",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercise - HT for one proportion\n\n\nExercise 16.5.\n\n\n\n\n\n\n04:00"
  },
  {
    "objectID": "slides/lec-9.html#bootstrapping",
    "href": "slides/lec-9.html#bootstrapping",
    "title": "Inference for proportions",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nSample with repetition from the observed sample to construct many bootstrap samples.\nBootstrap samples \\(\\Rightarrow\\) sampling distribution \\(\\Rightarrow\\) CI"
  },
  {
    "objectID": "slides/lec-9.html#section-7",
    "href": "slides/lec-9.html#section-7",
    "title": "Inference for proportions",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-9.html#bootstrap-in-r",
    "href": "slides/lec-9.html#bootstrap-in-r",
    "title": "Inference for proportions",
    "section": "Bootstrap in R",
    "text": "Bootstrap in R\n\nsample_observed <- tibble(support = c(rep(1, 900), rep(0, 600)))\n\n\nset.seed(0)\nslice_sample(sample_observed, n = 5) # 5 random rows from the data frame\n\n# A tibble: 5 x 1\n  support\n    <dbl>\n1       0\n2       0\n3       1\n4       1\n5       0\n\n\n\nsample_bootstrap <- function(data){\n  n                <- nrow(data)\n  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)\n  return(sample_bootstrap)\n}"
  },
  {
    "objectID": "slides/lec-9.html#section-8",
    "href": "slides/lec-9.html#section-8",
    "title": "Inference for proportions",
    "section": "",
    "text": "results <- tibble(p_hat = numeric()) # empty data frame to collect the results\nset.seed(0)\nfor(i in 1 : 1e4){\n  d_boot    <- sample_bootstrap(sample_observed) # bootstrap sample\n  stat_boot <- mean(d_boot$support)              # bootstrap statistic\n  results   <- results %>% add_row(p_hat = stat_boot) \n}\n\n\nquantile(results$p_hat, c(0.05 , 0.95 )) # 90% CI\n\n       5%       95% \n0.5793333 0.6206667 \n\n\n\nquantile(results$p_hat, c(0.025, 0.975)) # 95% CI (wider)\n\n     2.5%     97.5% \n0.5753333 0.6253333"
  },
  {
    "objectID": "slides/lec-9.html#section-9",
    "href": "slides/lec-9.html#section-9",
    "title": "Inference for proportions",
    "section": "",
    "text": "ggplot(results) + \n  geom_histogram(aes(p_hat)) + \n  geom_vline(xintercept = quantile(results$p_hat, c(0.05 , 0.95 )), col = \"gold1\", size = 2) + # 90% CI \n  geom_vline(xintercept = quantile(results$p_hat, c(0.025, 0.975)), col = \"maroon\", size = 2) # 95% CI"
  },
  {
    "objectID": "slides/lec-9.html#section-10",
    "href": "slides/lec-9.html#section-10",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercise - Bootstrap CI\n\n\nExercises 12.1 and 12.7\n\n\n\n\n\n\n03:30"
  },
  {
    "objectID": "slides/lec-9.html#setup-1",
    "href": "slides/lec-9.html#setup-1",
    "title": "Inference for proportions",
    "section": "Setup",
    "text": "Setup\nA population divided in two groups.\nPopulation parameter: difference in proportion\n\\[\np_{diff}=p_1-p_2\n\\]\nSample statistics: difference in proportion in the sample\n\\[\n\\hat{p}_{diff}=\\hat{p}_1-\\hat{p}_2\n\\]\n\n\\(H_0:p_{diff}=0\\) (no difference between the two groups)\n\\(H_a:p_{diff}\\neq0\\)"
  },
  {
    "objectID": "slides/lec-9.html#example-sex-discrimination",
    "href": "slides/lec-9.html#example-sex-discrimination",
    "title": "Inference for proportions",
    "section": "Example ‚Äì sex discrimination",
    "text": "Example ‚Äì sex discrimination\nAre individuals who identify as female discriminated against in promotion decisions?\n\n\n\n\nSummary results for the sex discrimination study.\n \n\n\ndecision\n\n\n  \n    sex \n    promoted \n    not promoted \n    Total \n  \n \n\n  \n    male \n    21 \n    3 \n    24 \n  \n  \n    female \n    14 \n    10 \n    24 \n  \n  \n    Total \n    35 \n    13 \n    48"
  },
  {
    "objectID": "slides/lec-9.html#section-11",
    "href": "slides/lec-9.html#section-11",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercise - two proportions\n\n\nWhat are \\(\\hat{p}_f\\), \\(\\hat{p}_m\\) and \\(\\hat{p}_{diff}\\)? Do you intuitively feel that the data provide convincing evidence of sex discrimination?\n\n\n\n\n\n\n02:00"
  },
  {
    "objectID": "slides/lec-9.html#section-12",
    "href": "slides/lec-9.html#section-12",
    "title": "Inference for proportions",
    "section": "",
    "text": "\\(H_0:p_{diff}=0\\)\n\\(H_a:p_{diff}\\neq 0\\)\n\n\nSimulate many samples under \\(H_0\\) (no discrimination)\nDetermine if the observed data could have plausibly arisen under \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-9.html#simulating-under-h_0",
    "href": "slides/lec-9.html#simulating-under-h_0",
    "title": "Inference for proportions",
    "section": "Simulating under \\(H_0\\)",
    "text": "Simulating under \\(H_0\\)\nUnder \\(H_0\\), there is no discrimination\n\\(\\Rightarrow\\) whether someone receives a promotion is independent of their sexual identification,\n\\(\\Rightarrow\\) randomly re-assign the \\(35\\) promotions independently of the sexual identification."
  },
  {
    "objectID": "slides/lec-9.html#section-13",
    "href": "slides/lec-9.html#section-13",
    "title": "Inference for proportions",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-9.html#simulation-result",
    "href": "slides/lec-9.html#simulation-result",
    "title": "Inference for proportions",
    "section": "Simulation result",
    "text": "Simulation result\n\n\n\n \n\n\ndecision\n\n\n  \n    sex \n    promoted \n    not promoted \n    Total \n  \n \n\n  \n    male \n    18 \n    6 \n    24 \n  \n  \n    female \n    17 \n    7 \n    24 \n  \n  \n    Total \n    35 \n    13 \n    48 \n  \n\n\n\n\n\nSource: IMS"
  },
  {
    "objectID": "slides/lec-9.html#section-14",
    "href": "slides/lec-9.html#section-14",
    "title": "Inference for proportions",
    "section": "",
    "text": "d <- gender_discrimination\nd_sim <- d %>% mutate(decision = sample(decision)) # shuffle the promotions\nd_sim\n\n# A tibble: 48 x 2\n   gender decision    \n   <fct>  <fct>       \n 1 male   not promoted\n 2 male   not promoted\n 3 male   promoted    \n 4 male   promoted    \n 5 male   promoted    \n 6 male   promoted    \n 7 male   promoted    \n 8 male   not promoted\n 9 male   promoted    \n10 male   not promoted\n# ... with 38 more rows"
  },
  {
    "objectID": "slides/lec-9.html#function-for-computing-the-test-statistic",
    "href": "slides/lec-9.html#function-for-computing-the-test-statistic",
    "title": "Inference for proportions",
    "section": "Function for computing the test statistic",
    "text": "Function for computing the test statistic\n\ncompute_p_diff <- function(data){\n  p_hat <- data %>%\n    group_by(gender) %>%\n    summarize(p_hat = mean(decision == \"promoted\"))\n  p_diff_hat <- p_hat$p_hat[1] - p_hat$p_hat[2]\n  return(p_diff_hat)\n}\ncompute_p_diff(d_sim)\n\n[1] -0.04166667"
  },
  {
    "objectID": "slides/lec-9.html#for-loop-for-simulating-under-h_0",
    "href": "slides/lec-9.html#for-loop-for-simulating-under-h_0",
    "title": "Inference for proportions",
    "section": "For-loop for simulating under \\(H_0\\)",
    "text": "For-loop for simulating under \\(H_0\\)\n\n# Setup\nresults    <- tibble(p_diff_hat = numeric())\nd          <- gender_discrimination\n\n# Simulations\nset.seed(0)\nfor(i in 1 : 1e4){\n  d_sim      <- d %>% mutate(decision = sample(decision)) # simulate under H0\n  p_diff_hat <- compute_p_diff(d_sim) # test statistic\n  results    <- results %>% add_row(p_diff_hat = p_diff_hat)\n}"
  },
  {
    "objectID": "slides/lec-9.html#sampling-distribution",
    "href": "slides/lec-9.html#sampling-distribution",
    "title": "Inference for proportions",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\np_diff_obs <- 21 / 24 - 14 / 24\np_diff_obs\n\n[1] 0.2916667\n\nggplot(results) + \n  geom_histogram(aes(p_diff_hat)) +\n  geom_vline(xintercept = p_diff_obs, col = \"maroon\", size = 2)"
  },
  {
    "objectID": "slides/lec-9.html#p-value-1",
    "href": "slides/lec-9.html#p-value-1",
    "title": "Inference for proportions",
    "section": "p-value",
    "text": "p-value\np-value: probability that the sample statistic of a sample simulated under \\(H_0\\) is at least as extreme as that of the observed sample.\n\nthe probability that \\(\\hat{p}_{diff}^{sim}\\ge\\) 0.29 or \\(\\hat{p}_{diff}^{sim}\\le\\) -0.29.\n\n\nresults %>%\n  mutate(is_more_extreme = p_diff_hat >= p_diff_obs | p_diff_hat <= -p_diff_obs) %>%\n  summarize(p_value = mean(is_more_extreme))\n\n# A tibble: 1 x 1\n  p_value\n    <dbl>\n1  0.0435"
  },
  {
    "objectID": "slides/lec-9.html#significance-level-alpha-0.05",
    "href": "slides/lec-9.html#significance-level-alpha-0.05",
    "title": "Inference for proportions",
    "section": "significance level \\(\\alpha = 0.05\\)",
    "text": "significance level \\(\\alpha = 0.05\\)\nUsing the usual significance level \\(\\alpha = 0.05\\), we reject the null hypothesis\n\nthe observed difference in promotions is unlikely to be due to random luck\nthe difference is statistically significant."
  },
  {
    "objectID": "slides/lec-9.html#section-15",
    "href": "slides/lec-9.html#section-15",
    "title": "Inference for proportions",
    "section": "",
    "text": "Statisticians as messengers\n\n\nStatisticians are just messengers; they only interpret what the data are indicating.\nIf you are a scientist and are not happy with the result of a statistical analysis, change the study not the statistician!\n\nlarger sample\nsmaller measurement errors\nnew variables, e.g.¬†salary instead of promotion"
  },
  {
    "objectID": "slides/lec-9.html#section-16",
    "href": "slides/lec-9.html#section-16",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercise - HT\n\n\n17.1 (skip part b)\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-9.html#section-17",
    "href": "slides/lec-9.html#section-17",
    "title": "Inference for proportions",
    "section": "",
    "text": "Same idea as before: sample with repetition from the observed data to construct many bootstrap samples.\nBootstrap samples \\(\\Rightarrow\\) sampling distribution \\(\\Rightarrow\\) CI"
  },
  {
    "objectID": "slides/lec-9.html#bootstrap",
    "href": "slides/lec-9.html#bootstrap",
    "title": "Inference for proportions",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nSource: IMS"
  },
  {
    "objectID": "slides/lec-9.html#bootstrap-in-r-1",
    "href": "slides/lec-9.html#bootstrap-in-r-1",
    "title": "Inference for proportions",
    "section": "Bootstrap in R",
    "text": "Bootstrap in R\n\nd <- gender_discrimination %>%\n  mutate(decision = if_else(decision == \"promoted\", 1, 0))\nsample_observed_m <- d %>% filter(gender == \"male\"  )\nsample_observed_f <- d %>% filter(gender == \"female\")\n\nset.seed(0)\nsample_bootstrap(sample_observed_m) # bootstrap sample\n\n# A tibble: 24 x 2\n   gender decision\n   <fct>     <dbl>\n 1 male          1\n 2 male          1\n 3 male          1\n 4 male          1\n 5 male          1\n 6 male          0\n 7 male          1\n 8 male          1\n 9 male          1\n10 male          1\n# ... with 14 more rows\n\nsample_bootstrap(sample_observed_f) # bootstrap sample\n\n# A tibble: 24 x 2\n   gender decision\n   <fct>     <dbl>\n 1 female        1\n 2 female        1\n 3 female        1\n 4 female        1\n 5 female        1\n 6 female        0\n 7 female        1\n 8 female        0\n 9 female        1\n10 female        1\n# ... with 14 more rows"
  },
  {
    "objectID": "slides/lec-9.html#section-18",
    "href": "slides/lec-9.html#section-18",
    "title": "Inference for proportions",
    "section": "",
    "text": "results <- tibble(p_diff_hat = numeric())\nfor(i in 1 : 1e3){\n  d_boot_m   <- sample_bootstrap(sample_observed_m) # bootstrap sample\n  d_boot_f   <- sample_bootstrap(sample_observed_f) # bootstrap sample\n  p_diff_hat <- compute_p_diff(rbind(d_boot_m, d_boot_f)) # bootstrap statistic\n  results    <- results %>% add_row(p_diff_hat = p_diff_hat)\n}\n\n\nquantile(results$p_diff_hat, c(0.05 , 0.95 )) # 90% CI\n\n 5% 95% \n  0   0 \n\nquantile(results$p_diff_hat, c(0.05 , 0.95 )) %>% signif(2) # 90% CI\n\n 5% 95% \n  0   0 \n\nquantile(results$p_diff_hat, c(0.025, 0.975)) %>% signif(2) # 95% CI\n\n 2.5% 97.5% \n    0     0"
  },
  {
    "objectID": "slides/lec-9.html#section-19",
    "href": "slides/lec-9.html#section-19",
    "title": "Inference for proportions",
    "section": "",
    "text": "ggplot(results) +\n  geom_histogram(aes(p_diff_hat))  + \n  geom_vline(xintercept = quantile(results$p_diff_hat, c(0.05 , 0.95 )), col = \"gold1\", size = 2) + # 90% CI \n  geom_vline(xintercept = quantile(results$p_diff_hat, c(0.025, 0.975)), col = \"maroon\", size = 2) # 95% CI"
  },
  {
    "objectID": "slides/lec-9.html#section-20",
    "href": "slides/lec-9.html#section-20",
    "title": "Inference for proportions",
    "section": "",
    "text": "Two sides of the same coin\n\n\nIn the two examples, the HT and the CI agree with one another. This is not a coincidence; they will agree in the vast majority of cases!\nWe can show mathematically that a HT and a CI are really two sides of the same coin."
  },
  {
    "objectID": "slides/lec-9.html#what-does-90-and-95-mean",
    "href": "slides/lec-9.html#what-does-90-and-95-mean",
    "title": "Inference for proportions",
    "section": "What does 90% and 95% mean?",
    "text": "What does 90% and 95% mean?\nRemember that if we could obtain multiple samples, they‚Äôd all be a bit different.\n\\(\\Rightarrow\\) the corresponding CIs would also be a bit different\n\n\nA 90% CI will capture the true value of the parameter 90% of the time.\nA 95% CI will be wider and thus more likely to capture the truth (95% of the time).\n\n\n\n\nTrade off between being informative and true."
  },
  {
    "objectID": "slides/lec-9.html#section-21",
    "href": "slides/lec-9.html#section-21",
    "title": "Inference for proportions",
    "section": "",
    "text": "Source: IMS"
  },
  {
    "objectID": "slides/lec-9.html#section-22",
    "href": "slides/lec-9.html#section-22",
    "title": "Inference for proportions",
    "section": "",
    "text": "Group exercises - CI\n\n\nExercises 17.3 (only do part c) and 17.5\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-9.html#recap-2",
    "href": "slides/lec-9.html#recap-2",
    "title": "Inference for proportions",
    "section": "Recap",
    "text": "Recap\n\nOne proportion\n\nHT via simulation\nCI via bootstrap\n\nTwo proportions\n\nHT via simulation\nCI via bootstrap\n\n\n\n\n\nhttps://rmorsomme.github.io/website/"
  },
  {
    "objectID": "teaching-triangle.html",
    "href": "teaching-triangle.html",
    "title": "Teaching triangle",
    "section": "",
    "text": "Time\nDate\n\n\n\n\n6:00\nE starts on time\nE lectures (slides) with numerous examples and pictures\n\n\n6:05\nE interacts with students through chat, thumb up/down\n\n\n6:10\n1/4 Zoom cameras on\nNich. Guar. provides answer, E. corrects Nich. (could have asked the rest of the class to correct Nich. instead)\n27 students\n\n\n6:15\nE uses a short video (2 min)\n\n\n6:20\nE answers question from Zoom chat\n\n\n6:25\nE illustrates phonology with many examples\n26 students\n\n\n6:30\nE applies phonology to sign language, many videos\nMuch focus given to sign language\n49 messages in chat\n\n\n6:35\nJaich. Pai. raises hand; E spots Jaich immediately\n\n\n6:40\nSemantics ‚Äì E connects main topics of lecture to one another\n\n\n6:45\n91 messages in Zoom chat\nMuch interaction with students\n\n\n6:50\nThe exchange with students continues\n\n\n6:55\nThe exchange has just concluded\nVideos about code-switching\n\n\n7:00\nAnother long exchange with students (>10 min)\n\n\n7:16\n127 messages in chat\n25 participants\nbreak\n\n\nQ\n-) English/US-centered; extending discussion before break was irritating\n+) videos"
  },
  {
    "objectID": "slides/lec-12.html#normal-approximation-is-good",
    "href": "slides/lec-12.html#normal-approximation-is-good",
    "title": "Classical inference",
    "section": "Normal approximation is good",
    "text": "Normal approximation is good\n\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))"
  },
  {
    "objectID": "slides/lec-12.html#standard-error-1",
    "href": "slides/lec-12.html#standard-error-1",
    "title": "Classical inference",
    "section": "Standard error",
    "text": "Standard error\nStandard error (SE): standard deviation of the normal approximation\nIt measures the variability of the sample statistic.\n\n\\(SE(\\hat{p})=\\sqrt{\\frac{p(1-p)}{n}}\\)\n\\(SE(\\hat{p}_{diff})=\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\)\n\\(SE(\\bar{x}) = \\sqrt{\\frac{\\sigma^2}{n}}\\)\n\\(SE(\\bar{x}_{diff}) = \\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\)\n\\(SE(\\hat{\\beta})\\) has a complicated form.\n\n\n\n\n\n\n\nNote\n\n\nNote the role of the sample size!"
  },
  {
    "objectID": "slides/lec-12.html#case-1-one-proportion",
    "href": "slides/lec-12.html#case-1-one-proportion",
    "title": "Classical inference",
    "section": "Case 1 ‚Äì one proportion",
    "text": "Case 1 ‚Äì one proportion\n\nn <- 1500 # sample size\nx <- 780  # number of successes\nprop.test(\n  x, n,             # observed data\n  p = 0.5,          # value in the null hypothesis\n  conf.level = 0.99 # confidence level for CI\n  )\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 2.3207, df = 1, p-value = 0.1277\nalternative hypothesis: true p is not equal to 0.5\n99 percent confidence interval:\n 0.4864251 0.5533970\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "slides/lec-12.html#case-2-two-proportions",
    "href": "slides/lec-12.html#case-2-two-proportions",
    "title": "Classical inference",
    "section": "Case 2 ‚Äì two proportions",
    "text": "Case 2 ‚Äì two proportions\nConsider the gender discrimination study.\n\nn_m <- 24; n_f <- 24 # sample sizes\nx_m <- 14; x_f <- 21 # numbers of promotions\nprop.test(c(x_m, x_f), c(n_m, n_f))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x_m, x_f) out of c(n_m, n_f)\nX-squared = 3.7978, df = 1, p-value = 0.05132\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.57084188 -0.01249145\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.8750000"
  },
  {
    "objectID": "slides/lec-12.html#conditions",
    "href": "slides/lec-12.html#conditions",
    "title": "Classical inference",
    "section": "Conditions",
    "text": "Conditions\n\nIndependence within groups (same as case 1)\nIndependence between groups\nSuccess-failure condition for each group (10 successes and 10 failures in each group)"
  },
  {
    "objectID": "slides/lec-12.html#section-3",
    "href": "slides/lec-12.html#section-3",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for two proportions\n\n\nExercise 17.7\nExercise 17.13\nExercise 17.19\n\n\n\n\n\n\n07:00"
  },
  {
    "objectID": "slides/lec-12.html#case-3-one-mean",
    "href": "slides/lec-12.html#case-3-one-mean",
    "title": "Classical inference",
    "section": "Case 3 ‚Äì one mean",
    "text": "Case 3 ‚Äì one mean\n\nd <- ggplot2::mpg\nt.test(d$hwy, mu = 25)\n\n\n    One Sample t-test\n\ndata:  d$hwy\nt = -4.0071, df = 233, p-value = 0.00008274\nalternative hypothesis: true mean is not equal to 25\n95 percent confidence interval:\n 22.67324 24.20710\nsample estimates:\nmean of x \n 23.44017"
  },
  {
    "objectID": "slides/lec-12.html#section-4",
    "href": "slides/lec-12.html#section-4",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for one mean\n\n\nMake a histogram and a boxplot of the variable. Are the conditions satisfied?\nConstruct a 99% CI for hwy.\nHint: use the command help(t.test) to access the help file of the function t.test and see what parameter determines the confidence level.\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-12.html#larger-samples-have-a-smaller-se",
    "href": "slides/lec-12.html#larger-samples-have-a-smaller-se",
    "title": "Classical inference",
    "section": "Larger samples have a smaller SE",
    "text": "Larger samples have a smaller SE\n\n\n\n\n\n\nSample size matters\n\n\nLarge \\(n\\)\n\\(\\Rightarrow\\) small SE\n\\(\\Rightarrow\\) normal approximation with small sd\n\\(\\Rightarrow\\) normal approximation is more concentrated\n\\(\\Rightarrow\\) tighter CI and smaller p-values."
  },
  {
    "objectID": "slides/lec-12.html#section-5",
    "href": "slides/lec-12.html#section-5",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for two means\n\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?\nAre the conditions satisfied?\n\n\n\n\n\n\n03:00"
  },
  {
    "objectID": "slides/lec-12.html#conditions-1",
    "href": "slides/lec-12.html#conditions-1",
    "title": "Classical inference",
    "section": "Conditions",
    "text": "Conditions\n\nIndependence\nNormality ‚Äì can be relaxed for larger samples \\((n\\ge30)\\)\n\n\n\n\n\n\n\n\nStatistics as an art\n\n\nThe normality assumption is vague. The most important feature of the sample to verify is the presence of outliers.\nRule of thumb: if \\(n<30\\), there should not be any clear outlier; if \\(n\\ge30\\), there should not be any extreme outlier."
  },
  {
    "objectID": "slides/lec-12.html#case-4-two-means",
    "href": "slides/lec-12.html#case-4-two-means",
    "title": "Classical inference",
    "section": "Case 4 ‚Äì two means",
    "text": "Case 4 ‚Äì two means\nThere are two implementation; which on is more convenient depends on the structure of the data.\n\nTwo vectorsFormula\n\n\n\nd <- ggplot2::mpg\nt.test(hwy ~ year, data = d)\n\n\n    Welch Two Sample t-test\n\ndata:  hwy by year\nt = -0.032864, df = 231.64, p-value = 0.9738\nalternative hypothesis: true difference in means between group 1999 and group 2008 is not equal to 0\n95 percent confidence interval:\n -1.562854  1.511572\nsample estimates:\nmean in group 1999 mean in group 2008 \n          23.42735           23.45299 \n\n\n\n\n\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy)\n\n\n    Welch Two Sample t-test\n\ndata:  d$cty and d$hwy\nt = -13.755, df = 421.79, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -7.521683 -5.640710\nsample estimates:\nmean of x mean of y \n 16.85897  23.44017"
  },
  {
    "objectID": "slides/lec-12.html#conditions-2",
    "href": "slides/lec-12.html#conditions-2",
    "title": "Classical inference",
    "section": "Conditions",
    "text": "Conditions\n\nIndependence within groups\nIndependence between groups\nNormality in each group (same as case 3 ‚Äì one mean)"
  },
  {
    "objectID": "slides/lec-12.html#case-4bis-paired-means",
    "href": "slides/lec-12.html#case-4bis-paired-means",
    "title": "Classical inference",
    "section": "Case 4bis ‚Äì paired means",
    "text": "Case 4bis ‚Äì paired means\n\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  d$cty and d$hwy\nt = -44.492, df = 233, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.872628 -6.289765\nsample estimates:\nmean of the differences \n              -6.581197"
  },
  {
    "objectID": "slides/lec-12.html#conditions-3",
    "href": "slides/lec-12.html#conditions-3",
    "title": "Classical inference",
    "section": "Conditions",
    "text": "Conditions\n\nPaired observations\nIndependence between pairs\nNormality"
  },
  {
    "objectID": "slides/lec-12.html#section-6",
    "href": "slides/lec-12.html#section-6",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for paired means\n\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?\nAre the conditions satisfied?\n\n\n\n\n\n\n03:00\n\n\n\n\n\n\n\n\n\n\nAlways pair the observations\n\n\nIf the data can paired, you should always do it! Pairing data yields an analysis that is more powerful:\n\nnarrower CI\nsmaller p-value"
  },
  {
    "objectID": "slides/lec-12.html#case-5-regression",
    "href": "slides/lec-12.html#case-5-regression",
    "title": "Classical inference",
    "section": "Case 5 ‚Äì regression",
    "text": "Case 5 ‚Äì regression\n\nsimple linear regressionmultiple linear regressionlogistic regression\n\n\n\nm <- lm(hwy ~ cty, data = mpg)\ntidy(m)\n\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    0.892    0.469       1.90 5.84e-  2\n2 cty            1.34     0.0270     49.6  1.87e-125\n\n\n\n\n\nm <- lm(hwy ~ cty + displ, data = mpg)\ntidy(m)\n\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   1.15      1.21       0.949 3.43e- 1\n2 cty           1.33      0.0449    29.6   1.43e-80\n3 displ        -0.0343    0.148     -0.232 8.17e- 1\n\n\n\\(H_0: \\beta_1 = 0\\) when displ is included in the model\n\\(H_a: \\beta_1 \\neq 0\\) when displ is included in the model\n. . .\n\\(H_0: \\beta_2 = 0\\) when cty is included in the model\n\\(H_a: \\beta_2 \\neq 0\\) when cty is included in the model\n\n\n\nd <- heart_transplant %>% mutate(survived_binary = survived == \"alive\")\nm <- glm(survived_binary ~ age + transplant, family = \"binomial\", data = d)\ntidy(m)\n\n# A tibble: 3 x 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)           0.973     1.08       0.904 0.366  \n2 age                  -0.0763    0.0255    -2.99  0.00277\n3 transplanttreatment   1.82      0.668      2.73  0.00635"
  },
  {
    "objectID": "slides/lec-12.html#conditions-linear-regression",
    "href": "slides/lec-12.html#conditions-linear-regression",
    "title": "Classical inference",
    "section": "Conditions ‚Äì linear regression",
    "text": "Conditions ‚Äì linear regression\n\nLinearity\nIndependence\nNormality\nEqual variability (homoskedasticity)\n\n\\(\\Rightarrow\\) verify with a residual plot!"
  },
  {
    "objectID": "slides/lec-12.html#section-7",
    "href": "slides/lec-12.html#section-7",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - the classical approach for regression\n\n\nWhat condition(s) are violated by each of the following data sets (see next slide)?\nExercise 24.10\nExercise 24.13 ‚Äì parts a and b\nExercise 24.15 ‚Äì part b\nExercise 25.3\nExercise 25.7\nExercise 26.1\nExercise 26.1\n\n\n\n\n\n\n10:00"
  },
  {
    "objectID": "slides/lec-12.html#data-sets",
    "href": "slides/lec-12.html#data-sets",
    "title": "Classical inference",
    "section": "Data sets",
    "text": "Data sets"
  },
  {
    "objectID": "slides/lec-12.html#section-8",
    "href": "slides/lec-12.html#section-8",
    "title": "Classical inference",
    "section": "",
    "text": "Individual exercise - sample size and CI\n\n\nExercise 16.13 ‚Äì part e\nExercise 16.15 ‚Äì part b\nExercise 13.4 ‚Äì part d\n\n\n\n\n\n\n01:00"
  }
]