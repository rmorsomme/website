{
  "hash": "9fad257dddb3ebf686f839857b8c7a46",
  "result": {
    "markdown": "---\ntitle: \"Classical inference\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements {.smaller}\n\n-   \n\n## Recap {.smaller}\n\n-   Simple linear regression (case 5)\n\n    -   HT via simulation\n\n    -   CI via bootstrap\n\n## Outline\n\n-   Normal approximation\n-   Statistical inference via normal approximation\n    -   Hypothesis test\n\n    -   Confidence interval\n-   Conditions\n\n# Normal approximation\n\n## Normal distribution\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = seq(-5, 5, by = 0.01)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n$\\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped\n\n## Normal approximation\n\nThe normal distribution describes the variability of the different statistics\n\n-   $\\hat{p}$, $\\bar{x}$, $\\hat{\\beta}$\n\n-   simply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!\n\n. . .\n\n**Classical approach**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.\n\n## Normal approximation for $\\bar{x}$\n\nWe have seen that if a numerical variable $X$ is normally distributed\n\n$$\nX\\sim N(\\mu, \\sigma^2)\n$$\n\nthen the sample average is also normally distributed\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n## Condition for the normality of $\\bar{x}$  {.smaller}\n\nIn practice, we cannot assume that the variable $X$ is *exactly* normally distributed.\n\nBut as long as\n\n1.  the sample is large $(n\\ge 30)$, or\n\n2.  the variable is *approximately* normal: unimodal, roughly symmetric and no serious outlier\n\n$\\bar{x}$ is well approximated by a normal distribution\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n. . .\n\nSee the numerous histograms for case 3 (one mean) where the distribution of $\\bar{x}$ always looks pretty normal.\n\n## Normal approximation for $\\hat{p}$\n\nIf\n\n1.  the sample is large enough $(n\\ge30)$\n\n2.  $p$ is not extreme $(pn\\ge 10 \\text{ and } (1-p)n\\ge 10)$\n\nthe distribution of $\\hat{p}$ can be approximated by a normal distribution\n\n$$\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right)\n$$\n\n. . .\n\nSee the numerous histograms for case 1 (one proportion) where the distribution of $\\hat{p}$ always looks pretty normal.\n\n## Conditions are satisfied\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-2_1715d2579f0afebdc608ae3613034ad3'}\n\n```{.r .cell-code}\np <- 0.4; n <- 100 # conditions are satisfied: n>30, p*n>10 and (1-p)*n>10\nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation is good\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-3_de9547af6da84a70f9c2e0dd7accb0af'}\n\n```{.r .cell-code}\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Conditions are *not* satisfied\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-4_88a1310547697ca9fc36659aa97276a3'}\n\n```{.r .cell-code}\np <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 \nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation fails\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-5_0f58ab8a56d4f22363a1d12fbdf2e3d5'}\n\n```{.r .cell-code}\ntibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation for $\\hat{\\beta}$\n\nIf\n\nthe distribution of $\\hat{\\beta}$ can also be approximated by a normal distribution.\n\n# The classical approach to HT\n\n## The classical approach\n\nStep 1: we are interested in the distribution of the statistic under $H_0$.\n\n-   **Modern approach**: *simulate* from this distribution\n\n-   **Classical approach**: *approximate* this distribution with a normal distribution\n\n. . .\n\nStep 2: we want to compute the p-value\n\n-   **Modern approach**: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample\n\n-   **Classical approach**: the p-value is the *area under the curve* of the normal distribution that is at least as extreme as the observed statistic.\n\n## The classical approach in `R`\n\n`R` will compute the p-value for you. Here is what `R` does behind the scene:\n\n## HT for one proportion\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 1500 # sample size\nx <- 780  # number of successes\nprop.test(\n  x, n,             # observed data\n  p = 0.5,          # value in the null hypothesis\n  conf.level = 0.99 # confidence level for CI\n  )\n```\n\n::: {.cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 2.3207, df = 1, p-value = 0.1277\nalternative hypothesis: true p is not equal to 0.5\n99 percent confidence interval:\n 0.4864251 0.5533970\nsample estimates:\n   p \n0.52 \n```\n:::\n:::\n\n## Comparison with simulation-based HT\n\nThe simulation-based HT yielded a p-value of 0.127.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - HT for one proportion\n\nSuppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for $p$, the proportion of US adults who are democrats? What is the length of the interval?\n\nWhat 95% CI do you obtained if 6000 out of 10000 US adults say they are democrat? What is its length?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a4c5de\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## HT for two proportions\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_m <- 24; n_f <- 24     # sample sizes\nx_m <- 14; x_f <- 21     # numbers of promotions\nprop.test(\n  c(x_m, x_f), c(n_m, n_f), # observed data\n  )\n```\n\n::: {.cell-output-stdout}\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  c(x_m, x_f) out of c(n_m, n_f)\nX-squared = 3.7978, df = 1, p-value = 0.05132\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.57084188 -0.01249145\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.8750000 \n```\n:::\n:::\n\n## Comparison with simulation-based HT\n\nUsing the simulation-based HT, we found a p-value of 0.0435.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are *not* satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.\n\nA simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied.\n:::\n\n## HT for one mean\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nx <- d$hwy\nt.test(x, mu = 25)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  x\nt = -4.0071, df = 233, p-value = 0.00008274\nalternative hypothesis: true mean is not equal to 25\n95 percent confidence interval:\n 22.67324 24.20710\nsample estimates:\nmean of x \n 23.44017 \n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - HT for one mean\n\nConstruct a 99% CI for `hwy`.\n\nHint: use the command `help(t.test)` to access the help file of the function `t.test` and see what parameter determines the confidence level.\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a4c651\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## HT for two means\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nx1 <- d$hwy\nx2 <- d$cty\nt.test(x1, x2)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  x1 and x2\nt = 13.755, df = 421.79, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 5.640710 7.521683\nsample estimates:\nmean of x mean of y \n 23.44017  16.85897 \n```\n:::\n:::\n\n## HT for simple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    0.892    0.469       1.90 5.84e-  2\n2 cty            1.34     0.0270     49.6  1.87e-125\n```\n:::\n:::\n\n## HT for multiple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty + displ, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   1.15      1.21       0.949 3.43e- 1\n2 cty           1.33      0.0449    29.6   1.43e-80\n3 displ        -0.0343    0.148     -0.232 8.17e- 1\n```\n:::\n:::\n\n## HT for logistic regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- heart_transplant %>% mutate(survived_binary = survived == \"alive\")\nm <- glm(survived_binary ~ age + transplant, family = \"binomial\", data = d)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)           0.973     1.08       0.904 0.366  \n2 age                  -0.0763    0.0255    -2.99  0.00277\n3 transplanttreatment   1.82      0.668      2.73  0.00635\n```\n:::\n:::\n\n## Standard error\n\n**Standard error**: standard deviation of statistic\n\n-   the sd of $\\hat{p}$ is $SE=\\sqrt{\\frac{p(1-p)}{n}}$\n\n-   the sd of $\\bar{x}$ is $SE=\\sqrt{\\frac{\\sigma^2}{n}} \\approx \\sqrt{\\frac{s^2}{n}}$ where $s^2$ is an estimate of the population variance $\\sigma^2$ based on the sample.\n\n-   the sd of $\\hat{\\beta}$ has a complicated form.\n\n## Hypothesis test\n\nHow many SE is the observed sample from the null value?\n\n$$\n\\dfrac{\\text{point estimate} - \\text{null value}}{SE}\n$$\n\n-   For one proportion: $Z = \\dfrac{\\hat{p}-p_0}{SE} = \\dfrac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}$\n\n-   For one mean: $t = \\dfrac{\\bar{x}-\\mu_0}{SE} = \\dfrac{\\bar{x}-\\mu_0}{\\sqrt{\\frac{s^2}{n}}}$\n\n-   For the slope in a regression model: $t = \\dfrac{\\hat{\\beta}-0}{SE}$\n\n## \n\nIf the observed statistic is many SE away from the null value, then the observed sample is unlikely to have occurred under $H_0$.\n\n. . .\n\n**Question**: how many SE is enough?\n\n**Answer**: use the normal approximation to compute a p-value!\n\n. . .\n\n`R` will compute the p-value for you.\n\n## Confidence interval\n\n$$\nCI = \\text{point estimate} \\pm \\text{critical value} *SE\n$$\n\nThe critical value depends on the confidence level (90%, 95%, 99% CI) and the type of data.\n\nTo find the critical value corresponding to the confidence level `cl` use the following commands\n\n-   Proportion: `qnorm(1-(1-cl)/2)`\n\n    -   for a 95% CI, the critical value is 1.96, for a 99% CI it is 2.57\n\n-   Mean and regression slope: `qt(1-(1-cl)/2, n-1)` where $n$ is the sample size.\n\n    -   these critical values are slightly larger than those for proportions\n\n    -   as $n$ increases, this difference decreases.\n\n## Larger sample give a smaller SE\n\n::: callout-note\n## Sample size matters\n\nNote that as $n$ increases, the SE decreases. This means that a larger sample will give narrower CI and smaller p-values.\n:::\n\n# Recap\n\n## Recap\n\n-   Simple linear regression (case 5)\n    -   HT via simulation\n\n    -   CI via bootstrap",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}