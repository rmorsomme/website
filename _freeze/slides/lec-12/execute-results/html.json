{
  "hash": "c8ce915b9fb252528c499c16ce0bcf4d",
  "result": {
    "markdown": "---\ntitle: \"Classical inference\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements {.smaller}\n\n-   \n\n## Recap {.smaller}\n\n-   Simple linear regression (case 5)\n\n    -   HT via simulation\n\n    -   CI via bootstrap\n\n## Outline\n\n-   Normal approximation\n-   Statistical inference via normal approximation\n    -   Hypothesis test\n\n    -   Confidence interval\n-   Conditions\n\n# Normal approximation\n\n## Normal distribution\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = seq(-5, 5, by = 0.01)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\n$\\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped\n\n## Normal approximation\n\nThe normal distribution describes the variability of the different statistics\n\n-   $\\hat{p}$, $\\bar{x}$, $\\hat{\\beta}$\n\n-   simply look at all the histograms we have constructed from simulated samples (HT) bootstrap samples (CI)!\n\n. . .\n\n**Classical statistics**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.\n\n## Normal approximation for $\\bar{x}$\n\nYou have seen that if a numerical random variable is normally distributed\n\n$$\nX\\sim N(\\mu, \\sigma^2)\n$$\n\nthen the sample average is also normally distributed\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n## Condition for the normality of $\\bar{x}$\n\nIn practice, we cannot assume that the random variable $X$ is *exactly normally* distributed.\n\nBut as long as\n\n-   the sample is large enough $(n\\ge 10)$ (condition 1)\n\n-   the variable is approximately normal: unimodal, roughly symmetric and does not have serious outliers (condition 2)\n\nthe sample average is well approximated by a normal distribution\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n. . .\n\nSee the numerous histograms for case 3 (one mean) where the distribution of $\\bar{x}$ always looks pretty normal.\n\n## Normal approximation for $\\hat{p}$\n\nIf\n\n1.  the sample is large enough $(n\\ge30)$ (condition 1)\n\n2.  $p$ is not extreme $(pn\\ge 10 \\text{ and } (1-p)n\\ge 10)$ (condition 2)\n\nthen the distribution of $\\hat{p}$ can be approximated by a normal distribution\n\n$$\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right)\n$$\n\n. . .\n\nSee the numerous histograms for case 1 (one proportion) where the distribution of $\\hat{p}$ always looks pretty normal.\n\n## Conditions are satisfied\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-2_1715d2579f0afebdc608ae3613034ad3'}\n\n```{.r .cell-code}\np <- 0.4; n <- 100 # conditions are satisfied: n>30, p*n>10 and (1-p)*n>10\nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-3_de9547af6da84a70f9c2e0dd7accb0af'}\n\n```{.r .cell-code}\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Conditions are *not* satisfied\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-4_88a1310547697ca9fc36659aa97276a3'}\n\n```{.r .cell-code}\np <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 \nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 1e4){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation fails\n\n::: {.cell layout-align=\"center\" hash='lec-12_cache/revealjs/unnamed-chunk-5_0f58ab8a56d4f22363a1d12fbdf2e3d5'}\n\n```{.r .cell-code}\ntibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation for $\\hat{\\beta}$\n\nIf\n\nthe distribution of $\\hat{\\beta}$ can also be approximated by a normal distribution.\n\n## Standard error\n\n**Standard error**: standard deviation of statistic\n\n-   the sd of $\\hat{p}$ is $SE=\\sqrt{\\frac{p(1-p)}{n}}$\n\n-   the sd of $\\bar{x}$ is $SE=\\sqrt{\\frac{\\sigma^2}{n}} \\approx \\sqrt{\\frac{s^2}{n}}$ where $s^2$ is an estimate of the population variance $\\sigma^2$ based on the sample.\n\n-   the sd of $\\hat{\\beta}$ has a complicated form.\n\n## Hypothesis test\n\nHow many SE is the observed sample from the null value?\n\n$$\n\\dfrac{\\text{point estimate} - \\text{null value}}{SE}\n$$\n\n-   For one proportion: $Z = \\dfrac{\\hat{p}-p_0}{SE} = \\dfrac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}$\n\n-   For one mean: $t = \\dfrac{\\bar{x}-\\mu_0}{SE} = \\dfrac{\\bar{x}-\\mu_0}{\\sqrt{\\frac{s^2}{n}}}$\n\n-   For the slope in a regression model: $t = \\dfrac{\\hat{\\beta}-0}{SE}$\n\n## \n\nIf the observed statistic is many SE away from the null value, then the observed sample is unlikely to have occurred under $H_0$.\n\n. . .\n\n**Question**: how many SE is enough?\n\n**Answer**: use the normal approximation to compute a p-value!\n\n. . .\n\n`R` will compute the p-value for you.\n\n## Confidence interval\n\n$$\nCI = \\text{point estimate} \\pm \\text{critical value} *SE\n$$\n\nThe critical value depends on the confidence level (90%, 95%, 99% CI) and the type of data.\n\nTo find the critical value corresponding to the confidence level `cl` use the following commands\n\n-   Proportion: `qnorm(1-(1-cl)/2)`\n\n    -   for a 95% CI, the critical value is 1.96, for a 99% CI it is 2.57\n\n-   Mean and regression slope: `qt(1-(1-cl)/2, n-1)` where $n$ is the sample size.\n\n    -   these critical values are slightly larger than those for proportions\n\n    -   as $n$ increases, this difference decreases.\n\n## Larger sample give a smaller SE\n\n::: callout-note\n## Sample size matters\n\nNote that as $n$ increases, the SE decreases. This means that a larger sample will give narrower CI and smaller p-values.\n:::\n\n# Recap\n\n## Recap\n\n-   Simple linear regression (case 5)\n    -   HT via simulation\n\n    -   CI via bootstrap",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}