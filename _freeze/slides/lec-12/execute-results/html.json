{
  "hash": "f2c2ac4a50c0260efcce0b7779169a89",
  "result": {
    "markdown": "---\ntitle: \"Classical inference\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Welcome\n\n## Announcements {.smaller}\n\n-   Tuesday: lecture\n\n-   Wednesday: work on project\n\n-   Thursday: work on project (3:30-5:00pm)\n\n-   Friday: presentations\n\n## Recap {.smaller}\n\n-   HT via simulation\n\n-   CI via bootstrap\n\n-   5 cases\n\n    -   one proprotion\n\n    -   two proportions\n\n    -   one mean\n\n    -   two means\n\n    -   linear regression\n\n## Outline\n\n-   Normal approximation\n-   Classical approach to statistical inference\n-   Standard error\n-   Case 6 -- many proportions ($\\chi^2$ test)\n-   Case 7 -- many means (ANOVA)\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - warm up\n\nExercise [14.5](https://openintro-ims.netlify.app/decerr.html#chp14-exercises)\n\nExercise [17.2](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- parts a and c\n\nExercise [17.4](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- part c\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d33c\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n# Normal approximation\n\n## Normal distribution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = seq(-5, 5, by = 0.001)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n$\\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped\n\n## HT using a normal approximation\n\n![](images/lec-12/p-value.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## CI using a normal approximation\n\n![](images/lec-12/3-sigma.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## Normal approximation\n\nThe normal distribution describes the variability of the different statistics\n\n-   $\\hat{p}$, $\\bar{x}$, $\\hat{\\beta}$\n\n-   simply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!\n\n. . .\n\n**Classical approach**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.\n\n## Normal approximation for $\\bar{x}$\n\nWe have seen that if a numerical variable $X$ is normally distributed\n\n\n$$\nX\\sim N(\\mu, \\sigma^2)\n$$\n\n\nthen the sample average is also normally distributed\n\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n\n## Condition for the normality of $\\bar{x}$ {.smaller}\n\nIn practice, we cannot assume that the variable $X$ is *exactly* normally distributed.\n\nBut as long as\n\n1.  the sample is large, or\n\n2.  the variable is *approximately* normal: unimodal, roughly symmetric and no serious outlier\n\n$\\bar{x}$ is well approximated by a normal distribution\n\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n\n. . .\n\nSee the numerous histograms for case 3 (one mean) where the distribution of $\\bar{x}$ always looks pretty normal.\n\n## Normal approximation for $\\hat{p}$ {.smaller}\n\nIf\n\n1.  the observations are independent -- the **independence** condition\n\n2.  $p$ is not extreme and $n$ is not small $(pn\\ge 10 \\text{ and } (1-p)n\\ge 10)$ -- the **success-failure** condition\n\nthe distribution of $\\hat{p}$ can be approximated by a normal distribution\n\n\n$$\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right)\n$$\n\n\n::: callout-important\n## Success-failure condition for CI\n\nFor CI, we verify the success-failure condition using the sample proportion $\\hat{p}$:\n\n\n$$\n\\hat{p}n\\ge 10 \\text{ and } (1-\\hat{p})n\\ge 10\n$$\n\n:::\n\n## Conditions are satisfied\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 0.4; n <- 100 # conditions are satisfied: p*n>10 and (1-p)*n>10\nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 5e3){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Normal approximation is good\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## \n\n::: callout-tip\n## When the conditions are satisfied\n\nWhen the conditions are satisfied, the normal distribution will be a good approximation. The classical and modern (simulation, bootstrap) approaches to statistical inference will give the same results.\n:::\n\n## Conditions are *not* satisfied\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 \nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 5e3){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Normal approximation fails\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## \n\n::: callout-important\n## When conditions are not satisfied\n\nWhen the conditions are not satisfied, the normal distribution will not be a good approximation to the sampling distribution. In this case, we should not use the classical approach to statistical inference, but instead use simulation (HT) or bootstrap (CI).\n:::\n\n# The classical approach to HT and CI\n\n## The classical approach\n\nStep 1: we are interested in the distribution of the statistic under $H_0$.\n\n-   **Modern approach**: *simulate* from this distribution\n\n-   **Classical approach**: *approximate* this distribution with a normal distribution\n\n## HT\n\nStep 2: we want to compute the p-value\n\n-   **Modern approach**: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample\n\n-   **Classical approach**: the p-value is the *area under the curve* of the normal distribution that is at least as extreme as the observed statistic.\n\n## What `R` does\n\n`R` will compute the p-value for you. Here is what `R` does behind the scene:\n\n![](images/lec-12/p-value.png){fig-align=\"center\"}\n\n## CI\n\nStep 2: identify the upper and lower bounds of the CI\n\n-   **Modern approach**: find the appropriate percentiles among the simulated values\n\n-   **Classical approach**:find the appropriate percentiles of the normal approximation\n\n## What `R` does\n\n`R` will compute the upper and lower bounds for you. Here is what `R` does behind the scene:\n\n![](images/lec-12/3-sigma.png){fig-align=\"center\"}\n\n## Case 1 -- one proportion\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 1500 # sample size\nx <- 780  # number of successes\nprop.test(\n  x, n,             # observed data\n  p = 0.5,          # value in the null hypothesis\n  conf.level = 0.99 # confidence level for CI\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 2.3207, df = 1, p-value = 0.1277\nalternative hypothesis: true p is not equal to 0.5\n99 percent confidence interval:\n 0.4864251 0.5533970\nsample estimates:\n   p \n0.52 \n```\n:::\n:::\n\n\n## Comparison with simulation-based HT\n\nThe simulation-based HT yielded a p-value of 0.127.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.\n:::\n\n**Conditions**: independence, success-failure condition\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for one proportion\n\nSuppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for $p$, the proportion of US adults who are democrats? What is the length of the interval?\n\nWhat 95% CI do you obtain if 6000 out of 10000 US adults say they are democrat? What is its length?\n\nExercise [16.19](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- find the 95% CI. Are the conditions satisfied?\n\nExercise [16.21](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)\n\nExercise [16.25](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d202\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Case 2 -- two proportions\n\nConsider the gender discrimination study.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_m <- 24; n_f <- 24 # sample sizes\nx_m <- 14; x_f <- 21 # numbers of promotions\nprop.test(c(x_m, x_f), c(n_m, n_f))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  c(x_m, x_f) out of c(n_m, n_f)\nX-squared = 3.7978, df = 1, p-value = 0.05132\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.57084188 -0.01249145\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.8750000 \n```\n:::\n:::\n\n\n## Conditions\n\n1.  Independence within groups (same as case 1)\n\n2.  Independence between groups\n\n3.  Success-failure condition for each group (10 successes and 10 failures in each group)\n\n## Comparison with simulation-based HT\n\nUsing the simulation-based HT, we found a p-value of 0.0435.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are *not* satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.\n\nA simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for two proportions\n\nExercise [17.7](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n\nExercise [17.13](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n\nExercise [17.19](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d0b3\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">06</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Case 3 -- one mean\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$hwy, mu = 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  d$hwy\nt = -4.0071, df = 233, p-value = 0.00008274\nalternative hypothesis: true mean is not equal to 25\n95 percent confidence interval:\n 22.67324 24.20710\nsample estimates:\nmean of x \n 23.44017 \n```\n:::\n:::\n\n\n## Conditions\n\n1.  Independence\n\n2.  Normality -- can be relaxed for larger samples $(n\\ge30)$\n\n. . .\n\n::: callout-tip\n## Statistics as an art\n\nThe normality assumption is vague. The most important feature of the sample to verify is the presence of outliers.\n\nRule of thumb: if $n<30$, there should not be any clear outlier; if $n\\ge30$, there should not be any extreme outlier.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for one mean\n\nMake a histogram and a boxplot of the variable. Are the conditions satisfied?\n\nConstruct a 99% CI for `hwy`.\n\nHint: run the command `help(t.test)` to access the help file of the function `t.test` and see what parameter determines the confidence level.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d1bb\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Case 4 -- two means\n\nThere are two implementation; which one is more convenient depends on the structure of the data.\n\n::: panel-tabset\n## Two vectors\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(hwy ~ year, data = d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  hwy by year\nt = -0.032864, df = 231.64, p-value = 0.9738\nalternative hypothesis: true difference in means between group 1999 and group 2008 is not equal to 0\n95 percent confidence interval:\n -1.562854  1.511572\nsample estimates:\nmean in group 1999 mean in group 2008 \n          23.42735           23.45299 \n```\n:::\n:::\n\n\n## Formula\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  d$cty and d$hwy\nt = -13.755, df = 421.79, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -7.521683 -5.640710\nsample estimates:\nmean of x mean of y \n 16.85897  23.44017 \n```\n:::\n:::\n\n:::\n\n## Conditions\n\n1.  Independence within groups\n\n2.  Independence between groups\n\n3.  Normality in each group (same as case 3 -- one mean)\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for two means\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? What is the length of this CI?\n\nAre the conditions satisfied?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d264\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## Case 4bis -- paired means\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  d$cty and d$hwy\nt = -44.492, df = 233, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.872628 -6.289765\nsample estimates:\nmean of the differences \n              -6.581197 \n```\n:::\n:::\n\n\n## Conditions\n\n1.  Paired observations\n\n2.  Independence between pairs\n\n3.  Normality\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for paired means\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? What is the length of this CI? Compare it with the CI obtained when the observations were not paired.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8cfcf\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n. . .\n\n::: callout-important\n## Always pair the observations\n\nIf the data can paired, you should always do it! Pairing data yields an analysis that is more *powerful*:\n\n-   narrower CI\n\n-   smaller p-value\n:::\n\n## Case 5 -- regression\n\n::: panel-tabset\n## simple linear regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    0.892    0.469       1.90 5.84e-  2\n2 cty            1.34     0.0270     49.6  1.87e-125\n```\n:::\n:::\n\n\n## multiple linear regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty + displ, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   1.15      1.21       0.949 3.43e- 1\n2 cty           1.33      0.0449    29.6   1.43e-80\n3 displ        -0.0343    0.148     -0.232 8.17e- 1\n```\n:::\n:::\n\n\n## logistic regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- heart_transplant %>% mutate(survived_binary = survived == \"alive\")\nm <- glm(survived_binary ~ age + transplant, family = \"binomial\", data = d)\ntidy(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)           0.973     1.08       0.904 0.366  \n2 age                  -0.0763    0.0255    -2.99  0.00277\n3 transplanttreatment   1.82      0.668      2.73  0.00635\n```\n:::\n:::\n\n:::\n\n## Conditions (LINE) -- linear regression\n\n1.  Linearity\n\n2.  Independence\n\n3.  Normality\n\n4.  Equal variability (homoskedasticity)\n\n$\\Rightarrow$ verify with a residual plot!\n\n## Data sets\n\n![](images/lec-12/conditions.png){fig-align=\"center\"}\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for regression\n\nWhat condition(s) are violated by each of the following data sets (see next slide)?\n\nExercise [24.10](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises)\n\nExercise [24.13](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- parts a and b\n\nExercise [24.15](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- part b\n\nExercise [25.3](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)\n\nExercise [25.7](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)\n\nExercise [26.1](https://openintro-ims.netlify.app/inf-model-logistic.html)\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d20e\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n\n# Standard error\n\n## Standard error {.smaller}\n\n**Standard error (SE)**: standard deviation of the normal approximation.\n\nThe SE measures the variability of the statistic.\n\n-   $SE(\\hat{p})=\\sqrt{\\frac{p(1-p)}{n}}$\n\n-   $SE(\\hat{p}_{diff})=\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}$\n\n-   $SE(\\bar{x}) = \\sqrt{\\frac{\\sigma^2}{n}}$\n\n-   $SE(\\bar{x}_{diff}) = \\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}$\n\n-   $SE(\\hat{\\beta})$ has a complicated form.\n\n::: callout-note\n## Sample size and SE\nNote the role of the sample size on the SE!\n:::\n\n## Larger samples have a smaller SE\n\n::: callout-note\n## Sample size matters\n\nLarger $n$\n\n$\\Rightarrow$ smaller SE\n\n$\\Rightarrow$ normal approximation with smaller sd\n\n$\\Rightarrow$ normal approximation is more concentrated\n\n$\\Rightarrow$ tighter CI and smaller p-value.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - sample size and CI\n\nExercise [16.13](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part e\n\nExercise [16.15](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part b\n\nExercise [13.4](https://openintro-ims.netlify.app/foundations-mathematical.html#chp13-exercises) -- part d\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d044\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n# Case 6 -- many proportions ($\\chi^2$ test)\n\n## Reproducibility\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nask <- openintro::ask %>%\n  mutate(\n    response = if_else(response == \"disclose\", \"Disclose problem\", \"Hide problem\"),\n    question_class = case_when(\n      question_class == \"general\" ~ \"General\",\n      question_class == \"neg_assumption\" ~ \"Negative assumption\",\n      question_class == \"pos_assumption\" ~ \"Positive assumption\"\n    ),\n    question_class = fct_relevel(question_class, \"General\", \"Positive assumption\", \"Negative assumption\")\n  )\n```\n:::\n\n\n## Example -- defective iPod\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Question </th>\n   <th style=\"text-align:right;\"> Disclose problem </th>\n   <th style=\"text-align:right;\"> Hide problem </th>\n   <th style=\"text-align:right;\"> Total </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> General </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 71 </td>\n   <td style=\"text-align:right;\"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Positive assumption </td>\n   <td style=\"text-align:right;\"> 23 </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:right;\"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Negative assumption </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:right;\"> 37 </td>\n   <td style=\"text-align:right;\"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total </td>\n   <td style=\"text-align:right;\"> 61 </td>\n   <td style=\"text-align:right;\"> 158 </td>\n   <td style=\"text-align:right;\"> 219 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## HT, no CI\n\n$H_0$: the response is independent of the question asked\n\n$H_a$: the response depends on the question asked\n\nWe will not quantify the differences between the three question with CIs.\n\n## Expected counts\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Disclose problem</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Hide problem</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"1\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Total</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\">  </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\">  </th>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\">  </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> General </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (20.33) </td>\n   <td style=\"text-align:right;\"> 71 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (52.67) </td>\n   <td style=\"text-align:right;width: 5em; \"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> Positive assumption </td>\n   <td style=\"text-align:right;\"> 23 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (20.33) </td>\n   <td style=\"text-align:right;\"> 50 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (52.67) </td>\n   <td style=\"text-align:right;width: 5em; \"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> Negative assumption </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (20.33) </td>\n   <td style=\"text-align:right;\"> 37 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> (52.67) </td>\n   <td style=\"text-align:right;width: 5em; \"> 73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> Total </td>\n   <td style=\"text-align:right;\"> 61 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> NA </td>\n   <td style=\"text-align:right;\"> 158 </td>\n   <td style=\"text-align:left;font-style: italic;color: #569BBD !important;\"> NA </td>\n   <td style=\"text-align:right;width: 5em; \"> 219 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## Chance or effect?\n\nIs the difference between the *expected* and *observed* counts is due to\n\n-   chance alone, or\n\n-   the fact that the way people responded depended on the question asked?\n\n. . .\n\n$\\chi^2$ (\"Kai-squared\") statistic:\n\n\n$$\n\\chi^2 = \\dfrac{(O_{11} - E_{11})^2}{E_{11}} + \\dfrac{(O_{21} - E_{21})^2}{E_{21}} + \\dots + \\dfrac{(O_{32} - E_{32})^2}{E_{32}}\n$$\n\n\n## Computing $\\chi^2$ {.smaller}\n\n\n$$\n\\begin{aligned}\n&\\text{General formula} &&\n    \\frac{(\\text{observed count } - \\text{expected count})^2}\n        {\\text{expected count}} \\\\\n&\\text{Row 1, Col 1} &&\n    \\frac{(2 - 20.33)^2}{20.33} = 16.53 \\\\\n&\\text{Row 2, Col 1} &&\n    \\frac{(23 - 20.33)^2}{20.33} = 0.35 \\\\\n& \\hspace{9mm}\\vdots &&\n    \\hspace{13mm}\\vdots \\\\\n&\\text{Row 3, Col 2} &&\n    \\frac{(37 - 52.67)^2}{52.67} = 4.66\n\\end{aligned}\n$$\n\n$$\\chi^2 = 16.53 + 0.35 + \\dots + 4.66 = 40.13$$\n\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## The approximate distribution of $\\chi^2$\n\nWhen the conditions of\n\n1.  independence\n\n2.  $>5$ expected counts per cell\n\nare satisfied, the $\\chi^2$ statistic approximately follows a $\\chi^2$ distribution.\n\n## Computing the p-value\n\n![](images/lec-12/chi-squared.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## The $\\chi^2$ test in `R`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(ask)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  question_class      question                               response        \n  <fct>               <chr>                                  <chr>           \n1 General             What can you tell me about it?         Hide problem    \n2 Positive assumption It doesn't have any problems, does it? Hide problem    \n3 Positive assumption It doesn't have any problems, does it? Disclose problem\n4 Negative assumption What problems does it have?            Disclose problem\n5 General             What can you tell me about it?         Hide problem    \n6 Negative assumption What problems does it have?            Disclose problem\n```\n:::\n\n```{.r .cell-code}\nchisq.test(ask$response, ask$question_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  ask$response and ask$question_class\nX-squared = 40.128, df = 2, p-value = 0.000000001933\n```\n:::\n:::\n\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - $\\chi^2$ test\n\nExercise [18.15](https://openintro-ims.netlify.app/inference-tables.html#chp18-exercises)\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d217\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">01</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">30</span></code>\n</div>\n```\n:::\n:::\n\n\n## HT via simulation {.smaller}\n\nWhen the conditions are not met, you need to conduct a HT via *simulation*.\n\n1.  Simulate artificial samples under $H_0$ by shuffling the response variable\n2.  Compute the $\\chi^2$ statistic of each simulated sample\n3.  Determine how extreme the $\\chi^2$ statistic of the observed sample is by computing a p-value\n\nSee [Section 18.1](https://openintro-ims.netlify.app/inference-tables.html#randomization-test-of-independence) for an example.\n\n# Case 7 -- many means (ANOVA)\n\n## Reproducibility\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlb_players_18 <- openintro::mlb_players_18 %>%\n  filter(\n    AB >= 100, \n    !position %in% c(\"P\", \"DH\")\n  ) %>%\n  mutate(\n    position = case_when(\n      position %in% c(\"LF\", \"CF\", \"RF\")       ~ \"OF\",\n      position %in% c(\"1B\", \"2B\", \"3B\", \"SS\") ~ \"IF\",\n      TRUE                                    ~ position\n    ),\n    position = fct_relevel(position, \"OF\", \"IF\", \"C\")\n  )\n```\n:::\n\n\n## Example -- batting performance and position\n\n![](images/lec-12/anova-data.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## HT, no CI\n\n$H_0: \\mu_{OF} = \\mu_{IF} = \\mu_{C}$: (the batting performance is the same across all three positions)\n\n$H_a$: at least one mean is different\n\nWe will not quantify the differences between the three positions with CIs.\n\n## ANOVA in `R`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- aov(OBP ~ position, data = mlb_players_18)\ntidy(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 6\n  term         df  sumsq  meansq statistic  p.value\n  <chr>     <dbl>  <dbl>   <dbl>     <dbl>    <dbl>\n1 position      2 0.0161 0.00803      5.08  0.00662\n2 Residuals   426 0.674  0.00158     NA    NA      \n```\n:::\n:::\n\n\n## Conditions\n\n1.  Independence within\n\n2.  Independence between\n\n3.  Normality (sample size and outliers)\n\n4.  Constant variance\n\nVerify assumptions 3 and 4 with side-sby-side histograms\n\n## Side-by-side histograms\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(mlb_players_18) +\n  geom_histogram(aes(OBP)) +\n  facet_grid(position ~ .)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n## \n::: {.callout-tip icon=\"false\"}\n## Individual exercise - ANOVA\nExercise [22.5](https://openintro-ims.netlify.app/inference-many-means.html#chp22-exercises)\n\nExercise [22.9](https://openintro-ims.netlify.app/inference-many-means.html#chp22-exercises) -- parts a and b\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a8d2cd\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n\n## HT via simulation {.smaller}\n\nWhen the conditions are not met, you need to conduct a HT via *simulation*.\n\n1.  Simulate artificial samples under $H_0$ by shuffling the response variable\n2.  Compute the $F$ statistic of each simulated sample (see [Section 22.2](https://openintro-ims.netlify.app/inference-many-means.html#randANOVA))\n3.  Determine how extreme the $F$ statistic of the observed sample is by computing a p-value\n\nSee [Section 22.2](https://openintro-ims.netlify.app/inference-many-means.html#randANOVA) for an example.\n\n# Recap\n\n## Recap\n\n-   Normal approximation\n-   Classical approach to statistical inference\n-   Standard error\n-   Case 6 -- many proportions ($\\chi^2$ test)\n-   Case 7 -- many means (ANOVA)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"../site_libs/countdown-0.3.5/countdown.js\"></script>\r\n<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}