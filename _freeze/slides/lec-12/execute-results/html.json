{
  "hash": "1f36e823e53386e1bbd70481ab1cef5b",
  "result": {
    "markdown": "---\ntitle: \"Classical inference\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements {.smaller}\n\n-   Tuesday: lecture\n\n-   Wednesday: work on project\n\n-   Thursday: work on project (3:30-5:00pm)\n\n-   Friday: presentations\n\n## Recap  {.smaller}\n\n-   HT via simulation\n\n-   CI via bootstrap\n\n-   5 cases\n\n    -   one proprotion\n\n    -   two proportions\n\n    -   one mean\n\n    -   two means\n\n    -   linear regression\n\n## Outline\n\n-   Normal approximation\n-   Classical approach to statistical inference\n-   Standard error\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - warm up\n\nExercise [14.5](https://openintro-ims.netlify.app/decerr.html#chp14-exercises)\n\nExercise [17.2](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- parts a and c\n\nExercise [17.4](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- part c\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a78fea\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Normal approximation\n\n## Normal distribution\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(x = seq(-5, 5, by = 0.001)) %>%\n  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%\n  ggplot() + \n  geom_line(aes(x, normal))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n$\\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped\n\n## HT using a normal approximation\n\n![](images/lec-12/p-value.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## CI using a normal approximation\n\n![](images/lec-12/3-sigma.png){fig-align=\"center\"}\n\nSource: [IMS](https://www.openintro.org/book/ims/)\n\n## Normal approximation\n\nThe normal distribution describes the variability of the different statistics\n\n-   $\\hat{p}$, $\\bar{x}$, $\\hat{\\beta}$\n\n-   simply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!\n\n. . .\n\n**Classical approach**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.\n\n## Normal approximation for $\\bar{x}$\n\nWe have seen that if a numerical variable $X$ is normally distributed\n\n$$\nX\\sim N(\\mu, \\sigma^2)\n$$\n\nthen the sample average is also normally distributed\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n## Condition for the normality of $\\bar{x}$ {.smaller}\n\nIn practice, we cannot assume that the variable $X$ is *exactly* normally distributed.\n\nBut as long as\n\n1.  the sample is large, or\n\n2.  the variable is *approximately* normal: unimodal, roughly symmetric and no serious outlier\n\n$\\bar{x}$ is well approximated by a normal distribution\n\n$$\n\\bar{x} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n. . .\n\nSee the numerous histograms for case 3 (one mean) where the distribution of $\\bar{x}$ always looks pretty normal.\n\n## Normal approximation for $\\hat{p}$ {.smaller}\n\nIf\n\n1.  the observations are independent -- the **independence** condition\n\n2.  $p$ is not extreme and $n$ is not small $(pn\\ge 10 \\text{ and } (1-p)n\\ge 10)$ -- the **success-failure** condition\n\nthe distribution of $\\hat{p}$ can be approximated by a normal distribution\n\n$$\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right)\n$$\n\n::: callout-important\n## Success-failure condition for CI\n\nFor CI, we verify the success-failure condition using the sample proportion $\\hat{p}$:\n\n$$\n\\hat{p}n\\ge 10 \\text{ and } (1-\\hat{p})n\\ge 10\n$$\n:::\n\n## Conditions are satisfied\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 0.4; n <- 100 # conditions are satisfied: p*n>10 and (1-p)*n>10\nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 5e3){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation is good\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() +\n  geom_line(aes(x = p_hat, y = normal_approximation))\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\n::: callout-tip\n## When the conditions are satisfied\n\nWhen the conditions are satisfied, the normal distribution will be a good approximation. The classical and modern (simulation, bootstrap) approaches to statistical inference will give the same results.\n:::\n\n## Conditions are *not* satisfied\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 \nresults <- tibble(p_hat = numeric())\nfor(i in 1 : 5e3){\n  sim     <- purrr::rbernoulli(n, p)\n  p_hat   <- mean(sim)\n  results <- results %>% add_row(p_hat)\n}\nggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Normal approximation fails\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%\n  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%\n  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)\n```\n\n::: {.cell-output-display}\n![](lec-12_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\n::: callout-important\n## When conditions are not satisfied\n\nWhen the conditions are not satisfied, the normal distribution will not be a good approximation to the sampling distribution. In this case, we should not use the classical approach to statistical inference, but instead use simulation (HT) or bootstrap (CI).\n:::\n\n# The classical approach to HT and CI\n\n## The classical approach\n\nStep 1: we are interested in the distribution of the statistic under $H_0$.\n\n-   **Modern approach**: *simulate* from this distribution\n\n-   **Classical approach**: *approximate* this distribution with a normal distribution\n\n## HT\n\nStep 2: we want to compute the p-value\n\n-   **Modern approach**: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample\n\n-   **Classical approach**: the p-value is the *area under the curve* of the normal distribution that is at least as extreme as the observed statistic.\n\n## What `R` does\n\n`R` will compute the p-value for you. Here is what `R` does behind the scene:\n\n![](images/lec-12/p-value.png){fig-align=\"center\"}\n\n## CI\n\nStep 2: identify the upper and lower bounds of the CI\n\n-   **Modern approach**: find the appropriate percentiles among the simulated values\n\n-   **Classical approach**:find the appropriate percentiles of the normal approximation\n\n## What `R` does\n\n`R` will compute the upper and lower bounds for you. Here is what `R` does behind the scene:\n\n![](images/lec-12/3-sigma.png){fig-align=\"center\"}\n\n## Case 1 -- one proportion\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 1500 # sample size\nx <- 780  # number of successes\nprop.test(\n  x, n,             # observed data\n  p = 0.5,          # value in the null hypothesis\n  conf.level = 0.99 # confidence level for CI\n  )\n```\n\n::: {.cell-output-stdout}\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability 0.5\nX-squared = 2.3207, df = 1, p-value = 0.1277\nalternative hypothesis: true p is not equal to 0.5\n99 percent confidence interval:\n 0.4864251 0.5533970\nsample estimates:\n   p \n0.52 \n```\n:::\n:::\n\n## Comparison with simulation-based HT\n\nThe simulation-based HT yielded a p-value of 0.127.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.\n:::\n\n**Conditions**: independence, success-failure condition\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for one proportion\n\nSuppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for $p$, the proportion of US adults who are democrats? What is the length of the interval?\n\nWhat 95% CI do you obtain if 6000 out of 10000 US adults say they are democrat? What is its length?\n\nExercise [16.19](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- find the 95% CI. Are the conditions satisfied?\n\nExercise [16.21](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)\n\nExercise [16.25](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a79187\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Case 2 -- two proportions\n\nConsider the gender discrimination study.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_m <- 24; n_f <- 24 # sample sizes\nx_m <- 14; x_f <- 21 # numbers of promotions\nprop.test(c(x_m, x_f), c(n_m, n_f))\n```\n\n::: {.cell-output-stdout}\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  c(x_m, x_f) out of c(n_m, n_f)\nX-squared = 3.7978, df = 1, p-value = 0.05132\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.57084188 -0.01249145\nsample estimates:\n   prop 1    prop 2 \n0.5833333 0.8750000 \n```\n:::\n:::\n\n## Conditions\n\n1.  Independence within groups (same as case 1)\n\n2.  Independence between groups\n\n3.  Success-failure condition for each group (10 successes and 10 failures in each group)\n\n## Comparison with simulation-based HT\n\nUsing the simulation-based HT, we found a p-value of 0.0435.\n\n::: callout-note\n## A good normal approximation\n\nWhen the conditions for the normal approximation are *not* satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.\n\nA simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for two proportions\n\nExercise [17.7](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n\nExercise [17.13](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n\nExercise [17.19](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a78fd2\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">06</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Case 3 -- one mean\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$hwy, mu = 25)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  d$hwy\nt = -4.0071, df = 233, p-value = 0.00008274\nalternative hypothesis: true mean is not equal to 25\n95 percent confidence interval:\n 22.67324 24.20710\nsample estimates:\nmean of x \n 23.44017 \n```\n:::\n:::\n\n## Conditions\n\n1.  Independence\n\n2.  Normality -- can be relaxed for larger samples $(n\\ge30)$\n\n. . .\n\n::: callout-tip\n## Statistics as an art\n\nThe normality assumption is vague. The most important feature of the sample to verify is the presence of outliers.\n\nRule of thumb: if $n<30$, there should not be any clear outlier; if $n\\ge30$, there should not be any extreme outlier.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for one mean\n\nMake a histogram and a boxplot of the variable. Are the conditions satisfied?\n\nConstruct a 99% CI for `hwy`.\n\nHint: run the command `help(t.test)` to access the help file of the function `t.test` and see what parameter determines the confidence level.\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a79143\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Case 4 -- two means\n\nThere are two implementation; which on is more convenient depends on the structure of the data.\n\n::: panel-tabset\n## Two vectors\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(hwy ~ year, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  hwy by year\nt = -0.032864, df = 231.64, p-value = 0.9738\nalternative hypothesis: true difference in means between group 1999 and group 2008 is not equal to 0\n95 percent confidence interval:\n -1.562854  1.511572\nsample estimates:\nmean in group 1999 mean in group 2008 \n          23.42735           23.45299 \n```\n:::\n:::\n\n## Formula\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  d$cty and d$hwy\nt = -13.755, df = 421.79, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -7.521683 -5.640710\nsample estimates:\nmean of x mean of y \n 16.85897  23.44017 \n```\n:::\n:::\n:::\n\n## Conditions\n\n1.  Independence within groups\n\n2.  Independence between groups\n\n3.  Normality in each group (same as case 3 -- one mean)\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for two means\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?\n\nAre the conditions satisfied?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a79100\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Case 4bis -- paired means\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nt.test(d$cty, d$hwy, paired = TRUE)\n```\n\n::: {.cell-output-stdout}\n```\n\n\tPaired t-test\n\ndata:  d$cty and d$hwy\nt = -44.492, df = 233, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.872628 -6.289765\nsample estimates:\nmean of the differences \n              -6.581197 \n```\n:::\n:::\n\n## Conditions\n\n1.  Paired observations\n\n2.  Independence between pairs\n\n3.  Normality\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for paired means\n\nWhat is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?\n\nAre the conditions satisfied?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a79097\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n. . .\n\n::: callout-important\n## Always pair the observations\n\nIf the data can paired, you should always do it! Pairing data yields an analysis that is more *powerful*:\n\n-   narrower CI\n\n-   smaller p-value\n:::\n\n## Case 5 -- regression\n\n::: panel-tabset\n## simple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    0.892    0.469       1.90 5.84e-  2\n2 cty            1.34     0.0270     49.6  1.87e-125\n```\n:::\n:::\n\n## multiple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty + displ, data = mpg)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   1.15      1.21       0.949 3.43e- 1\n2 cty           1.33      0.0449    29.6   1.43e-80\n3 displ        -0.0343    0.148     -0.232 8.17e- 1\n```\n:::\n:::\n\n$H_0: \\beta_1 = 0$ when `displ` is included in the model\n\n$H_a: \\beta_1 \\neq 0$ when `displ` is included in the model\n\n$H_0: \\beta_2 = 0$ when `cty` is included in the model\n\n$H_a: \\beta_2 \\neq 0$ when `cty` is included in the model\n\n## logistic regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- heart_transplant %>% mutate(survived_binary = survived == \"alive\")\nm <- glm(survived_binary ~ age + transplant, family = \"binomial\", data = d)\ntidy(m)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 x 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)           0.973     1.08       0.904 0.366  \n2 age                  -0.0763    0.0255    -2.99  0.00277\n3 transplanttreatment   1.82      0.668      2.73  0.00635\n```\n:::\n:::\n:::\n\n## Conditions (LINE) -- linear regression\n\n1.  Linearity\n\n2.  Independence\n\n3.  Normality\n\n4.  Equal variability (homoskedasticity)\n\n$\\Rightarrow$ verify with a residual plot!\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - the classical approach for regression\n\nWhat condition(s) are violated by each of the following data sets (see next slide)?\n\nExercise [24.10](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises)\n\nExercise [24.13](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- parts a and b\n\nExercise [24.15](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- part b\n\nExercise [25.3](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)\n\nExercise [25.7](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)\n\nExercise [26.1](https://openintro-ims.netlify.app/inf-model-logistic.html)\n\nExercise [26.1](https://openintro-ims.netlify.app/inf-model-logistic.html)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a793b0\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">10</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Data sets\n\n![](images/lec-12/conditions.png){fig-align=\"center\"}\n\n# Standard error\n\n## Standard error  {.smaller}\n\n**Standard error (SE)**: standard deviation of the normal approximation\n\nIt measures the variability of the sample statistic.\n\n-   $SE(\\hat{p})=\\sqrt{\\frac{p(1-p)}{n}}$\n\n-   $SE(\\hat{p}_{diff})=\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}$\n\n-   $SE(\\bar{x}) = \\sqrt{\\frac{\\sigma^2}{n}}$\n\n-   $SE(\\bar{x}_{diff}) = \\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}$\n\n-   $SE(\\hat{\\beta})$ has a complicated form.\n\n::: callout-note\nNote the role of the sample size!\n:::\n\n## Larger samples have a smaller SE\n\n::: callout-note\n## Sample size matters\n\nLarge $n$\n\n$\\Rightarrow$ small SE\n\n$\\Rightarrow$ normal approximation with small sd\n\n$\\Rightarrow$ normal approximation is more concentrated\n\n$\\Rightarrow$ tighter CI and smaller p-values.\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Individual exercise - sample size and CI\n\nExercise [16.13](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part e\n\nExercise [16.15](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part b\n\nExercise [13.4](https://openintro-ims.netlify.app/foundations-mathematical.html#chp13-exercises) -- part d\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a79337\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Recap\n\n## Recap\n\n-   Normal approximation\n-   Classical approach to statistical inference\n    -   case 1 -- one proportion\n    -   case 2 -- two proportions\n    -   case 3 -- one mean\n    -   case 4 -- two means\n    -   case 4bis -- paired means\n    -   case 5 -- regression\n-   Standard error",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}