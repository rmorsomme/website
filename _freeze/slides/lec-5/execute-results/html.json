{
  "hash": "e69736cb5b953855267b30800d214958",
  "result": {
    "markdown": "---\ntitle: \"Multiple Linear Regression Models\"\nsubtitle: \"STA 101 - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[sta101-suI22.github.io/website](https://sta101-suI22.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-link: true\n    self-contained: true\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   assignments\n\n## Recap\n\n::: incremental\n-   simple linear regression model $$\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n    $$\n-   residuals\n-   least-square estimates\n-   parameter interpretation\n-   model comparison with \\$R\\^2%\n-   outliers\n:::\n\n## Outline\n\n-   multiple linear regression\n-   feature engineering\n-   model comparison\n-   predictive performance\n\n# Multiple linear regression\n\nRemember, to improve our initial model with $(\\beta_0, \\beta_1) = (1, 1.3)$, we could (i) find better estimates, (ii) use additional predictors\n\n. . .\n\n-   for (i), the least-square estimates are usually very good\n-   for (ii), we use a **multiple linear regression model**\n\n. . .\n\nFor instance, to predict `hwy` we could more variables than just `cty`.\n\n## The `mpg` data set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nhead(d, n = 4)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n```\n:::\n:::\n\nInstead of fitting a model with only `cty` or only `displ`, we could fit a model with both predictors!\n\n## Linear regression with 2 predictors\n\nThe model equation is $$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty} + \\beta_2 \\text{displ}\n$$\n\nWe can find the least-square estimate (minimizing the SSR \\[SSE\\]) with the command `lm` on `R`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ cty + displ, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty + displ, data = d)\n\nCoefficients:\n(Intercept)          cty        displ  \n    1.15145      1.32914     -0.03432  \n```\n:::\n:::\n\nwhich give the following regression equation\n\n$$\n\\text{hwy} \\approx 1.15145 + 1.32914 \\text{cty} - 0.03432  \\text{displ}\n$$\n\n## Special case: categorical predictor\n\nIn a regression model, categorical predictors are represented using indicator variables.\n\nTo represent a categorical predictor with $k$ levels, we use $(k-1)$ indicator variables.\n\n## Including `drv`\n\nFor instance, the categorical variable `drv` has $k=3$ levels (`4`, `f` and `r`), so we can represent it with 2 indicator variables with the following model equation\n\n$$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{drv_f} + \\beta_2 \\text{drv_r}\n$$\n\n. . .\n\nNote that for a binary variable $k=2$, so we only need $k-1=1$ indicator variable (see previous set of slides).\n\n## Using categorical predictors in `R`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ drv, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ drv, data = d)\n\nCoefficients:\n(Intercept)         drvf         drvr  \n     19.175        8.986        1.825  \n```\n:::\n:::\n\nThe equation with the least-square estimates is $$\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n$$\n\n## Intepreting the output\n\n$$\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n$$\n\nIf a new vehicle has a `drv` that is:\n\n-   `4`, then the indicator variables `drv_f` and `drv_r` take the value 0, and the prediction is $\\widehat{hwy} = 19.175$\n-   `f`, then the indicator variables `drv_f` takes the value 1 and `drv_r` the value 0, and the prediction is $\\widehat{hwy} = 19.175 + 8.986 = 28.161$\n-   `r`, then the indicator variables `drv_f` takes the value 0 and `drv_r` the value 1, and the prediction is $\\widehat{hwy} = 19.175 + 1.825 = 21$\n\n. . .\n\nThe level `4` is called the **reference level**.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - categorical predictor\n\n-   Exercises 8.5, 8.7\n\n-   In addition\n\ni.  fit the first models in `R`\nii. identify the baseline level of the categorical variable in each model.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771e88\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Fitting a large model\n\nLet us fit a model with `cty`, `drv` and `disp`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_large <- lm(hwy ~ cty + drv + displ, data = d)\nm_large\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty + drv + displ, data = d)\n\nCoefficients:\n(Intercept)          cty         drvf         drvr        displ  \n      3.424        1.157        2.158        2.360       -0.208  \n```\n:::\n:::\n\nIts $R^2$ is 0.9384357.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m_large)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9384357\n```\n:::\n:::\n\n. . .\n\nUnsurprisingly, including additional predictors makes the regression line closer to the points $\\Rightarrow$ residuals are smaller $\\Rightarrow$ SSR is smaller $\\Rightarrow$ $R^2$ is larger.\n\n## Fitting a larger model\n\nActually, I include all predictors (except for `model`).\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_larger <- lm(hwy ~ manufacturer + displ + year + cyl + trans + drv + cty + fl + class, data = d)\n```\n:::\n\nThanks to the additional predictors, the residuals are very small, making $R^2$ close to $1$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m_larger)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9773386\n```\n:::\n:::\n\n. . .\n\nWe will see in the next lecture that this is not always a good sign.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - multiple linear regression\n\n-   Exercises 8.9\n\n-   In addition,\n\ni.  fit the model in `R`\nii. identify the type of each variable\niii. identify the baseline level of the categorical predictors\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd_birth <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771d1a\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Statistics as an art -- feature engineering\n\nWe saw that adding predictors to the model seems to help.\n\nHowever, the variables including in the data set, e.g. `displ`, `year`, etc, may not be the most useful predictors for `hwy`.\n\n. . .\n\n**Feature engineering** refers to the creation of new predictors from existing ones.\n\n. . .\n\nThis is where your understanding of the data makes, scientific knowledge, etc, makes a big difference.\n\n## Transforming a variable\n\nConsider the predictor `displ`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(displ, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-5_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n. . .\n\nThe relation bwteen `displ` and `hwy` is not exactly linear.\n\n## \n\nLet us include the predictor $\\dfrac{1}{\\text{displ}}$ to capture this nonlinear relation.\n\nThe model equation is $$\n\\text{hwy} \\approx \\beta_0+ \\beta_1 \\text{displ} + \\beta_2 \\dfrac{1}{\\text{displ}}\n$$ The least-square coefficient estimates are\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|\"}\nd_transf <- mutate(d, displ_inv = 1/displ)\nm <- lm(hwy ~ displ + displ_inv, data = d_transf)\nm\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ displ + displ_inv, data = d_transf)\n\nCoefficients:\n(Intercept)        displ    displ_inv  \n    12.2601      -0.2332      36.1456  \n```\n:::\n:::\n\n## \n\nAnd the regression lines captures the nonlinear relation.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(m) %>%\n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n```\n\n::: {.cell-output-display}\n![](lec-5_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## The `trees` data set\n\n::: columns\n::: {.column width=\"50%\"}\n-   Measurements of the diameter, height and volume of timber in 31 felled black cherry trees.\n-   Note that the diameter (in inches) is erroneously labelled `Girth` in the data\n-   The diameter (`Girth`) is measured at 4 ft 6 in above the ground.\n-   Source: Atkinson, A. C. (1985) Plots, Transformations and Regression. Oxford University Press.\n:::\n\n::: {.column width=\"50%\"}\n![](images/lec-5/tree.jpg){fig-alt=\"single tree\" fig-align=\"center\" width=\"250\"}\n:::\n:::\n\n## Combining variables\n\nGoing one step further, we can combine existing variables to create a new predictor.\n\nConsider the `tree` dataset\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree <- datasets::trees\nhead(d_tree)\n```\n\n::: {.cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n```\n:::\n:::\n\nwhere `Girth` indicates the tree diameter (twice the radius) in inches.\n\n## \n\nNow, estimate volume from height and girth\n\nwe could approximate the shape of a tree with a cylinder\n\nvolume of a cylinder is\n\n$$\nV = \\pi r^2 h\n$$\n\nThis suggests approximating the volume of a tree with the model\n\n$$\n\\text{Volume} = \\beta_1 (\\dfrac{\\text{Girth}}{2}})^2 * \\text{Height}\n$$\n\n## \n\nWe simply need to create a new variable corresponding to $(\\dfrac{\\text{Girth}}{2}})^2 * \\text{Height}$. Note that I first need to transform the variable `Girth` into feet to ensure that all variable have the same unit.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree_comb <- d_tree %>%\n  mutate(\n    Girth_ft    = Girth / 12,\n    radius      = Girth_ft / 2,\n    r_squared   = radius^2,\n    r2h = r_squared * Height\n  )\nhead(d_tree_comb)\n```\n\n::: {.cell-output-stdout}\n```\n  Girth Height Volume  Girth_ft    radius r_squared       r2h\n1   8.3     70   10.3 0.6916667 0.3458333 0.1196007  8.372049\n2   8.6     65   10.3 0.7166667 0.3583333 0.1284028  8.346181\n3   8.8     63   10.2 0.7333333 0.3666667 0.1344444  8.470000\n4  10.5     72   16.4 0.8750000 0.4375000 0.1914062 13.781250\n5  10.7     81   18.8 0.8916667 0.4458333 0.1987674 16.100156\n6  10.8     83   19.7 0.9000000 0.4500000 0.2025000 16.807500\n```\n:::\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(Volume ~ r2h - 1, data = d_tree_comb)\nsummary(m)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ r2h - 1, data = d_tree_comb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6696 -1.0832 -0.3341  1.6045  4.2944 \n\nCoefficients:\n    Estimate Std. Error t value            Pr(>|t|)    \nr2h  1.21427    0.01568   77.44 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.455 on 30 degrees of freedom\nMultiple R-squared:  0.995,\tAdjusted R-squared:  0.9949 \nF-statistic:  5996 on 1 and 30 DF,  p-value: < 0.00000000000000022\n```\n:::\n:::\n\n::: warning\nWhen we do not include the intercept in a model, $R^2$ measures something different and should therefore not be interpreted in the usual way.\n:::\n\n## \n\n## A special case of data combination: interaction\n\nSuppose you are interested in predicting the average run time (duration) of amateur jogging races.\n\nTwo variables that impact the duration are (i) the distance of the race, and (ii) the weather.\n\nFor simplicity, we measure the weather as either *good* (nice weather) or *bad* (rain, heat wave, etc).\n\n## \n\nOur model equation is therefore\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad}\n$$\n\nwhere $\\beta_1$ indicate the effect of an additional miles on the duration and $\\beta_2$ the effect of bad weather (probably positive).\n\nNote that the effect of weather is fixed in this model, say \"$+5$ minutes.\n\n. . .\n\nIs this reasonable? No!\n\n. . .\n\nThe effect of weather should vary with distance. For shorter races, bad weather may add only 2 or 3 minutes, while for longer races, bad weather may increase the average duration by 10 or 15 minutes.\n\n. . .\n\nWe capture this expected pattern using an **interaction term**.\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad} + \\beta_3 \\text{weather_bad}*\\text{distance}\n$$\n\n## Interpreting interactions\n\n-   When the weather is good, the equation simplifies to $$\n    \\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 0 + \\beta_3 0*\\text{distance} = \\beta_0 + \\beta_1 \\text{distance}\n    $$\n\n-   When the weather is bad, the equation simplifies to $$\n    \\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 1 + \\beta_3 1*\\text{distance} = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3) \\text{distance}\n    $$\n\nThe new slope term is $\\beta_1+\\beta_3$, meaning that the effect of an additional miles on the average duration is $\\beta_1+\\beta_3$ (not $\\beta_3$).\n\nThe effect of distance varies depending on the weather; the two variable **interact**.\n\n# Recap\n\n## Recap\n\n::: incremental\n-   simple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X\n    $$\n\n-   multiple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p \n    $$\n\n-   categorical predictor\n\n    -   $(k-1)$ indicator variables\n\n-   feature engineering\n\n    -   transforming variables\n    -   combining variables\n:::",
    "supporting": [
      "lec-5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}