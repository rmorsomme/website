{
  "hash": "b6196aff206279c152fa14a094314bbe",
  "result": {
    "markdown": "---\ntitle: \"Multiple Linear Regression Models\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   Find the last person you have not worked with\n-   Prediction project\n    -   teams\n    -   deadlines: May 31 (model and presentation) and June 1 (report)\n\n## Project - reading in the data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(readr)\nd <- read_csv(\"https://rmorsomme.github.io/website/projects/training_set.csv\")\n```\n:::\n\n## Recap\n\n-   simple linear regression model\n\n$$\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n$$\n\n-   residuals\n-   least-square estimates\n-   parameter interpretation\n-   model comparison with $R^2$\n-   outliers\n\n## Outline\n\n-   multiple linear regression\n-   categorical predictor\n-   feature engineering\n    -   transformation\n    -   combination\n    -   interaction\n\n# Multiple linear regression\n\nRemember, to improve our initial model with $(\\beta_0, \\beta_1) = (1, 1.3)$, we could\n\ni.  find better estimates $\\Rightarrow$ least-square estimates,\n\n. . .\n\nii. use additional predictors $\\Rightarrow$ **multiple linear regression models**\n\n. . .\n\nFor instance, to predict `hwy` we could include multiple variables in our model.\n\n## The `mpg` data set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nhead(d, n = 4)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n```\n:::\n:::\n\nInstead of fitting a model with only `cty` or only `displ`, we could fit a linear regression model with both predictors!\n\n## Linear regression with 2 predictors {.smaller}\n\nThe model equation is\n\n$$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty} + \\beta_2 \\text{displ}\n$$\n\nWe can find the least-square estimates (minimizing the SSR) with the command `lm`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ cty + displ, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty + displ, data = d)\n\nCoefficients:\n(Intercept)          cty        displ  \n    1.15145      1.32914     -0.03432  \n```\n:::\n:::\n\nwhich give the following regression equation\n\n$$\n\\text{hwy} \\approx 1.15 + 1.33 \\text{cty} - 0.034  \\text{displ}\n$$\n\n## Interpretation {.smaller}\n\n-   Interpreting $\\hat{\\beta}_1 = 1.33$:\n    -   \"Keeping `displ` constant, for each additional unit in `cty`, we would expect `hwy` to be higher, on average, by 1.33 units.\"\n\n. . .\n\n-   Interpreting $\\hat{\\beta}_2 = -0.034$:\n    -   \"Keeping `cty` constant, for each additional unit in `displ`, we would expect `hwy` to be lower, on average, by 0.034 unit.\"\n\n. . .\n\n-   Interpreting $\\hat{\\beta}_0 = 1.15$:\n    -   \"For a car with `cty` and `displ` equal to 0, we would expect `hwy` to be 1.15.\"\n    -   meaningless in this context.\n\n## Special case: categorical predictor\n\nIn a regression model, categorical predictors are represented using indicator variables.\n\nTo represent a categorical predictor with $k$ levels (categories), we use $(k-1)$ indicator variables[^1].\n\n[^1]: We cannot use $k$ indicator variables for identifiability reasons (advanced topic).\n\n## Including `drv`\n\nFor instance, the categorical variable `drv` has $k=3$ levels (`4`, `f` and `r`), so we can represent it with $3-1=2$ indicator variables with the following model equation\n\n$$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{drv_f} + \\beta_2 \\text{drv_r}\n$$\n\n. . .\n\n::: callout-tip\nFor a binary variable $k=2$, so we only need $k-1=1$ indicator variable.\n\nFor instance, to include the binary variable `year_binary`, we only added a single indicator variable to the model.\n:::\n\n## Using categorical predictors in `R`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ drv, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ drv, data = d)\n\nCoefficients:\n(Intercept)         drvf         drvr  \n     19.175        8.986        1.825  \n```\n:::\n:::\n\nThe model equation with the least-square estimates is\n\n$$\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n$$\n\n## Interpreting the output {.smaller}\n\n$$\n\\text{hwy} \\approx 19.175 + 8.986 \\text{drv_f} + 1.825 \\text{drv_r}\n$$\n\nIf a new vehicle has a `drv` that is:\n\n-   `4`, then the two indicator variables `drv_f` and `drv_r` take the value 0, and the prediction is $\\widehat{hwy} = 19.175$\n\n. . .\n\n-   `f`, then the indicator variables `drv_f` takes the value 1 and `drv_r` the value 0, and the prediction is $\\widehat{hwy} = 19.175 + 8.986 = 28.161$\n\n. . .\n\n-   `r`, then the indicator variables `drv_f` takes the value 0 and `drv_r` the value 1, and the prediction is $\\widehat{hwy} = 19.175 + 1.825 = 21$\n\n. . .\n\nThe level (category) `4` is called the **reference (baseline) level**.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - categorical predictor\n\n-   Exercises [8.5](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises), [8.7](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises)\n\n-   In addition\n\n    i.  identify the baseline level of the categorical variable in each model.\n\n    ::: {.cell layout-align=\"center\"}\n    \n    ```{.r .cell-code}\n    library(openintro)\n    d <- openintro::births14\n    ```\n    :::\n\n    ii. fit the first model in `R`\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_629e3da0\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Fitting a larger model {.smaller}\n\nLet us fit a model with `cty`, `drv` and `disp`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_larger <- lm(hwy ~ cty + drv + displ, data = d)\nm_larger\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty + drv + displ, data = d)\n\nCoefficients:\n(Intercept)          cty         drvf         drvr        displ  \n      3.424        1.157        2.158        2.360       -0.208  \n```\n:::\n:::\n\nIts $R^2$ is 0.938.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m_larger)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9384357\n```\n:::\n:::\n\n. . .\n\nUnsurprisingly, including additional predictors makes the regression line closer to the points $\\Rightarrow$ residuals are smaller $\\Rightarrow$ SSR is smaller $\\Rightarrow$ $R^2$ is larger.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - computing $R^2$ by hand\n\n-   What is the formula for $R^2$?\n-   Compute $R^2$ by hand in `R` (do not use `glance`). You can use the following code.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nm_larger <- lm(hwy ~ cty + drv + displ, data = d)\nm_augment <- augment(m_larger)\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_629e3c57\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Fitting the full[^2] model {.smaller}\n\n[^2]: This not exactly the full model since I do not include the variable `model` due to identifiability issues (technical point).\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_full <- lm(\n  hwy ~ manufacturer + displ + year + cyl + trans + drv + cty + fl + class,\n  data = d\n  )\n```\n:::\n\nThanks to the additional predictors, the residuals are very small, making $R^2$ close to $1$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m_full)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9773386\n```\n:::\n:::\n\n. . .\n\nLarge $R^2$\n\n-   seems great!\n-   ...but we will see in the next lecture that this is not always a good sign.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - multiple linear regression\n\nExercise [8.9](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises)\n\ni.  fit the model in `R`\nii. identify the type of each variable\niii. identify the baseline level of the categorical predictors\niv. do parts a-d\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd_birth <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_629e3be3\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Statistics as an art -- feature engineering\n\n## Feature engineering\n\nWe saw that adding predictors to the model seems to help.\n\nHowever, the variables included in the data set, e.g. `displ`, `year`, etc, may not be the most useful predictors for `hwy`.\n\n. . .\n\n**Feature engineering** refers to the creation of new predictors from the raw variables.\n\n. . .\n\n::: callout-tip\nThis is where your understanding of the data, scientific knowledge and experience make a big difference.\n:::\n\n# Transforming a variable\n\n## Nonlinearity\n\nConsider the relation between the predictor `displ` and the response `hwy`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(displ, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-5_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n. . .\n\nThe relationis not exactly linear.\n\n## Modeling nonlinearity {.smaller}\n\nLet us include the predictor $\\dfrac{1}{\\text{displ}}$ to capture this nonlinear relation.\n\nHere is the model equation\n\n$$\n\\text{hwy} \\approx \\beta_0+ \\beta_1 \\text{displ} + \\beta_2 \\dfrac{1}{\\text{displ}}\n$$\n\nThe least-square coefficient estimates are\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2\"}\nd_transf <- mutate(d, displ_inv = 1/displ)\nm <- lm(hwy ~ displ + displ_inv, data = d_transf)\nm\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ displ + displ_inv, data = d_transf)\n\nCoefficients:\n(Intercept)        displ    displ_inv  \n    12.2601      -0.2332      36.1456  \n```\n:::\n:::\n\n## \n\nAnd the regression lines captures the nonlinear relation.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|4\"}\naugment(m) %>%\n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  geom_line(aes(y = .fitted))\n```\n\n::: {.cell-output-display}\n![](lec-5_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=80%}\n:::\n:::\n\n# Combining variables {.smaller}\n\n## The `trees` data set {.smaller}\n\n::: columns\n::: {.column width=\"45%\"}\n-   Measurements of the diameter, height and volume of timber in 31 felled black cherry trees.\n-   Note that the diameter (in inches) is erroneously labelled `Girth` in the data\n-   The diameter (`Girth`) is measured at 4 ft 6 in above the ground.\n-   Source: Atkinson, A. C. (1985) Plots, Transformations and Regression. Oxford University Press.\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/lec-5/tree.jpg)\n:::\n:::\n\n## Combining variables\n\nTransforming a variable may be helpful ...but we can go a step further!\n\n. . .\n\nWe can construct new predictors by **combining** existing variables.\n\n. . .\n\nConsider the `trees` dataset\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree <- datasets::trees\nhead(d_tree, n = 4)\n```\n\n::: {.cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n```\n:::\n:::\n\nwhere `Girth` indicates the tree diameter (twice the radius) in inches.\n\n## Geomtric considerations\n\nSuppose we want to estimate `Volume` (expensive to measure) from `Girth` and `Height` (cheap to measure).\n\n. . .\n\nYou might decide to approximate the shape of a tree with a shape that is between a cylinder and a cone.\n\nFrom geometry, we know that the volume of a cylinder is\n\n$$\nV = \\pi r^2 h\n$$\n\nand that of a cone is\n\n$$\nV = \\frac{1}{3} \\pi r^2 h\n$$\n\n## \n\nThis suggests approximating the volume of a tree with the following model\n\n$$\n\\text{Volume} = \\beta_1 \\left[ \\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right] = \\beta_1X\n$$\n\nwhere\n\n-   $\\beta_1$ is an unknown parameter that we expect to be between $\\pi$ (pure cylinder) and $\\frac{1}{3}\\pi$ (pure cone)\n-   $X = \\left[\\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right]$ is our new predictor.\n\n## \n\nTo accomplish this, we simply create a new variable corresponding to $\\left[\\left(\\dfrac{\\text{Girth}}{2}\\right)^2 * \\text{Height}\\right]$.\n\nBefore doing that, we just need to transform `Girth` into feet to ensure that all variables have the same units.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree_comb <- d_tree %>%\n  mutate(\n    Girth_ft    = Girth / 12,\n    radius      = Girth_ft / 2,\n    r_squared   = radius^2,\n    r2h = r_squared * Height\n  )\nhead(d_tree_comb, n = 3)\n```\n\n::: {.cell-output-stdout}\n```\n  Girth Height Volume  Girth_ft    radius r_squared      r2h\n1   8.3     70   10.3 0.6916667 0.3458333 0.1196007 8.372049\n2   8.6     65   10.3 0.7166667 0.3583333 0.1284028 8.346181\n3   8.8     63   10.2 0.7333333 0.3666667 0.1344444 8.470000\n```\n:::\n:::\n\n## \n\nOur model does not include an intercept. To exclude the intercept from the model, I use `-1` in the `lm` command.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1\"}\nm_engineered <- lm(Volume ~ r2h - 1, data = d_tree_comb)\nm_engineered\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ r2h - 1, data = d_tree_comb)\n\nCoefficients:\n  r2h  \n1.214  \n```\n:::\n:::\n\n. . .\n\nThe coefficient estimate is between the two anticipated bounds $\\pi=3.14$ and $\\frac{\\pi}{3}=1.047$!\n\n. . .\n\n::: callout-warning\nWhen we do not include the intercept in a model, $R^2$ measures something different and should therefore not be interpreted in the usual way.\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m_engineered)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9950219\n```\n:::\n:::\n\n## Comparison with the full model {.smaller}\n\nAlthough $R^2$ has a different meaning when there is no intercept, it can be still used for comparison[^3].\n\n[^3]: as long as both models exclude the intercept\n\n. . .\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_full <- lm(Volume ~ Girth + Height -1, data = d_tree)\nglance(m_full)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9696913\n```\n:::\n\n```{.r .cell-code}\nglance(m_engineered)$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9950219\n```\n:::\n:::\n\nOur new variable $(\\dfrac{\\text{Girth}}{2})^2 * \\text{Height}$ improves the model!\n\n. . .\n\nNote that the full model has two predictors, while our geometry-based model has only a single predictor!\n\n::: callout-tip\n## Feature engineering\n\nA carefully constructed predictor can do a better job than multiple raw predictors!\n:::\n\n# Special case of data combination: interaction\n\n## Predicting amateur jogging races duration {.smaller}\n\n::: columns\n::: {.column width=\"45%\"}\nSuppose you are interested in predicting the average run time (**duration**) of amateur jogging races.\n\nTwo variables that impact the duration are (i) the **distance** of the race, and (ii) the **weather**.\n\nFor simplicity, we measure the weather as either *good* (nice weather) or *bad* (rain, heat wave, etc).\n:::\n\n::: {.column width=\"5%\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/lec-5/jogging.jpg)\n:::\n:::\n\n## Fixed effect weather? {.smaller}\n\nTe full model is\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad}\n$$\n\nwhere $\\beta_1$ indicate the effect of an additional miles on the expected duration and $\\beta_2$ the effect of bad weather.\n\nNote that the effect of weather is fixed in this model, say \"$+5$ minutes\" if $\\hat{\\beta}_2 = 5$.\n\n. . .\n\nIs this reasonable? No!\n\n. . .\n\n$\\Rightarrow$ the effect of weather should vary with distance. For shorter races, bad weather may add only 2 or 3 minutes, while for longer races, bad weather may increase the average duration by 10 or 15 minutes.\n\n## Model equation with interaction {.smaller}\n\nWe capture such pattern using an **interaction term**.\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 \\text{weather_bad} + \\beta_3 \\text{weather_bad}*\\text{distance}\n$$\n\n-   When the weather is good, the equation simplifies to\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 0 + \\beta_3 0*\\text{distance} = \\beta_0 + \\beta_1 \\text{distance}\n$$\n\n-   When the weather is bad, the equation simplifies to\n\n$$\n\\text{duration} \\approx \\beta_0 + \\beta_1 \\text{distance} + \\beta_2 1 + \\beta_3 1*\\text{distance} = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3) \\text{distance}\n$$\n\n## Interpreting interactions {.smaller}\n\n-   When the weather is **good**, the slope estimate is $\\hat{\\beta}_1$, meaning that the effect of an additional miles on the average duration is $\\hat{\\beta}_1$.\n\n-   When the weather is **bad**, the slope estimate is $\\hat{\\beta}_1+\\hat{\\beta}_3$, meaning that the effect of an additional miles on the average duration is $\\hat{\\beta}_1+\\hat{\\beta}_3$ (not $\\hat{\\beta}_1$).\n\n::: callout-tip\n## Interactions\n\nThe effect of the distance depends on the weather. Similarly, the effect of the weather depends on the distance.\n\n$\\Rightarrow$ the two variables **interact**.\n:::\n\n# Recap\n\n## Recap {.smaller}\n\n::: incremental\n-   simple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X\n    $$\n\n-   multiple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p \n    $$\n\n-   categorical predictor\n\n    -   $(k-1)$ indicator variables\n\n-   feature engineering\n\n    -   transforming variables\n    -   combining variables\n    -   interactions\n:::",
    "supporting": [
      "lec-5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}