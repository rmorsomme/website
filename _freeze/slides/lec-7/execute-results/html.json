{
  "hash": "fe4359781f790f3ba51b1396c13ff740",
  "result": {
    "markdown": "---\ntitle: \"Logistic Regression\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\n    link-external-newwindow: true\n    history: false\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   \n\n## Recap {.smaller}\n\n::: incremental\n-   Simple and multiple lrm\n\n    -   numerical response\n\n-   model selection\n\n    -   overall criterion\n\n    -   predictive performance\n\n    -   stepwise procedures\n:::\n\n## Outline\n\n-   Modeling a binary response\n-   Logistic regression\n-   One predictor\n-   Multiple predictors\n\n# Modeling a binary response\n\n## Example -- [Stanford University Heart Transplant Study](https://www.jstor.org/tc/accept?origin=%2Fstable%2Fpdf%2F2285502.pdf%3Fcasa_token%3DtsfisPZrE4QAAAAA%3AjmSc4CpJPjkyTwKwoUdROLd5ZMHodUmGdclzMIXQGWUXmT0FQ7zbwLympmqvjcGrsbH-iU2qfA3DYSY5oxC4E0qSbRvoQN4hAlB5Jy01zwHbvwVrHPU&is_image=False)\n\n::: columns\n::: {.column width=\"60%\"}\n-   Goals: to determine whether an experimental heart transplant program increases lifespan\n\n-   observations: patients\n\n-   response: survival after 5 years (binary)\n\n-   predictors: age, prior surgery, waiting time for transplant.\n:::\n\n::: {.column width=\"40%\"}\n![](images/lec-7/surgery.jpg){fig-align=\"center\"}\n:::\n:::\n\n## Data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- heart_transplant\nd\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 103 x 8\n      id acceptyear   age survived survtime prior transplant  wait\n   <int>      <int> <int> <fct>       <int> <fct> <fct>      <int>\n 1    15         68    53 dead            1 no    control       NA\n 2    43         70    43 dead            2 no    control       NA\n 3    61         71    52 dead            2 no    control       NA\n 4    75         72    52 dead            2 no    control       NA\n 5     6         68    54 dead            3 no    control       NA\n 6    42         70    36 dead            3 no    control       NA\n 7    54         71    47 dead            3 no    control       NA\n 8    38         70    41 dead            5 no    treatment      5\n 9    85         73    47 dead            5 no    control       NA\n10     2         68    51 dead            6 no    control       NA\n# ... with 93 more rows\n```\n:::\n:::\n\n## Effect of age on survival\n\n::: panel-tabset\n## geom_point\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd %>%\n  ggplot(aes(x = age, y = survived)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## geom_jitter\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd %>%\n  ggplot(aes(x = age, y = survived)) +\n  geom_jitter(     # add random noise to point location\n    width = 0,     # horizontal jitter\n    height = 0.05, # vertical jitter\n    alpha = 0.5    # transparency\n    )\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## Binary response\n\n-   Technical problem: levels of the response are labels\n\n    -   can't fit a regression model to words, only to numbers\n\n-   use `mutate` to create a binary variable (either 0 or 1)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(is_alive = if_else(survived == \"alive\", 1, 0))\n```\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = d, aes(x = age, y = is_alive)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Linear regression?\n\nIntuitively, `age` should be a good predictor of survival.\n\n. . .\n\nLet us fit a linear regression model\n\n$$\n\\text{is_alive} \\approx \\beta_0 + \\beta_1 age\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_lrm <- lm(is_alive ~ age, data = d)\n```\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = d, aes(x = age, y = is_alive)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.5) +\n  geom_abline(intercept = 0.80955, slope = -0.01205)\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Problems with linear regression\n\n-   The response is not continuous\n\n    -   unlike fuel consumption, tree volume or newborn weight\n\n-   nonsensical predictions, even for reasonable value of `age`.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - limitation of linear regression\n\nWhat is the prediction for a 70-year-old patient?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a39d20\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Generalized linear models\n\n## GLMs {.smaller}\n\n**Generalized linear models** (GLMs -- STA310) extend the linear regression framework to settings where the response variable is *restricted*:\n\n::: incremental\n-   **logistic regression** with binary response\n\n-   multinomial regression with nominal (categorical) response, e.g. voting in multiparty systems\n\n-   ordinal regression with ordinal (categorical) response, e.g. course letter grade\n\n-   poisson regression with count response, e.g. number of children\n\n-   gamma regression with positive response variable, e.g. mpg, insurance costs\n\n-   beta regression with continuous response between 0 and 1 (percentage), e.g. cancer rates in counties\n:::\n\n# Logistic regression\n\n## Logistic regression\n\n-   Example of a *generalized linear model*\n\n-   We'll focus on implementation and interpretation\n\n-   Useful for inference project (not the prediction project)\n\n**Logistic regression**: model the probability of success $p$ based on a set of predictors $X_1, \\dots, X_p$.\n\n## Binary outcome and probability\n\nLet $Y_i$ denote the response of person $i$.\n\nThe logistic regression model assumes that\n\n$$\nY_i = \n\\begin{cases}\n1 \\text{ (success)}, \\text{with probability } p_i, \\\\\n0 \\text{ (failure)}, \\text{with probability } 1-p_i.\n\\end{cases}\n$$\n\nwhere $p_i$ denotes the probability of success (survival) of person $i$.\n\n## Modeling a probability with linear regression? {.smaller}\n\nWe saw that we cannot model $p$ with a lrm ($p \\approx \\beta_0 + \\beta_1\\text{age}$)\n\n-   $0\\le p_i\\le1$\n\n-   ...but $\\beta_0 + \\beta_1 \\text{age}$ may be negative or larger than 1!\n\n    -   in fact, the regression line extends infinitely in either either direction\n\n::: callout-warning\nBinary variables should **not** be modeled using linear regression!\n:::\n\n## Modeling a probability with logistic regression\n\nIntuitively, an older patient should have a smaller probability of surviving.\n\n$\\Rightarrow$ We want a model that associates a smaller probability $p$ for a patient with a larger `year` variable.\n\n. . .\n\n$\\Rightarrow$ We need to find a way to go from `age` (could be any number) to $p$ (between 0 and 1)\n\n## The logit transformation\n\nConsider the **logit transformation**\n\n$$\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n$$\n\nwhere $\\mu$ can be any number.\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(mu = seq(-7.5, 7.5, by = 0.01)) %>%\n  mutate(p = exp(mu)/(1+exp(mu))) %>%\n  ggplot() + \n  geom_line(aes(x = mu, y = p))\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\nNote that $\\dfrac{e^{\\mu}}{1+e^{\\mu}}$ is bounded between 0 and 1.\n\nMoreover, larger values of $\\mu$ will give larger $p$, and smaller $\\mu$ will give smaller $p$.\n\n## \n\n::: callout-tip\n## Group exercise - logit transformation\n\n-   What value of $p$ corresponds to\n\n    -   $\\mu = 0.5$?\n\n    -   $\\mu = 2$?\n\n    -   $\\mu = -2$?\n\n-   What value of $\\mu$ gives\n\n    -   $p=0.5$?\n\n    -   $p=0.9$?\n\n    -   $p=0.1$?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a39a10\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Modeling $\\mu$ {.smaller}\n\nWe can now simply model $\\mu$ using a simple lrm with age\n\n$$\n\\mu \\approx \\beta_0 + \\beta_1 \\text{age}\n$$\n\nand then transform $\\mu$ into $p$ using the logit transformation\n\n$$\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n$$\n\n. . .\n\nPutting everything together gives the **logistic regression model**\n\n$$\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}} \\approx \\dfrac{e^{\\beta_0 + \\beta_1 \\text{age}}}{1+e^{\\beta_0 + \\beta_1 \\text{age}}}\n$$\n\n## \n\n::: callout-note\n## Alternative formulation\n\nThe formulation\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right) \\approx \\beta_0 +\\beta_1 \\text{age}\n$$\n\nis also widely to describe the logistic regression model. This formulation is equivalent to that used on the previous slide.\n:::\n\n## Maximum likelihood estimates\n\nHow are the unknown coefficients $\\beta_0$ and $\\beta_1$ estimated?\n\nWhen we fit a logistic regression model with `R`, the so-called *maximum likelihood estimates* (MLE[^1]) are returned.\n\n[^1]: MLE are extremely popular in statistics.\n\n# Implementation\n\n## `glm`\n\nWe fit a logistic regression model in `R` with the command `glm` (not `lm`)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|3|4|\"}\nm <- glm( # glm, not lm\n  is_alive ~ age, \n  family = binomial, # logistic regression\n  data = d\n  )\nm\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:  glm(formula = is_alive ~ age, family = binomial, data = d)\n\nCoefficients:\n(Intercept)          age  \n    1.56438     -0.05847  \n\nDegrees of Freedom: 102 Total (i.e. Null);  101 Residual\nNull Deviance:\t    120.5 \nResidual Deviance: 113.7 \tAIC: 117.7\n```\n:::\n:::\n\n## `R` output {.smaller}\n\nFor the moment simply focus on\n\n-   coefficient estimates\n\n-   AIC (no (adjusted-) $R^2$) for model selection\n\n::: callout-warning\n## glm not lm\n\nTo fit a logistic regression model in `R`, use the command `glm` (not `lm`) with the argument `family = binomial`.\n:::\n\n## Interpretation\n\nA positive coefficient estimate indicates that a higher value of the predictor is associated with a higher probability of success; and vice-versa for a smaller value.\n\nIn our case, the coefficient estimate for age (-0.058) is negative, so the model indicates that older participants have a smaller probability of surviving.\n\n## Visualizing the regression curve\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_augm <- augment(m, type.predict = \"response\")\nd_augm # .fitted is equivalent to p_hat \n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 103 x 8\n   is_alive   age .fitted .resid .std.resid   .hat .sigma .cooksd\n      <dbl> <int>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>   <dbl>\n 1        0    53   0.177 -0.625     -0.630 0.0156   1.06 0.00174\n 2        0    43   0.279 -0.809     -0.813 0.0106   1.06 0.00210\n 3        0    52   0.186 -0.642     -0.646 0.0147   1.06 0.00173\n 4        0    52   0.186 -0.642     -0.646 0.0147   1.06 0.00173\n 5        0    54   0.169 -0.608     -0.614 0.0166   1.06 0.00175\n 6        0    36   0.368 -0.958     -0.967 0.0181   1.06 0.00548\n 7        0    47   0.234 -0.731     -0.735 0.0111   1.06 0.00174\n 8        0    41   0.303 -0.850     -0.855 0.0115   1.06 0.00257\n 9        0    47   0.234 -0.731     -0.735 0.0111   1.06 0.00174\n10        0    51   0.195 -0.659     -0.663 0.0138   1.06 0.00172\n# ... with 93 more rows\n```\n:::\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_augm %>%\n  ggplot(aes(x = age)) +\n  geom_jitter(aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +\n  geom_line(aes(y = .fitted), col = \"maroon\", size = 2) # .fitted is equivalent to p_hat \n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Extending the regression curve\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#artificial data with larger range (0 to 100)\nd_artificial <- tibble(age = seq(0, 100, by = 0.1))\np_hat <- predict(m, newdata = d_artificial, type = \"response\")\n\nd_artificial <- mutate(d_artificial, p_hat = p_hat)\nd_artificial\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1,001 x 2\n     age p_hat\n   <dbl> <dbl>\n 1   0   0.827\n 2   0.1 0.826\n 3   0.2 0.825\n 4   0.3 0.824\n 5   0.4 0.824\n 6   0.5 0.823\n 7   0.6 0.822\n 8   0.7 0.821\n 9   0.8 0.820\n10   0.9 0.819\n# ... with 991 more rows\n```\n:::\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(mapping = aes(x = age)) +\n  geom_jitter(data = d, aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +\n  geom_line(data = d_artificial, aes(y = p_hat), col = \"maroon\", size = 2)\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Multiple logistic regression\n\nBased on what we know about multiple lrm, it is easy to extend the previous framework to **multiple** logistic regression:\n\n$$\np = \\dfrac{e^{\\mu}}{1+e^{\\mu}}\n$$\n\nwhere\n\n$$\n\\mu \\approx \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n$$\n\n## Implementation in `R`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm2 <- glm( # glm, not lm\n  is_alive ~ age + transplant, \n  family = binomial, # logistic regression\n  data = d\n  )\nm2\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:  glm(formula = is_alive ~ age + transplant, family = binomial, \n    data = d)\n\nCoefficients:\n        (Intercept)                  age  transplanttreatment  \n            0.97311             -0.07632              1.82316  \n\nDegrees of Freedom: 102 Total (i.e. Null);  100 Residual\nNull Deviance:\t    120.5 \nResidual Deviance: 103.9 \tAIC: 109.9\n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - interpretation\n\nWhat is the interpretation of the coefficient estimates for `age` and `transplant`?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a39b7e\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">02</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - interpretation\n\nExercise [9.5](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a39bba\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - implementation\n\nExercise [9.3](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)\n\n**You do not need to do parts a and b.**\n\nSimply fit the two models in `R`. The `possum` data used in this exercise can be found in the `openintro` `R` package. You should obtain the same coefficient estimates as in the book, though with an opposite sign.\n\nSave these models; you will need them later!\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62a39a59\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Artificial data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_artificial <- expand.grid(\n  age = seq(0, 100, by = 0.1),\n  transplant = c(\"treatment\", \"control\")\n  ) %>%\n  as_tibble() %>%\n  arrange(age)\nd_artificial\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2,002 x 2\n     age transplant\n   <dbl> <fct>     \n 1   0   treatment \n 2   0   control   \n 3   0.1 treatment \n 4   0.1 control   \n 5   0.2 treatment \n 6   0.2 control   \n 7   0.3 treatment \n 8   0.3 control   \n 9   0.4 treatment \n10   0.4 control   \n# ... with 1,992 more rows\n```\n:::\n:::\n\n## Visualization\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_hat <- predict(m2, d_artificial, type = \"response\")\nd_artificial %>%\n  mutate(p_hat = p_hat) %>%\n  ggplot() +\n  geom_line(aes(x = age, y = p_hat, col = transplant))\n```\n\n::: {.cell-output-display}\n![](lec-7_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Prediction\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_augm <- augment(m2, type.predict = \"response\") %>%\n  mutate(is_alive_hat = if_else(.fitted < 0.5, 0, 1)) %>%\n  select(is_alive, age, transplant, .fitted, is_alive_hat)\nd_augm\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 103 x 5\n   is_alive   age transplant .fitted is_alive_hat\n      <dbl> <int> <fct>        <dbl>        <dbl>\n 1        0    53 control     0.0443            0\n 2        0    43 control     0.0904            0\n 3        0    52 control     0.0476            0\n 4        0    52 control     0.0476            0\n 5        0    54 control     0.0412            0\n 6        0    36 control     0.145             0\n 7        0    47 control     0.0682            0\n 8        0    41 treatment   0.418             0\n 9        0    47 control     0.0682            0\n10        0    51 control     0.0512            0\n# ... with 93 more rows\n```\n:::\n:::\n\n## Confusion matrix\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_augm %>%\n  select(is_alive, is_alive_hat) %>%\n  table()\n```\n\n::: {.cell-output-stdout}\n```\n        is_alive_hat\nis_alive  0  1\n       0 71  4\n       1 20  8\n```\n:::\n:::\n\nThe model got $71+8=79$ observations right out of $71+4+20+8=103$; so its **accuracy** is\n\n$$\n\\frac{79}{103} \\approx 77\\%\n$$\n\nTo estimate the prediction accuracy on new data, simply use the holdout method or cross-validation.\n\n## Model selection\n\n-   AIC, BIC (not adjusted-$R^2$)\n\n-   the holdout method using *prediction* *accuracy* (not RMSE)\n\n-   cross-validation using *prediction* *accuracy* (not RMSE)\n\n::: callout-tip\n## AIC and BIC\n\nWith AIC and BIC, lower is better!\n\n$$\nAIC = 2p - \\text{Goodness of fit}, \\qquad BIC = p\\ln(n) - \\text{Goodness of fit}\n$$\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - model comparison with AIC\n\nGo back to the two models you fitted for exercise [9.3](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises). What are their respective AIC? Which model is better?\n\nSimilarly, compare the AIC of the simple (`age`) and multiple (`age + implant`) logistic regression models for the heart transplant study. Which model is better?\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - stepwise model selection with AIC\n\nExercise [9.7](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)\n:::",
    "supporting": [
      "lec-7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}