{
  "hash": "0c8042d7eca71d133215d253aa62554b",
  "result": {
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   Homework 4 is due on **Thursday**.\n-   Homework 5 will be due after the prediction project on Sunday June 5.\n-   Prediction project\n    -   Find your teammate [here](slides/prediction-project-teams.html).\n\n    -   If you have not started yet, start now. You can do already do 80% of the project.\n\n    -   You will have the tools to do the remaining 20% after this lecture.\n\n## Recap {.smaller}\n\n::: incremental\n-   simple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X\n    $$\n\n-   multiple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p \n    $$\n\n-   categorical predictor\n\n    -   $(k-1)$ indicator variables\n\n-   feature engineering\n\n    -   transforming variables\n    -   combining variables\n    -   interactions\n:::\n\n## Outline\n\n-   Overfitting\n    -   limitations of $R^2$ for model selection\n-   Model selection through\n    -   overall criteria (adjusted-$R^2$, AIC, BIC)\n    -   predictive performance (holdout method, cross-validation)\n    -   stepwise procedure (forward, backward)\n\n# Competing models\n\n## Competing models\n\n-   Earlier, we saw that adding the predictor $\\dfrac{1}{\\text{displ}}$ gave a better fit.\n\n-   Let us see if the same idea work with the `trees` dataset.\n\n## A nonlinear association?\n\nSuppose we want to predict volume using only the variable `girth`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(0)\nd_tree <- datasets::trees %>% \n  mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5)) # add some random noise to girth for illustration purpose\nggplot(d_tree) +\n  geom_point(aes(Girth, Volume))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\nOne could argue that there is a slight nonlinear association\n\n## Function to compute $R^2$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}\n```\n:::\n\n## Starting simple...\n\nWe start with the simple model\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(Volume ~ Girth, data = d_tree)\ncompute_R2(m1)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.933\n```\n:::\n:::\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m1, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.44        5.51\n2  8.44        5.52\n3  8.44        5.52\n4  8.44        5.53\n5  8.44        5.53\n6  8.44        5.54\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...taking it up a notch...\n\nTo capture the nonlinear association between `Girth` and `Volume`, we consider the predictor $\\text{Girth}^2$.\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2\n$$\n\nThe command to fit this model is\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree2 <- mutate(d_tree, Girth2 = Girth^2)\nm2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)\ncompute_R2(m2)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.958\n```\n:::\n:::\n\n. . .\n\n$R^2$ has increased! It went from 0.933 (model 1) to 0.958.\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2)\nVolume_pred <- predict(m2, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 3\n  Girth Girth2 Volume_pred\n  <dbl>  <dbl>       <dbl>\n1  8.44   71.2        11.5\n2  8.44   71.2        11.5\n3  8.44   71.2        11.5\n4  8.44   71.2        11.5\n5  8.44   71.2        11.5\n6  8.44   71.3        11.5\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...taking it up another notch...\n\nLet us consider the predictor $\\text{Girth}^3$.\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\beta_3 \\text{girth}^3\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree3 <- mutate(d_tree, Girth2 = Girth^2, Girth3 = Girth^3)\nm3 <- lm(Volume ~ Girth + Girth2 + Girth3, data = d_tree3)\ncompute_R2(m3)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n:::\n\n. . .\n\n$R^2$ has increased! It went from 0.958 (model 2) to 0.96.\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2, Girth3 = Girth^3)\nVolume_pred <- predict(m3, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 4\n  Girth Girth2 Girth3 Volume_pred\n  <dbl>  <dbl>  <dbl>       <dbl>\n1  8.44   71.2   601.        9.83\n2  8.44   71.2   601.        9.83\n3  8.44   71.2   601.        9.83\n4  8.44   71.2   601.        9.84\n5  8.44   71.2   601.        9.84\n6  8.44   71.3   602.        9.84\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...before taking things to the extreme\n\nWhat if we also include the predictors $\\text{Girth}^4, \\text{Girth}^5, \\dots, \\text{Girth}^{k}$ for some larger number $k$?\n\nThe following `R` command fits the model with $k=81$, that is,\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\dots + \\beta_{81} \\text{girth}^{81}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_extreme <- lm(Volume ~ poly(Girth, degree = 81, raw = TRUE), data = d_tree)\ncompute_R2(m_extreme)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.981\n```\n:::\n:::\n\n$R^2$ has again increased! It went from 0.96 (model 3) to 0.981.\n\n## A limitation of $R^2$\n\nAs we keep adding predictors, $R^2$ will always increase.\n\n-   additional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.\n\n. . .\n\nIs the model `m_extreme` a good model?\n\n::: callout-note\nWe want to learn about the relation between `Volume` and `Girth` present in the *population* (not the sample).\n\nA good model should accurately represent the *population*.\n:::\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\" hash='lec-6_cache/revealjs/unnamed-chunk-13_d84467c58d1b332409a92ca52ebf2c56'}\n\n```{.r .cell-code  code-line-numbers=\"1|2|3|4|\"}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.0001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m_extreme, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.44        10.3\n2  8.44        10.4\n3  8.44        10.4\n4  8.44        10.4\n5  8.44        10.5\n6  8.44        10.5\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Regression line (zoomed)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred)) +\n  ylim(0, 90)\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## Overfitting\n\nThe model `m_extreme` **overfits** the data.\n\nA model overfits the data when it fits the sample extremely well but does a poor job for new data.\n\n. . .\n\nRemember that we want to learn about the *population*, not the *sample*!\n\n# Model selection: overall criteria\n\n## $R^2$\n\nWe saw that $R^2$ keeps increasing as we add predictors.\n\n$$\nR^2 = 1 - \\dfrac{SSR}{SST}\n$$\n\n. . .\n\n$R^2$ can therefore not be used to identify models that overfit the data.\n\n## Adjusted-$R^2$\n\nThe **adjusted-**$R^2$ is similar to $R^2$, but penalizes large models:\n\n$$\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\\dfrac{n-1}{n-p-1}\n$$\n\nwhere $p$ corresponds to the number of predictors (model size).\n\n## \n\nThe adjusted-$R^2$ therefore balances goodness of fit and parsimony:\n\n-   The ratio $\\dfrac{SSR}{SST}$ favors model with good fit (like $R^2$)\n-   The ratio $\\dfrac{n-1}{n-k-1}$ favors parsimonious models\n\n. . .\n\nThe model with the highest adjusted-$R^2$ typically provides a good fit without overfitting.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - a function for the adjusted-$R^2$\n\nConsider the function `compute_R2` which takes a model as input and returns its $R^2$ rounded to the 3rd decimal.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}\n```\n:::\n\nAdapt the function so that it computes the adjusted-$R^2$ instead of $R^2$. Give an appropriate name to the new function.\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_628cf1f7\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## AIC and BIC {.smaller}\n\nTwo other popular overall criteria that balance goodness of fit (small SSR) and parsimony (small $p$) are\n\n-   the Akaike Information Criterion (**AIC**)\n-   the Bayesian Information Criterion (**BIC**)\n\n. . .\n\nThe formula for AIC and BIC are respectively\n\n$$\nAIC = 2p - \\text{GoF}, \\qquad BIC = \\ln(n)p- \\text{GoF}\n$$\n\nwhere $\\text{GoF}$ measures the *goodness of fit* of the model[^1].\n\n[^1]: The exact formula for $\\text{GoF}$ is beyond the scope of this class.\n\n## AIC and BIC in practice\n\n::: callout-warning\nUnlike the adjusted-$R^2$, **smaller is better for the AIC and BIC**.\n:::\n\n. . .\n\n::: callout-note\nThe BIC penalizes the number of parameters $p$ more strongly than AIC. BIC will therefore tend to favor smaller models (models with fewer parameters).\n:::\n\n## Computing AIC and BIC\n\nAIC and BIC are easily accessible in `R` with the command `glance`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbind(glance(m1), glance(m2), glance(m3), glance(m_extreme))\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 4 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.933         0.930  4.34     401.  1.57e-18     1  -88.5  183.  187.\n2     0.958         0.955  3.50     317.  5.93e-20     2  -81.3  171.  176.\n3     0.960         0.955  3.48     214.  6.39e-19     3  -80.5  171.  178.\n4     0.981         0.960  3.28      46.3 2.32e- 9    16  -68.5  173.  199.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n. . .\n\nIn this case, BIC favor `m2`, AIC cannot decide between `m2` and `m3` and the adjusted-$R^2$ (wrongly) favors `m_extreme`.\n\n-   For AIC and BIC, smaller is better!\n-   BIC favors more parsimonious models than AIC.\n\n# Model selection: predictive performance\n\n## Limitations of the previous criteria\n\nThe adjusted-$R^2$, AIC and BIC all try to balance\n\n1.  goodness of fit\n2.  parsimony\n\n. . .\n\nThey achieve this balance by favoring models with small SSR while penalizing models with larger $p$.\n\n. . .\n\n...But\n\n-   The form of the penalty for $p$ is somewhat arbitrary, e.g. AIC versus BIC.\n-   The adjusted-$R^2$ failed to penalize `m_extreme`, although it was clearly overfitting the data.\n\n## Predictive performance\n\nInstead of using these criteria, we could look for the model with the best **predictions performance**.\n\nThat is, the model that makes predictions for **new** observations that are the closest to the true values.\n\n. . .\n\nWe will learn two approaches to accomplish this\n\n-   the holdout method\n-   cross-validation\n\n## The holdout method {.smaller}\n\nThe holdout method is a simple method to evaluate the predictive performance of a model.\n\n. . .\n\n1.  Randomly partition your sample into two sets: a **training set** (typically 2/3 of the sample) and a **test set** (the remaining 1/3).\n\n. . .\n\n2.  Fit your model to the training set.\n\n. . .\n\n3.  Evaluate the prediction accuracy of the model on the test set.\n\n. . .\n\nNote that the test set consists of *new* observations for the model.\n\n. . .\n\n$\\Rightarrow$ Select the model with the best prediction accuracy in step 3.\n\n## \n\n![The holdout method.](images/lec-6/holdout.PNG){fig-align=\"center\"}\n\nSource: [IMS](https://openintro-ims.netlify.app/inf-model-mlr.html#comparing-two-models-to-predict-body-mass-in-penguins)\n\n## Step 1: training and test sets\n\nThe following `R` function splits a sample into a training and a test set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconstruct_training_test <- function(sample, prop_training = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n * prop_training)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,    1 : n_training )\n  sample_test     <- slice(sample_random, - (1 : n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}\n```\n:::\n\n## \n\n::: panel-tabset\n## Sample\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree # entire sample\n```\n\n::: {.cell-output-stdout}\n```\n       Girth Height Volume\n1   8.931477     70   10.3\n2   8.436883     65   10.3\n3   9.464900     63   10.2\n4  11.136215     72   16.4\n5  10.907321     81   18.8\n6  10.030025     83   19.7\n7  10.535716     66   15.6\n8  10.852640     75   18.2\n9  11.097116     80   22.6\n10 12.402327     75   19.9\n11 11.681797     79   24.2\n12 11.000495     76   21.0\n13 10.826171     76   21.4\n14 11.555269     69   21.3\n15 11.850392     75   19.1\n16 12.694245     74   22.2\n17 13.026112     85   33.8\n18 12.854039     86   27.4\n19 13.917842     71   25.7\n20 13.181231     64   24.9\n21 13.887866     78   34.5\n22 14.388698     80   31.7\n23 14.566668     74   36.3\n24 16.402095     72   38.3\n25 16.271447     77   42.6\n26 17.551804     81   55.4\n27 18.042885     82   55.7\n28 17.554523     80   58.3\n29 17.357700     80   51.5\n30 18.023363     80   51.0\n31 20.482147     87   77.0\n```\n:::\n:::\n\n## Training set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(0) # set the seed of the random number generator to 0\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntraining_set\n```\n\n::: {.cell-output-stdout}\n```\n       Girth Height Volume\n1  11.555269     69   21.3\n2  16.271447     77   42.6\n3  11.136215     72   16.4\n4  10.535716     66   15.6\n5   8.931477     70   10.3\n6   8.436883     65   10.3\n7  14.566668     74   36.3\n8  11.681797     79   24.2\n9  20.482147     87   77.0\n10 12.854039     86   27.4\n11 13.917842     71   25.7\n12 18.042885     82   55.7\n13 12.402327     75   19.9\n14 18.023363     80   51.0\n15 13.887866     78   34.5\n16 17.554523     80   58.3\n17 11.097116     80   22.6\n18 10.907321     81   18.8\n19 14.388698     80   31.7\n20 11.850392     75   19.1\n21 11.000495     76   21.0\n```\n:::\n:::\n\n## Test set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_set <- training_test_sets[[\"test\"]]\ntest_set\n```\n\n::: {.cell-output-stdout}\n```\n      Girth Height Volume\n1  10.82617     76   21.4\n2  13.02611     85   33.8\n3  17.55180     81   55.4\n4  10.85264     75   18.2\n5  10.03002     83   19.7\n6  13.18123     64   24.9\n7  17.35770     80   51.5\n8   9.46490     63   10.2\n9  16.40209     72   38.3\n10 12.69424     74   22.2\n```\n:::\n:::\n:::\n\n## Step 2: fit the model to the training set\n\nWe simply fit our regression model to the **training set**.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(Volume ~ Girth, data = training_set)\n```\n:::\n\n## Step 3: Evaluate the prediction accuracy on the test set\n\nTo evaluate the prediction accuracy, we start by computing the predictions for the observations in the **test set**.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nVolume_hat <- predict(m1, test_set)\n```\n:::\n\n. . .\n\nA good model will make predictions that are closed to the true values of the response variable.\n\n## Sum of squared errors\n\nA good measure of prediction accuracy is the **sum of squared errors** (SSE).\n\n$$\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2\n$$\n\nSmall SSE is better.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nVolume <- test_set$Volume\nsum((Volume - Volume_hat)^2)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 211.4543\n```\n:::\n:::\n\n## Mean squared error\n\nIn practice, the **(root) mean sum of squared errors** ((R)MSE) is often used.\n\n$$\nMSE = \\dfrac{SSE}{m} = \\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}\n$$\n\n$$\nRMSE = \\sqrt{\\dfrac{SSE}{m}} = \\sqrt{\\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}}\n$$ Again, **small (R)MSE is better**.\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSSE <- sum ((Volume - Volume_hat)^2)\nMSE <- mean((Volume - Volume_hat)^2)\nRMSE <- sqrt(MSE)\nSSE; MSE; RMSE\n```\n\n::: {.cell-output-stdout}\n```\n[1] 211.4543\n```\n:::\n\n::: {.cell-output-stdout}\n```\n[1] 21.14543\n```\n:::\n\n::: {.cell-output-stdout}\n```\n[1] 4.598416\n```\n:::\n:::\n\n::: callout-tip\n## (R)MSE\n\nThe advantage of (R)MSE over SSE is that we can compare models that are evaluated with test sets of different sizes.\n:::\n\n## Selecting a model\n\nApply steps 2 and 3 on different models.\n\n-   use the same training and test sets from step 1 for the different models\n\n. . .\n\n$\\Rightarrow$ Choose the model with the lowest (R)MSE!\n\nThis model has the best **predictive accuracy**.\n\n## Function to compute the RMSE\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_RMSE <- function(test_set, model){\n  \n  y     <- test_set$Volume # truth\n  y_hat <- predict(model, test_set) # predicted value\n  \n  E    <- y - y_hat\n  MSE  <- mean(E^2)\n  RMSE <- sqrt(MSE)\n  \n  return(RMSE)\n  \n}\n```\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1 - partition sample into a training set and a test set\n# already done\n\n# Step 2 - fit models to training set\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3 - evaluate models on test set\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2)\ncompute_RMSE(test_set, m3) # best\ncompute_RMSE(test_set, m48)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4.598416\n[1] 4.07333\n[1] 3.957379\n[1] 29.97533\n```\n:::\n:::\n\nWe select `m3`.\n\n## Limitation of the holdout method\n\nThe previous analysis was conducted with `set.seed(0)`.\n\n. . .\n\nNote that models `m2` and `m3` were pretty close.\n\nCould we have obtained a different result with a different seed?\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2|\"}\n# Step 1\nset.seed(1) # new seed\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntest_set <- training_test_sets[[\"test\"]]\n\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2) # best\ncompute_RMSE(test_set, m3)\ncompute_RMSE(test_set, m48)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 6.097541\n[1] 4.248172\n[1] 10.37698\n[1] 103753769\n```\n:::\n:::\n\n## The test set matters!\n\nWith `set.seed(1)`, the holdout method indicates that `m2` is the best model.\n\n-   `m3`, our previous best model, is worse than `m1`!\n\n. . .\n\nA drawback of the holdout method is that the test set matters a lot.\n\n-   Repeating steps 2 and 3 with a different partition in step 1 may give different results.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - limitation of the holdout method\n\nCopy and paste the following code in your `R` session. Try a few different seed numbers for step 1. Keep track of which model has the lowest RMSE. Is `m1` ever the best model?\n\nYou can safely ignore the warning messages.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setup\nset.seed(0) # do not change this seed number\nd_tree <- datasets::trees %>% mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5))\ncompute_RMSE <- function(test_set, model){\n  \n  y     <- test_set$Volume\n  y_hat <- predict(model, test_set)\n  \n  E    <- y - y_hat\n  MSE  <- mean(E^2)\n  RMSE <- sqrt(MSE)\n  \n  return(RMSE)\n  \n}\nconstruct_training_test <- function(sample, prop_training = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n * prop_training)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,    1 : n_training )\n  sample_test     <- slice(sample_random, - (1 : n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}\n\n# Step 1\nset.seed(2) # try a few different seed numbers\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntest_set <- training_test_sets[[\"test\"]]\n\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\ncompute_RMSE(test_set, m2)\ncompute_RMSE(test_set, m3)\ncompute_RMSE(test_set, m48)\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_628cf12c\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Cross-validation\n\nCross-validation (CV) is a natural generalization of the holdout method:\n\n-   repeat the holdout method many times with different partitions in step 1.\n\n## \n\n0.  randomly partition the sample into $k$ folds of equal size. $k$ is typically $5$ or $10$.\n\n. . .\n\nRepeat the following steps $k$ times\n\n1.  Let fold 1 be the test set and the remaining folds be the training set.\n\n2.  Fit the model to the training set (like the holdout method)\n\n3.  Evaluate the prediction accuracy of the model on the test set (like the holdout method)\n\n4.  Go back to step 1, let the next fold be the test set.\n\n. . .\n\n$\\Rightarrow$ Select the model with the best overall prediction accuracy in step 3.\n\n## \n\n![5-fold cross-validation](images/lec-6/cv.PNG){fig-align=\"center\"}\n\nSource: [towardsdatascience](https://towardsdatascience.com/validating-your-machine-learning-model-25b4c8643fb7)\n\n## Step 0\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(0)\n\n# Setup\nn <- nrow(d_tree)\nn_folds <- 10\n\n# Fold assignment\nfolds <- c(rep(1:n_folds, n %/% n_folds), seq_along(n %% n_folds))\n# you do not need to understand the details of the previous line\n# just note that it creates folds of (almost) the same size\nfolds\n```\n\n::: {.cell-output-stdout}\n```\n [1]  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5\n[26]  6  7  8  9 10  1\n```\n:::\n\n```{.r .cell-code}\n# Step 1\nd_tree_fold <- d_tree %>%\n  slice_sample(n = n) %>% # shuffle the rows\n  mutate(fold = folds)\nhead(d_tree_fold)\n```\n\n::: {.cell-output-stdout}\n```\n      Girth Height Volume fold\n1 11.555269     69   21.3    1\n2 16.271447     77   42.6    2\n3 11.136215     72   16.4    3\n4 10.535716     66   15.6    4\n5  8.931477     70   10.3    5\n6  8.436883     65   10.3    6\n```\n:::\n:::\n\n## Steps 1-4 -- Setup\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncreate_empty_RMSE <- function(){\n  tibble(\n    fold     = numeric(),\n    rmse_m1  = numeric(), \n    rmse_m2  = numeric(), \n    rmse_m3  = numeric(), \n    rmse_m48 = numeric()\n  )\n}\n\nRMSE <- create_empty_RMSE()\nRMSE\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 0 x 5\n# ... with 5 variables: fold <dbl>, rmse_m1 <dbl>, rmse_m2 <dbl>,\n#   rmse_m3 <dbl>, rmse_m48 <dbl>\n```\n:::\n:::\n\n## Steps 1-4\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nRMSE <- create_empty_RMSE()\n\nfor(i in 1 : n_folds){\n  \n  # Step 1\n  training_set <- filter(d_tree_fold, fold != i)\n  test_set     <- filter(d_tree_fold, fold == i)\n  \n  # Step 2\n  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\n  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\n  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\n  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n  \n  # Step 3\n  rmse_m1  <- compute_RMSE(test_set, m1 )\n  rmse_m2  <- compute_RMSE(test_set, m2 )\n  rmse_m3  <- compute_RMSE(test_set, m3 )\n  rmse_m48 <- compute_RMSE(test_set, m48)\n  \n  RMSE <- RMSE %>% \n    add_row(fold = i, rmse_m1, rmse_m2, rmse_m3, rmse_m48)\n  \n}\n```\n:::\n\n## Model selection with RMSE\n\nWhich model has the lowest RMSE across all folds?\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nRMSE\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 10 x 5\n    fold rmse_m1 rmse_m2 rmse_m3   rmse_m48\n   <dbl>   <dbl>   <dbl>   <dbl>      <dbl>\n 1     1    5.07    3.50    3.54       4.25\n 2     2    2.89    2.34    1.95      10.8 \n 3     3    4.78    5.02    5.11       5.54\n 4     4    2.82    3.49    3.45       3.15\n 5     5    1.67    2.99    2.67      72.5 \n 6     6    6.38    4.45    5.38     412.  \n 7     7    3.52    2.89    2.83       4.73\n 8     8    1.37    2.01    1.77       3.90\n 9     9    7.49    3.14    3.92 2481852.  \n10    10    5.60    4.53    4.27       4.47\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummarise_all(RMSE, mean)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1   5.5    4.16    3.44    3.49  248237.\n```\n:::\n\n```{.r .cell-code}\nsummarise_all(RMSE, sum ) # what the book uses\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1    55    41.6    34.4    34.9 2482374.\n```\n:::\n:::\n\n`m2` has slightly better overall RMSE.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - cross-validation\n\nExercise [25.9](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises), [25.11](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises), [25.12](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)\n\nIn part c (25.9 and 25.11) , replace *left* and *right*, by *top* and *bottom*.\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_628cf001\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Short podcast on cross-validation\n\nCross-validation is a central technique in machine learning.\n\nStill not totally clear how it works? Check out this [podcast](https://dataskeptic.com/blog/episodes/2014/cross-validation) to hear more about this important topic.\n\n. . .\n\nThese two other podcasts on [muliple linear regression](https://dataskeptic.com/blog/episodes/2016/multiple-regression) and [$R^2$](https://dataskeptic.com/blog/episodes/2016/r-squared) are also excellent.\n\n## Taking things to the extreme: LOOCV\n\nSet $k=n$.\n\n-   that is, use $n$ folds, each of size $1$.\n\n-   Each test set will therefore consist of a single observation and the corresponding training set of the remaining $n-1$ observations.\n\n## LOOCV {.smaller}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nRMSE <- create_empty_RMSE()\n\n# Step 0\nn <- nrow(d_tree)\nd_tree_loocv <- mutate(d_tree, folds = 1 : n) # every observation belongs to a different fold \n\nfor(i in 1:n){\n  \n  # Step 1\n  training_set <- filter(d_tree_loocv, folds != i)\n  test_set     <- filter(d_tree_loocv, folds == i)\n  \n  # Step 2\n  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\n  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\n  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\n  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n  \n  # Step 3\n  rmse_m1  <- compute_RMSE(test_set, m1 )\n  rmse_m2  <- compute_RMSE(test_set, m2 )\n  rmse_m3  <- compute_RMSE(test_set, m3 )\n  rmse_m48 <- compute_RMSE(test_set, m48)\n  \n  RMSE <- RMSE %>%\n    add_row(fold = i, rmse_m1, rmse_m2, rmse_m3, rmse_m48)\n  \n}\n\nsummarize_all(RMSE, mean)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 5\n   fold rmse_m1 rmse_m2 rmse_m3 rmse_m48\n  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>\n1    16    3.64    3.11    3.12  142697.\n```\n:::\n:::\n\n# Model selection: stepwise procedures\n\n## Stepwise procedures\n\nNot my favorite method, but it is widely used.\n\n. . .\n\n-   Akin the \"throw cooked spaghetti to the wall and see what sticks\" technique.\n-   This is what you do when you are clueless about what variables may be good predictors\n    -   ...but then you should probably collaborate with a scientific expert.\n\n. . .\n\nTwo types of stepwise procedures: (i) **forward selection** and (ii) **backward elimination**.\n\n## Forward selection {.smaller}\n\n1.  Choose an overall criterion that balances GoF (small SSR) and parsimony (small $p$)\n\n    $\\Rightarrow$ adjusted-$R^2$, AIC or BIC\n\n. . .\n\n2.  Start with the **empty** model $Y \\approx \\beta_0$, i.e. the model with no predictor. This is our **current model**\n\n3.  Fit all possible models with one additional predictor.\n\n. . .\n\n4.  Compute the criterion, say the AIC, of each of these models.\n\n5.  Identify the model with the smallest AIC. This is our **candidate model**.\n\n. . .\n\n6.  If the AIC of the candidate model is *smaller* (better) than the AIC of the current model, the candidate model becomes the current model. Go back to step 3.\n\n    If the AIC of the candidate model is *larger* than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - forward selection\n\nExercise [8.13](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises). In addition, fit the candidate model (a single model) in `R` with the command `lm`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_628cf26f\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Backward elimination\n\nSimilar to forward selection, except that\n\n-   We start with the full model.\n-   We remove one predictor at a time,\n-   until removing any predictor makes the model worse.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - backward elimination\n\nExercise [8.11](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises). In addition, fit the current and candidate models in `R` with the command `lm`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_628cf2b5\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## \n\n::: callout-warning\n\n## Forward selection versus backward elimination \nNote that forward selection and backward elimination need not agree; they may select different models!\n\n. . .\n\n-   What to do in that case? Nobody knows.\n\n:::\n\n\n# How to prevent overfitting?\n\n::: tabset-tip\n## Parsimony\n\nTo obtain a parsimonious model\n\n-   use your knowledge of the subject to **carefuly** choose which variables to consider and construct new ones, and\n-   implement a model selection procedure.\n:::\n\n# Recap\n\n## Recap {.smaller}\n\n::: incremental\n-   Overfitting\n\n    -   limitations of $R^2$ for model selection\n\n-   Model selection through\n\n    ::: incremental\n    -   overall criteria (adjusted-$R^2$, AIC, BIC)\n\n    -   predictive performance (holdout method, cross-validation)\n\n    -   stepwise procedure (forward, backward)\n    :::\n    \n:::\n\n. . .\n\nYou are now fully equipped for the prediction project!\n\nüçÄ Good luck! üçÄ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}