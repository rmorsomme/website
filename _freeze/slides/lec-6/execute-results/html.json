{
  "hash": "dc0bf3ba06f5fa98a6ad78ce04ac5490",
  "result": {
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"STA 101L - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-link: true\n    self-contained: true\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   \n\n## Recap {.smaller}\n\n::: incremental\n-   simple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X\n    $$\n\n-   multiple linear regression model $$\n    Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots +  + \\beta_p X_p \n    $$\n\n-   categorical predictor\n\n    -   $(k-1)$ indicator variables\n\n-   feature engineering\n\n    -   transforming variables\n    -   combining variables\n    -   interactions\n:::\n\n## Outline\n\n-   Competing models\n-   Overfitting\n\n# Competing models\n\n## Competing models\n\n-   Earlier, we saw that adding the predictor $\\dfrac{1}{\\text{displ}}$ gave a better fit.\n\n-   Let us see if the same idea work with the `trees` dataset.\n\n## A nonlinear association?\n\nSuppose we want to predict volume using only the variable `girth`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree <- datasets::trees\nggplot(d_tree) +\n  geom_point(aes(Girth, Volume))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\nOne could argue that there is a slight nonlinear association\n\n## Function to compute $R^2$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}\n```\n:::\n\n## Starting simple...\n\nWe start with the simple model\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(Volume ~ Girth, data = d_tree)\ncompute_R2(m1)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.935\n```\n:::\n:::\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m1, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.3         5.10\n2  8.30        5.11\n3  8.30        5.11\n4  8.30        5.12\n5  8.30        5.12\n6  8.31        5.13\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...taking it up a notch...\n\nTo capture the nonlinear association between `Girth` and `Volume`, we consider the predictor $\\text{Girth}^2$.\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2\n$$\n\nThe command to fit this model is\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree2 <- mutate(d_tree, Girth2 = Girth^2)\nm2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)\ncompute_R2(m2)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.962\n```\n:::\n:::\n\n. . .\n\n$R^2$ has increased! It went from 0.935 (model 1) to 0.962.\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2)\nVolume_pred <- predict(m2, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 3\n  Girth Girth2 Volume_pred\n  <dbl>  <dbl>       <dbl>\n1  8.3    68.9        11.0\n2  8.30   68.9        11.0\n3  8.30   68.9        11.0\n4  8.30   68.9        11.0\n5  8.30   69.0        11.0\n6  8.31   69.0        11.0\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...taking it up another notch...\n\nLet us consider the predictor $\\text{Girth}^3$.\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\beta_3 \\text{girth}^3\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree3 <- mutate(d_tree, Girth2 = Girth^2, Girth3 = Girth^3)\nm3 <- lm(Volume ~ Girth + Girth2 + Girth3, data = d_tree3)\ncompute_R2(m3)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.963\n```\n:::\n:::\n\n. . .\n\n$R^2$ has increased! It went from 0.962 (model 2) to 0.963.\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)\nnew_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2, Girth3 = Girth^3)\nVolume_pred <- predict(m3, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 4\n  Girth Girth2 Girth3 Volume_pred\n  <dbl>  <dbl>  <dbl>       <dbl>\n1  8.3    68.9   572.        9.88\n2  8.30   68.9   572.        9.88\n3  8.30   68.9   572.        9.89\n4  8.30   68.9   572.        9.89\n5  8.30   69.0   573.        9.89\n6  8.31   69.0   573.        9.90\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## ...before taking things to the extreme\n\nWhat if we also include the predictors $\\text{Girth}^4, \\text{Girth}^5, \\dots, \\text{Girth}^{k}$ for some larger number $k$?\n\nThe following `R` code fits such a model with $k=34$, that is,\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\dots + \\beta_{29} \\text{girth}^{34}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_extreme <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = d_tree)\ncompute_R2(m_extreme)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.98\n```\n:::\n:::\n\n$R^2$ has again increased! It went from 0.963 (model 3) to 0.98.\n\n## A limitation of $R^2$\n\nAs we keep adding predictors, $R^2$ will always increase.\n\n-   additional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.\n\n. . .\n\nIs the model `m_extreme` a good model?\n\n::: callout-note\nWe want to learn about the relation between `Volume` and `Girth` present in the *population* (not the sample).\n\nA good model should accurately represent the *population*.\n:::\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\" hash='lec-6_cache/revealjs/unnamed-chunk-13_55e139a5fc3b2c342b07f99c3b3629a9'}\n\n```{.r .cell-code  code-line-numbers=\"1|2|3|4|\"}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.00001)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m_extreme, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.3         10.3\n2  8.30        10.3\n3  8.30        10.3\n4  8.30        10.3\n5  8.30        10.3\n6  8.30        10.3\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Regression line (zoomed)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred)) +\n  ylim(0, 90)\n```\n\n::: {.cell-output-display}\n![](lec-6_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## Overfitting\n\nThe model `m_extreme` **overfits** the data.\n\nA model overfits the data when it fits the sample extremely well but does a poor job for new data.\n\n. . .\n\nRemember that we want to learn about the *population*, not the *sample*!\n\n# Model selection: criteria\n\n## $R^2$\n\nWe saw that $R^2$ keeps increasing as we add predictors.\n\n$$\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\n$$\n\n. . .\n\n$R^2$ can therefore not be used to identify models that overfit the data.\n\n## Adjusted-$R^2$\n\nThe **adjusted-**$R^2$ is similar to \\$R^2^, but penalizes large models:\n\n$$\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\\dfrac{n-1}{n-p-1}\n$$\n\nwhere $p$ corresponds to the number of predictors.\n\n## \n\nThe adjusted-$R^2$ therefore balances goodness of fit and parsimony:\n\n-   The ratio $\\dfrac{SSR}{SST}$ favors model with good fit (like $R^2$)\n-   The ratio $\\dfrac{n-1}{n-k-1}$ favors parsimonious models\n\n. . .\n\nThe model with the highest adjusted-$R^2$ typically provides a good fit without overfitting.\n\n## AIC and BIC {.smaller}\n\nTwo other popular criteria that balance goodness of fit (small SSR) and parsimony (small $p$) are\n\n-   the Akaike Information Criterion (AIC)\n-   the Bayesian Inofrmation Criterion (BIC)\n\n. . .\n\nThe formula for AIC and BIC are respectively\n\n$$\nAIC = 2p - \\text{GoF}, \\qquad BIC = \\ln(n)p- \\text{GoF}\n$$\n\nwhere $\\text{GoF}$ measures the *Goodness of fit* of the model[^1].\n\n[^1]: The exact formula for $\\text{GoF}$ is beyond the scope of this class.\n\n. . .\n\nUnlike the adjusted-$R^2$, smaller is better for the AIC and BIC.\n\n. . .\n\nNote that the BIC penalizes the number of parameters $p$ more strongly.\n\n## Computing AIC and BIC\n\nEasily accessible in `R` with the command `glance`. For instance,\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m1)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.935         0.933  4.25      419. 8.64e-19     1  -87.8  182.  186.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(m2)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.962         0.959  3.33      350. 1.52e-20     2  -79.7  167.  173.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(m3)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.963         0.959  3.35      232. 2.17e-19     3  -79.3  169.  176.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(m_extreme)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.980         0.960  3.30      48.6 5.64e-10    15  -69.7  173.  198.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n. . .\n\nIn this case, AIC and BIC favor `m2`, while the adjusted-$R^2$ (wrongly) favor `m_extreme`.\n\n-   For AIC and BIC, smaller is better.\n\n# Model selection: predictive performance\n\n## Limitations of the previous criteria\n\nThe adjutsed-$R^2$, AIC and BIC all try to balance\n\n1.  goodness of fit\n2.  parsimony\n\n. . .\n\nThey achieve this balance by favoring models with small SSR while penalizing models with larger $p$.\n\n. . .\n\nBut\n\n-   The form of the penalty for $p$ is somewhat arbitrary, e.g. AIC versus BIC.\n-   The adjusted-$R^2$ failed to penalize `m_extreme`, although it was clearly overfitting the data.\n\n## Predictive performance\n\nInstead of using these criteria, we could look for the model with the best **predictions performance**.\n\nThat is, the model that makes predictions for new observations that are the closest to the true values.\n\n. . .\n\nWe will learn two approaches to accomplish this\n\n-   the holdout method\n-   cross-validation\n\n## The holdout method {.smaller}\n\nThe holdout method is a simple method to evaluate the predictive performance of a model.\n\n. . .\n\n1.  Randomly partition your sample into two sets: a **training set** (typically 2/3 of the sample) and a **test set** (the remaining 1/3).\n\n. . .\n\n2.  Fit your model to the training set.\n\n. . .\n\n3.  Evaluate the prediction accuracy of the model on the test set.\n\n. . .\n\nNote that the test set consists of *new* observations for the model.\n\n. . .\n\n$\\Rightarrow$ Select the model with the best prediction accuracy in step 3.\n\n## \n\n![The holdout method.](images/lec-6/holdout.PNG){fig-align=\"center\"}\n\nSource: [IMS](https://openintro-ims.netlify.app/inf-model-mlr.html#comparing-two-models-to-predict-body-mass-in-penguins)\n\n## Step 1: training and test sets\n\nThe following `R` function splits a sample into a training and a test set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconstruct_training_test <- function(sample, prop_training = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n * prop_training)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,    1 : n_training )\n  sample_test     <- slice(sample_random, - (1 : n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}\n```\n:::\n\n## \n\n::: panel-tabset\n## Sample\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree # entire sample\n```\n\n::: {.cell-output-stdout}\n```\n   Girth Height Volume\n1    8.3     70   10.3\n2    8.6     65   10.3\n3    8.8     63   10.2\n4   10.5     72   16.4\n5   10.7     81   18.8\n6   10.8     83   19.7\n7   11.0     66   15.6\n8   11.0     75   18.2\n9   11.1     80   22.6\n10  11.2     75   19.9\n11  11.3     79   24.2\n12  11.4     76   21.0\n13  11.4     76   21.4\n14  11.7     69   21.3\n15  12.0     75   19.1\n16  12.9     74   22.2\n17  12.9     85   33.8\n18  13.3     86   27.4\n19  13.7     71   25.7\n20  13.8     64   24.9\n21  14.0     78   34.5\n22  14.2     80   31.7\n23  14.5     74   36.3\n24  16.0     72   38.3\n25  16.3     77   42.6\n26  17.3     81   55.4\n27  17.5     82   55.7\n28  17.9     80   58.3\n29  18.0     80   51.5\n30  18.0     80   51.0\n31  20.6     87   77.0\n```\n:::\n:::\n\n## Training set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(0)\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntraining_set\n```\n\n::: {.cell-output-stdout}\n```\n   Girth Height Volume\n1   11.7     69   21.3\n2   16.3     77   42.6\n3   10.5     72   16.4\n4   11.0     66   15.6\n5    8.3     70   10.3\n6    8.6     65   10.3\n7   14.5     74   36.3\n8   11.3     79   24.2\n9   20.6     87   77.0\n10  13.3     86   27.4\n11  13.7     71   25.7\n12  17.5     82   55.7\n13  11.2     75   19.9\n14  18.0     80   51.0\n15  14.0     78   34.5\n16  17.9     80   58.3\n17  11.1     80   22.6\n18  10.7     81   18.8\n19  14.2     80   31.7\n20  12.0     75   19.1\n21  11.4     76   21.0\n```\n:::\n:::\n\n## Test set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_set <- training_test_sets[[\"test\"]]\ntest_set\n```\n\n::: {.cell-output-stdout}\n```\n   Girth Height Volume\n1   11.4     76   21.4\n2   12.9     85   33.8\n3   17.3     81   55.4\n4   11.0     75   18.2\n5   10.8     83   19.7\n6   13.8     64   24.9\n7   18.0     80   51.5\n8    8.8     63   10.2\n9   16.0     72   38.3\n10  12.9     74   22.2\n```\n:::\n:::\n:::\n\n## Step 2: fit the model to the training set\n\nWe simply fit our regression model to the training set.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(Volume ~ Girth, data = training_set)\n```\n:::\n\n## Step 3: Evaluate the prediction accuracy on the test set\n\nTo evaluate the prediction accuracy, we start by computing the predictions for the observations in test set.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nVolume_hat <- predict(m1, test_set)\n```\n:::\n\n. . .\n\nA good model will make predictions that are closed to the true values of the response variable.\n\n## Sum of squared errors\n\nA good measure of prediction accuracy is the **sum of squared errors** (SSE).\n\n$$\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2\n$$\n\nSmall SSE is better.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nVolume <- test_set$Volume\nsum(Volume - Volume_hat)^2\n```\n\n::: {.cell-output-stdout}\n```\n[1] 149.2704\n```\n:::\n:::\n\n\n## Mean squared error\n\nIn practice, the **(root) mean sum of squared errors** ((R)MSE) is often used.\n\n$$\nMSE = \\dfrac{SSE}{m} = \\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}\n$$\n\n$$\nRMSE = \\sqrt{\\dfrac{SSE}{m}} = \\sqrt{\\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}}\n$$\nAgain, small (R)MSE is better.\n\n##\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nSSE <- sum((Volume - Volume_hat)^2)\nm <- nrow(test_set)\nMSE <- SSE / m\nRMSE <- sqrt(MSE)\nSSE; MSE; RMSE\n```\n\n::: {.cell-output-stdout}\n```\n[1] 225.2413\n```\n:::\n\n::: {.cell-output-stdout}\n```\n[1] 22.52413\n```\n:::\n\n::: {.cell-output-stdout}\n```\n[1] 4.745959\n```\n:::\n:::\n\n:::callout-tip\n##(R)MSE\nThe advantage of (R)MSE over the SSE is that we can compare models evaluate with test sets of different sizes.\n:::\n\n## Selecting a model\n\nApply steps 2 and 3 on different models.\n\n-   use the same training and test sets for the different models\n\nSimply choose the model with the lowest (R)MSE.\n\nThis model has the best **out-of-sample** accuracy.\n\n## Function to compute the RMSE\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_RMSE <- function(test_set, model){\n  \n  y     <- test_set$Volume\n  y_hat <- predict(model, test_set)\n  \n  E    <- y - y_hat\n  SSE  <- sum(E^2)\n  MSE  <- mean(E^2)\n  RMSE <- sqrt(MSE)\n  \n  return(RMSE)\n  \n}\n```\n:::\n\n\n##\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4.745959\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m2)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4.16503\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m3) # best\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4.093268\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m48)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 13.47366\n```\n:::\n:::\n\nWe select `m3`.\n\n## Limitation of the holdout method\n\nThe previous analysis was conducted with `set.seed(0)`.\n\n. . .\n\nModels `m2` and `m3` were pretty close.\n\nCould we have obtained a different result with a different seed?\n\n##\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Step 1\nset.seed(1)\ntraining_test_sets <- construct_training_test(d_tree) \ntraining_set <- training_test_sets[[\"training\"]]\ntest_set <- training_test_sets[[\"test\"]]\n\n# Step 2\nm1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)\nm2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)\nm3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)\nm48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)\n\n# Step 3\ncompute_RMSE(test_set, m1)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 6.589842\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m2) # best\n```\n\n::: {.cell-output-stdout}\n```\n[1] 4.332965\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m3)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 5.437581\n```\n:::\n\n```{.r .cell-code}\ncompute_RMSE(test_set, m48)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 310105724\n```\n:::\n:::\n\n\n## The test set matters!\n\nA drawback of the holdout method is that the test set matters a lot.\n\nRepeating steps 2 and 3 with a different partition from step 1 may give different results.\n\n\n## Cross-validation\n\nCross-validation (CV) is the natural generalization of the holdout method.\n\n. . .\n\n0.    randomly partition the sample into $k$ folds of equal size. $k$ is typically $5$ or $10$. \n. . . \n\nRepeat the following steps $k$ times\n\n1.    Let fold 1 be the test set and the other folds be the training set\n\n2.    Fit the model to the training set (like the holdout method)\n\n3.    Evaluate the prediction accuracy of the model on the test set (like the holdout method)\n\n4.    Go back to 1, letting the next fold be the test set.\n\n. . .\n\n$\\Rightarrow$ Select the model with the best overall prediction accuracy in step 3.\n\n## \n\n![5-fold cross-validation](images/lec-6/cv.PNG){fig-align=\"center\"}\n\nSource: [towardsdatascience](https://towardsdatascience.com/validating-your-machine-learning-model-25b4c8643fb7)\n\nReasoning behind CV: let each observation be in the test set once.\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(345)\n\nn_folds <- 10\n\ncounty_2019_nc_folds <- county_2019_nc %>%\n  slice_sample(n = nrow(county_2019_nc)) %>%\n  mutate(fold = rep(1:n_folds, n_folds)) %>%\n  arrange(fold)\n\npredict_folds <- function(i) {\n  fit <- lm(uninsured ~ hs_grad, data = county_2019_nc_folds %>% filter(fold != i))\n  predict(fit, newdata = county_2019_nc_folds %>% filter(fold == i)) %>%\n    bind_cols(county_2019_nc_folds %>% filter(fold == i), .fitted = .)\n}\n\nnc_fits <- map_df(1:n_folds, predict_folds)\n\np_nc_fits <- ggplot(nc_fits, aes(x = hs_grad, y = .fitted, group = fold)) +\n  geom_line(stat = \"smooth\", method = \"lm\", se = FALSE, size = 0.3, alpha = 0.5) +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Predicted uninsurance rate in NC\",\n    subtitle = glue(\"For {n_folds} different testing datasets\")\n    )\n\np_nc_fits\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\nn_folds <- 10\n\ncounty_2019_ny_folds <- county_2019_ny %>%\n  slice_sample(n = nrow(county_2019_ny)) %>%\n  mutate(fold = c(rep(1:n_folds, 6), 1, 2)) %>%\n  arrange(fold)\n\npredict_folds <- function(i) {\n  fit <- lm(uninsured ~ hs_grad, data = county_2019_ny_folds %>% filter(fold != i))\n  predict(fit, newdata = county_2019_ny_folds %>% filter(fold == i)) %>%\n    bind_cols(county_2019_ny_folds %>% filter(fold == i), .fitted = .)\n}\n\nny_fits <- map_df(1:n_folds, predict_folds)\n\np_ny_fits <- ggplot(ny_fits, aes(x = hs_grad, y = .fitted, group = fold)) +\n  geom_line(stat = \"smooth\", method = \"lm\", se = FALSE, size = 0.3, alpha = 0.5) +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Predicted uninsurance rate in NY\",\n    subtitle = glue(\"For {n_folds} different testing datasets\")\n    )\n\np_ny_fits\n```\n:::\n\n## Taking things to the extreme: LOOCV\n\nSet $k=n$; that is, we use $n$ folds, each of size $1$. The test sets will therefore consist of a single observation and the training sets of $n-1$ observations.\n\n# Model selection: stepwise procedures\n\n-   Not my favorite method,\n\n-   but this is something you should learn because it is widely used.\n\n. .\n\n-   Akin the \"throw cooked spaghetti to the wall and see what sticks\" technique.\n\n. . .\n\nStepwise selection procedures ar of two types: (i) forward and (ii) backward.\n\n## Forward selection procedure\n\n1.  Choose a criterion that balances model fit (smaller SSR) and parsimony (small $k$).\n\n<!-- -->\n\ni.  e.g. adjusted-$R^2$, **AIC** or BIC\n\n. . .\n\n2.  Start with the empty model $Y \\approx \\beta_0$, i.e. the model with no predictor. This is our **current model**\n\n3.  Fit all possible models with one additional predictor.\n\n. .\n\n4.  Compute the AIC of each of these models\n\n<!-- -->\n\ni.  Identify the model with the smallest AIC. This is our **candidate model**.\n\n. . .\n\nii. If the AIC of the candidate model is *smaller* (better) than the AIC of the current model, the candidate model becomes the current model, and we go back to step 3.\n\niii. If the AIC of the candidate model is *larger* than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model.\n\n## Backward selection procedure\n\nSimilar to forward selection, except that\n\n-   we start with the full model,\n-   remove one predictor at a time\n-   until removing any predictors makes the model worse.\n\n. . .\n\nNote that forward and backward selection need not agree; they may select different models.\n\n. . .\n\n-   What to do in that case? Nobody knows.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - stepwise selection\n\n-   Exercises 8.11, 8.13\n\n-   In addition\n\ni.  fit the first models in `R`\nii. identify the baseline level of the categorical variable in each model.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_6286996d\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# How to prevent overfitting?\n\n-   Parsimony\n    -   Use domain knowledge\n    -   One-in-ten rule (could be one-in-five)\n-   Multicollinearity\n    -   select one predictor\n    -   combine predictors, e.g. take the average.\n\n# Recap\n\n## Recap",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}