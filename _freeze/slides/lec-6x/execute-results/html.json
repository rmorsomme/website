{
  "hash": "aff2d7cfa16fd25cf726637359c4c056",
  "result": {
    "markdown": "---\ntitle: \"Model Selection\"\nsubtitle: \"STA 101 - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[sta101-suI22.github.io/website](https://sta101-suI22.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-link: true\n    self-contained: true\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   assignments\n\n## Recap\n\n# Competing Models\n\n-   Earlier, we saw that adding the predictor $\\dfrac{1}{\\text{displ}}$ gave a better fit.\n\n-   Let us see if the same idea work with the `trees` dataset.\n\n## a nonlinear association?\n\nSuppose we want to predict volume using only the variable `girth`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree <- datasets::trees\nggplot(d_tree) +\n  geom_point(aes(Girth, Volume))\n```\n\n::: {.cell-output-display}\n![](lec-6x_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=80%}\n:::\n:::\n\nOne could argue that there is a slight nonlinear association\n\n## `R` function to compute $R^2$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncompute_R2 <- function(model){\n  \n  model_glanced <- glance(model)\n  R2 <- model_glanced$r.squared\n  R2_rounded <- round(R2, 3) \n  \n  return(R2_rounded)\n  \n}\n```\n:::\n\n## Starting simple...\n\nWe start with the simple model\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1 <- lm(Volume ~ Girth, data = d_tree)\ncompute_R2(m1)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.935\n```\n:::\n:::\n\n## ...taking it up a notch...\n\nTo capture the nonlinear association between `Girth` and `Volume`, we consider the predictor $\\text{Girth}^2$.\n\n$$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2\n$$\n\nThe `R` to fit this model is\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_tree2 <- mutate(d_tree, Girth2 = Girth^2)\nm2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)\ncompute_R2(m2)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.962\n```\n:::\n:::\n\n. . .\n\n$R^2$ has increased! It went from 0.935 to 0.962.\n\n## ...before taking things to the extreme\n\nWhat if we also include the predictors $\\text{Girth}^3, \\text{Girth}^4, \\dots, \\text{Girth}^{k}$ for some larger number $k$?\n\nThe following `R` code fits such a model with $k=29$, that is, $$\n\\text{Volume} \\approx \\beta_0 + \\beta_1 \\text{girth} + \\beta_2 \\text{girth}^2 + \\dots + \\beta_{29} \\text{girth}^{29}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_extreme <- lm(Volume ~ poly(Girth, degree = 30, raw = TRUE), data = d_tree)\ncompute_R2(m_extreme)\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.975\n```\n:::\n:::\n\n$R^2$ has again increased! It went from 0.962 to 0.975.\n\n## Overfitting\n\nIn fact, if we keep adding predictors, $R^2$ will always increase. - additional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.\n\n. . .\n\nIs the `m_extreme` model a good model? - does it accurately represent the *population*? - remember that we want to learn about the relation between `Volume` and `Girth` present in the *population* (not the sample).\n\n## \n\n::: panel-tabset\n## `R` code\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|3|4|\"}\nGirth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.01)\nnew_data <- tibble(Girth = Girth_new)\nVolume_pred <- predict(m_extreme, new_data)\nnew_data <- mutate(new_data, Volume_pred = Volume_pred)\nhead(new_data)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 2\n  Girth Volume_pred\n  <dbl>       <dbl>\n1  8.3         10.2\n2  8.31        10.3\n3  8.32        10.4\n4  8.33        10.5\n5  8.34        10.6\n6  8.35        10.7\n```\n:::\n:::\n\n## Regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred))\n```\n\n::: {.cell-output-display}\n![](lec-6x_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Regression line (zoomed)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot() +\n  geom_point(data = d_tree  , aes(Girth, Volume     )) +\n  geom_line (data = new_data, aes(Girth, Volume_pred)) +\n  ylim(10, 77)\n```\n\n::: {.cell-output-display}\n![](lec-6x_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## \n\nThe model `m_extreme` **overfits** the data.\n\nA model overfits data when it corresponds very closely to the data set and does a poor job for new data.\n\n. . .\n\nRemember that we want to learn about the *population*, not the *sample*!\n\n# Model selection: criteria\n\n## Adjusted-$R^2$\n\nWe saw that $R^2$ keeps increasing as we add predictors.\n\n$$\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\n$$\n\n$R^2$ can therefore not be used to identify models that overfit the data.\n\n. . .\n\nInstead, we use the **adjusted-**$R^2$.\n\n$$\nR^2_{\\text{adj}} = 1 - \\dfrac{SSR}{SST}\\dfrac{n-1}{n-p-1}\n$$\n\nwhere $p$ corresponds to the number of predictors.\n\n## \n\nThe ratio $\\dfrac{SSR}{SST}$ favors model that closely fit the data.\n\nThe ratio $\\dfrac{n-1}{n-k-1}$ penalizes model with many predictors.\n\n. . .\n\nThe model with the highest adjusted-$R^2$ typically hits the sweet spot.\n\n## AIC and BIC\n\nTwo popular criteria that balance goodness of fit (small SSR) and parsimony (small $p$) are\n\n-   the Akaike Information Criterion (AIC)\n-   the Bayesian Inofrmation Criterion (BIC)\n\n. . .\n\nThe formula for AIC and BIC are respectively $$\nAIC = 2p - \\text{GoF}, \\qquad BIC = \\ln(n)p- \\text{GoF}\n$$ where $\\text{GoF}$ is a measure of the *Goodness of fit* of the model[^1].\n\n[^1]: The exact formula for $\\text{GoF}$ is beyond the scope of this class.\n\n. . .\n\nUnlike the adjusted-$R^2$, smaller is better for the AIC and BIC.\n\n. . .\n\nNote that the BIC penalizes the number of parameters $p$ more strongly.\n\n## Computing AIC and BIC\n\nEasily accessible in `R` with the command `glance`. For instance,\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(m1)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.935         0.933  4.25      419. 8.64e-19     1  -87.8  182.  186.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(m2)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.962         0.959  3.33      350. 1.52e-20     2  -79.7  167.  173.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(m_extreme)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.975         0.956  3.44      51.4 5.11e-11    13  -73.0  176.  197.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n. . .\n\nIn this case, all three criteria (ajudtsed-$R^2$, AIC and BIC) indicate that `m2` id the best model.\n\n-   For AIC and BIC, smaller is better.\n\n# Model selection: predictive performance\n\n## Limitations of the previous methods\n\nThe adjutsed-$R^2$, AIC and BIC all try to balance\n\n1.  goodness of fit\n2.  parsimony\n\n. . .\n\nThey achieve this balance by favoring models with small SSR while penalizing models with larger $p$.\n\n. . .\n\nThe form of the penalty for $p$ may seem somewhat arbitrary. - E.g. AIC versus BIC.\n\n## Predictive performance\n\nInstead, we could look for the model with the best **predictions performance**; that is, the model that makes predictions for new observations that are the closest to the true values.\n\n## The holdout method\n\nThe holdout method is a simple method to evaluate the predictive performance of a model.\n\n. . .\n\n1.  Randomly partition your sample into two sets: a **training set** (typically 2/3 of the sample) and a **test set**.\n\n. . .\n\n2.  Fit your model to the training set.\n\n. . .\n\n3.  Evaluate the prediction accuracy on the test set.\n\n. . .\n\nNote that the test set consists of *new* observations for the model.\n\n. . .\n\nA good model will model will have good prediction accuracy in step 3\n\n## Step 1: training and test sets\n\nThe following `R` function splits a sample into a training and a test set\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconstruct_training_test <- function(sample, prop = 2/3){\n  \n  n          <- nrow(sample)\n  n_training <- round(n*prop)\n  \n  sample_random   <- slice_sample(sample, n = n)\n  sample_training <- slice(sample_random,   1:n_training )\n  sample_test     <- slice(sample_random, -(1:n_training))\n  \n  return(list(\n    training = sample_training, test = sample_test\n    ))\n  \n}\n```\n:::\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_car <- ggplot2::mpg\ntraining_test_sets <- construct_training_test(d_car) \ntraining_set <- training_test_sets[[\"training\"]]\ntraining_set\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 156 x 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 volkswagen   new beetle   2.5  2008     5 auto~ f        20    29 r     subc~\n 2 honda        civic        2    2008     4 manu~ f        21    29 p     subc~\n 3 audi         a4 quattro   1.8  1999     4 auto~ 4        16    25 p     comp~\n 4 volkswagen   gti          2    1999     4 auto~ f        19    26 r     comp~\n 5 toyota       corolla      1.8  2008     4 auto~ f        26    35 r     comp~\n 6 volkswagen   gti          2    2008     4 manu~ f        21    29 p     comp~\n 7 dodge        dakota pi~   3.9  1999     6 auto~ 4        13    17 r     pick~\n 8 audi         a4 quattro   3.1  2008     6 manu~ 4        15    25 p     comp~\n 9 ford         mustang      4.6  2008     8 auto~ r        15    22 r     subc~\n10 honda        civic        1.8  2008     4 auto~ f        25    36 r     subc~\n# ... with 146 more rows\n```\n:::\n\n```{.r .cell-code}\ntest_set     <- training_test_sets[[\"test\"]]\ntest_set\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 78 x 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 ford         explorer ~   4.6  2008     8 auto~ 4        13    19 r     suv  \n 2 chevrolet    malibu       2.4  2008     4 auto~ f        22    30 r     mids~\n 3 hyundai      tiburon      2.7  2008     6 manu~ f        17    24 r     subc~\n 4 volkswagen   gti          2    2008     4 auto~ f        22    29 p     comp~\n 5 dodge        dakota pi~   4.7  2008     8 auto~ 4         9    12 e     pick~\n 6 volkswagen   jetta        2.8  1999     6 auto~ f        16    23 r     comp~\n 7 volkswagen   passat       1.8  1999     4 auto~ f        18    29 p     mids~\n 8 chevrolet    c1500 sub~   5.3  2008     8 auto~ r        14    20 r     suv  \n 9 chevrolet    corvette     6.2  2008     8 manu~ r        16    26 p     2sea~\n10 honda        civic        1.6  1999     4 auto~ f        24    32 r     subc~\n# ... with 68 more rows\n```\n:::\n:::\n\n## Step 2: fit the model to the training set\n\nWe now fit our regression model to the training set, as we have done many times.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = training_set)\n```\n:::\n\n## Step 3: Evaluate the prediction accuracy on the test set\n\nTo evaluate the prediction accuracy, we start by computing the predictions for the test set.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhwy_hat <- predict(m, test_set)\n```\n:::\n\n. . .\n\nA good model will make predictions that are closed to the true values of the response variable.\n\n. . .\n\nA common measure of prediction accuracy is the **root mean of squared errors** (RMSE): \n\n$$\nRMSE = \\sqrt{\\dfrac{SSE}{m}} = \\sqrt{\\dfrac{(y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\dots + (y_m - \\hat{y}_m)^2}{m}}\n$$\n\nwhere $m$ corresponds to the size of the test set.\n\n## Selecting a model\n\nApply steps 2 and 3 on different models.\n\n-   use the same training and test sets for the different models\n\nSimply choose the model with the lowest SSE.\n\nThis model has the best **out-of-sample** accuracy.\n\n## Cross-validation\n\nCross-validation (CV) is the same as the holdout method, but repeated many times.\n\n. . .\n\ndrawback of the holdout method is that the test set matters a lot.\n\nRepeating steps 2 and 3 with a different partition from step 1 may give different results.\n\n. . .\n\nIdea of CV: let each observation be in the test set once\n\n## $k$-fold CV\n\nChoose a number of folds $k$ (typically $5$ or $10$).\n\n. . .\n\n1.  Partition the data into $k$ folds of equal size.\n\n. . .\n\n2.  Let the test set be composed of fold $1$ and the training set of the other folds.\n\n. . .\n\n3.  Apply steps 2 and 3 of the holdout method.\n\n. . .\n\n4.  Go back to step 2, this time letting the next fold be the test set.\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(345)\n\nn_folds <- 10\n\ncounty_2019_nc_folds <- county_2019_nc %>%\n  slice_sample(n = nrow(county_2019_nc)) %>%\n  mutate(fold = rep(1:n_folds, n_folds)) %>%\n  arrange(fold)\n\npredict_folds <- function(i) {\n  fit <- lm(uninsured ~ hs_grad, data = county_2019_nc_folds %>% filter(fold != i))\n  predict(fit, newdata = county_2019_nc_folds %>% filter(fold == i)) %>%\n    bind_cols(county_2019_nc_folds %>% filter(fold == i), .fitted = .)\n}\n\nnc_fits <- map_df(1:n_folds, predict_folds)\n\np_nc_fits <- ggplot(nc_fits, aes(x = hs_grad, y = .fitted, group = fold)) +\n  geom_line(stat = \"smooth\", method = \"lm\", se = FALSE, size = 0.3, alpha = 0.5) +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Predicted uninsurance rate in NC\",\n    subtitle = glue(\"For {n_folds} different testing datasets\")\n    )\n\np_nc_fits\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\n\nn_folds <- 10\n\ncounty_2019_ny_folds <- county_2019_ny %>%\n  slice_sample(n = nrow(county_2019_ny)) %>%\n  mutate(fold = c(rep(1:n_folds, 6), 1, 2)) %>%\n  arrange(fold)\n\npredict_folds <- function(i) {\n  fit <- lm(uninsured ~ hs_grad, data = county_2019_ny_folds %>% filter(fold != i))\n  predict(fit, newdata = county_2019_ny_folds %>% filter(fold == i)) %>%\n    bind_cols(county_2019_ny_folds %>% filter(fold == i), .fitted = .)\n}\n\nny_fits <- map_df(1:n_folds, predict_folds)\n\np_ny_fits <- ggplot(ny_fits, aes(x = hs_grad, y = .fitted, group = fold)) +\n  geom_line(stat = \"smooth\", method = \"lm\", se = FALSE, size = 0.3, alpha = 0.5) +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Predicted uninsurance rate in NY\",\n    subtitle = glue(\"For {n_folds} different testing datasets\")\n    )\n\np_ny_fits\n```\n:::\n\n## Taking things to the extreme: LOOCV\n\nSet $k=n$; that is, we use $n$ folds, each of size $1$. The test sets will therefore consist of a single observation and the training sets of $n-1$ observations.\n\n# Model selection: stepwise procedures\n\n-   Not my favorite method,\n\n-   but this is something you should learn because it is widely used.\n\n. .\n\n-   Akin the \"throw cooked spaghetti to the wall and see what sticks\" technique.\n\n. . .\n\nStepwise selection procedures ar of two types: (i) forward and (ii) backward.\n\n## Forward selection procedure\n\n1.  Choose a criterion that balances model fit (smaller SSR) and parsimony (small $k$).\n\n```{=html}\n<!-- -->\n```\ni.  e.g. adjusted-$R^2$, **AIC** or BIC\n\n. . .\n\n2.  Start with the empty model $Y \\approx \\beta_0$, i.e. the model with no predictor. This is our **current model**\n\n3.  Fit all possible models with one additional predictor.\n\n. .\n\n4.  Compute the AIC of each of these models\n\n```{=html}\n<!-- -->\n```\ni.  Identify the model with the smallest AIC. This is our **candidate model**.\n\n. . .\n\nii. If the AIC of the candidate model is *smaller* (better) than the AIC of the current model, the candidate model becomes the current model, and we go back to step 3.\n\niii. If the AIC of the candidate model is *larger* than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model.\n\n## Backward selection procedure\n\nSimilar to forward selection, except that\n\n-   we start with the full model,\n-   remove one predictor at a time\n-   until removing any predictors makes the model worse.\n\n. . .\n\nNote that forward and backward selection need not agree; they may select different models.\n\n. . .\n\n-   What to do in that case? Nobody knows.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - stepwise selection\n\n-   Exercises 8.11, 8.13\n\n-   In addition\n\ni.  fit the first models in `R`\nii. identify the baseline level of the categorical variable in each model.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(openintro)\nd <- openintro::births14\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627acfa9\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# How to prevent overfiting?\n\n## Parsimony\n\nUse domain knowledge\n\none-in-ten rule (could be one-in-five)\n\n## Multicollinearity",
    "supporting": [
      "lec-6x_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}