{
  "hash": "02a3502ba8b7a321fbfdace13f9a13e5",
  "result": {
    "markdown": "---\ntitle: \"Simple Linear Regression Models\"\nsubtitle: \"STA 101 - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[sta101-suI22.github.io/website](https://sta101-suI22.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-link: true\n    self-contained: true\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   Assignments\n\n## Recap of last lecture\n\n::: incremental\n-   Histogram, scatterplot, boxplot\n-   Average, median, variance, sd and IQR; robustness\n-   Frequency, contigency and proportion tables\n-   Barplot, mosaic plot\n-   Effective communication: well-edited figures, $\\ge3$ variables (symbols, colors, facets)\n-   [R for Data Science - chapters 3 and 7](https://r4ds.had.co.nz/data-visualisation.html)\n:::\n\n## Outline\n\n-   simple linear regression model\n-   least-square estimates\n-   model comparison\n-   outliers\n\n# Simple linear regression\n\n## Regression model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nhead(d, n = 4)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n```\n:::\n:::\n\n. . .\n\nSuppose that the variable `hwy` (fuel efficiency on the highway) is very expensive to measure.\n\nWe decide to estimate it using the other variables. To do so, we will fit a **regression model**. $$\n\\text{hwy} \\approx \\text{model}(\\text{other variables})\n$$\n\n## Simple regression\n\nWe expect the variable `cty` to be a good proxy for `hwy`.\n\nAfter all, if a car is efficient in the city, we expect it to also be efficient on the highway! We will therefore consider a **simple regression** model\n\n$$\n\\text{hwy} \\approx \\text{model}(\\text{cty})\n$$\n\n## Simple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\nIt turns out that the variables `cty` and `hwy` are *linearly* associated.\n\nWe therefore decide to use a \\*\\*simple linear regression\\* model\n\n$$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n$$\n\n## Parameters\n\nThe numbers $\\beta_0$ and $\\beta_1$ are the two parameters of the model.\n\n. . .\n\nWe now want to find good values for these parameters.\n\n. . .\n\nLet us look at two sets of values:\n\n-   $\\beta_0 = 1$, $\\beta_1 = 1.3$ $$\n      \\text{hwy} \\approx 1 + 1.3 \\text{cty}\n      $$\n-   $\\beta_0 = 15$, $\\beta_1 = 0.5$ $$\n      \\text{hwy} \\approx 15 + 0.5\\text{cty}\n      $$\n\n## \n\n::: panel-tabset\n## Good model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 1, slope = 1.3)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Bad model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 15, slope = 0.5)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## Prediction\n\nWe can use our models to estimate `hwy` for new vehicles.\n\nImagine there is a new vehicle with $\\text{cty} = 30$. Instead of measuring its `hwy`, we decide to use our model to estimate it. Using the \"good\" model gives the following estimate \\\\begin{align\\*} \\text{hwy} & \\approx \\beta\\_0 + \\beta\\_1 \\text{cty} \\\\ & = 1 + 1.3 \\* 30\\\\ & = 40\\\\ \\\\end{align}\n\n## \n\nIs this the true `hwy` os the new vehicle? No! - this is only an estimate based on the value of the variable `cty` and our \"good\" model.\n\n. . .\n\nCan we do better? Yes! - Take additional variable into account in the model (e.g. engine size, vehicle age, etc) - Use better values for $\\beta_0$ and $\\beta_1$.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - parameters\n\n1.  What is the prediction for the new vehicle if we use the bad model?\n\n2.  copy andpaste the following piece of code and try different values for the parameters to find a good set of values.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nd <- ggplot2::mpg\nbeta_0 <- 15\nbeta_1 <- 0.5\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = beta_0, slope = beta_1)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627719b5\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Linear association\n\nA simple linear regression model is only applicable if the relation between the predictor and the response is **linear**.\n\n. . .\n\nIf the relation is not linear, the simple linear regression is not suitable.\n\nIn this case, we need to model the non-linearity (next lecture).\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - linear association\n\nExercise 7.3\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627719a5\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Leat-square estimates\n\n## Residuals\n\nOur predictions are only approximate.\n\n-   Let us represent our prediction with $\\widehat{\\text{hwy}}$ and the true value with $\\text{hw}$\n-   the error we make is $\\text{hw} - \\widehat{\\text{hwy}}$\n-   this is the **residual**\n\n$$e = \\text{hwy} - \\widehat{\\text{hwy}}$$\n\n## Visualizing residuals {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n::: nonincremental\n-   Black circles: Observed values (`y = hwy`)\n-   Pink solid line: Least-squares regression line\n-   Maroon triangles: Predicted values (`y = .fitted`)\n-   **Gray lines:** Residuals\n:::\n:::\n\n::: {.column width=\"75%\"}\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n:::\n:::\n\n## Residual plot\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3|4|7|8\"}\nd %>%\n  mutate(\n    hwy_hat = 1 + 1.3 * cty,\n    resid   = hwy - hwy_hat\n    ) %>%\n  ggplot() +\n  geom_point(aes(cty, resid)) +\n  geom_abline(intercept = 0, slope = 0, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - residuals\n\nExercises 7.1, 7.17, 7.19\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771bee\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Good estimates\n\n-   We want to choose estimates that give accurate prediction\n-   We want to minimize the residuals\n\n. . .\n\nPerhaps the most natural thing to do is to find the values for $\\beta_0$ and $\\beta_1$ that minimize the **sum of absolute residuals** $$|e_1|+|e_2|+\\dots+|e_n|$$ For practical reasons, the **sum of squared residuals** is more common criterion $$e_1^2+e_2^2+\\dots+e_n^2$$ \\## Why squaring the residuals?\n\n-   can work by hand (pre-computer)\n-   reflects the assumptions that being off by $4$ is more than twice as bad as being off by $2$\n-   nice mathematical properties\n-   mainstream\n\n## Least-square estimates\n\nWe find the values for $\\beta_0$ and $\\beta_1$ that minimize the SSR with the R function `lm`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ cty, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n```\n:::\n:::\n\nWe denote the least-square estimates with the symbols $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\nIn our model, we therefore have $$\\hat{\\beta}_0 = 0.892, \\qquad \\hat{\\beta}_1 = 1.337$$ \\##\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 0.892, slope = 1.337)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Reading `R` output\n\nWhen you fit a model, `R` can provide the results in different formats.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = d)\n```\n:::\n\n::: panel-tabset\n## Estimates only\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n```\n:::\n:::\n\nPrinting the model itself provides the least-square estimates. This is sufficient for now.\n\n## Raw `R` output\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,\tAdjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThis format contains information that will be useful when we do inference. It is, however, difficult to read.\n\n## Tidy `R` output\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m) %>%\n  tidy() %>%\n  kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.8920411 </td>\n   <td style=\"text-align:right;\"> 0.4689457 </td>\n   <td style=\"text-align:right;\"> 1.902227 </td>\n   <td style=\"text-align:right;\"> 0.05838 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cty </td>\n   <td style=\"text-align:right;\"> 1.3374556 </td>\n   <td style=\"text-align:right;\"> 0.0269732 </td>\n   <td style=\"text-align:right;\"> 49.584698 </td>\n   <td style=\"text-align:right;\"> 0.00000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\nThis format contains the information necessary for doing inference *and* is easy to read. This is the format used the textbook.\n:::\n\n## Exploring the model\n\nWe can access the residuals and prediction (fitted values) with the command `augment`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2\"}\nm <- lm(hwy ~ cty, data = d)\naugment(m)[,1:4]\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 234 x 4\n     hwy   cty .fitted .resid\n   <int> <int>   <dbl>  <dbl>\n 1    29    18    25.0 4.03  \n 2    29    21    29.0 0.0214\n 3    31    20    27.6 3.36  \n 4    30    21    29.0 1.02  \n 5    26    16    22.3 3.71  \n 6    26    18    25.0 1.03  \n 7    27    18    25.0 2.03  \n 8    26    18    25.0 1.03  \n 9    25    16    22.3 2.71  \n10    28    20    27.6 0.359 \n# ... with 224 more rows\n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - exploring a model\n\nFor the third observation,\n\n1.  What is the value of `cty`?\n2.  What is the predicted value for `hwy` based on the model?\n3.  What is the actual value for `hwy`?\n4.  What is its residual?\n5.  Did the model over- or under-predict?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771bfd\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Intepreting the parameters\n\n-   the intercept $\\hat{\\beta}_0$ is the estimate for a car with $\\text{cty} = 0$\n    -   meaningless in our case\n\n. . .\n\n-   the slope $\\hat{\\beta}_1$ is the increase in a our estimate for `hwy` for each additional unit of\n    -   For each additional unit of `cty`, we expect `hwy` to increase by 1.337.\n\n. . .\n\n-   extrapolation\n    -   stick to the range of the data\n    -   the intercept will not always be meaningful\n\n## Special case: categorical predictor\n\nLet us create a binary predictor indicating if the car is from $1999$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_binary <- mutate(d, year_binary = year == 1999)\nhead(select(d_binary, hwy, year, year_binary))\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 3\n    hwy  year year_binary\n  <int> <int> <lgl>      \n1    29  1999 TRUE       \n2    29  1999 TRUE       \n3    31  2008 FALSE      \n4    30  2008 FALSE      \n5    26  1999 TRUE       \n6    26  1999 TRUE       \n```\n:::\n:::\n\nThe variable `year_binary` takes the value $1$ (`TRUE`) if the car is from $1999$ and $0$ (`FALSE`) otherwise.\n\n##  {.smaller}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ year_binary, data = d_binary)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ year_binary, data = d_binary)\n\nCoefficients:\n    (Intercept)  year_binaryTRUE  \n       23.45299         -0.02564  \n```\n:::\n:::\n\nThe model equation is\n\n$$\n\\text{hwy} \\approx 23.46 - 0.026 \\text{year_binary}\n$$\n\nFor a new car from $1999$, the variable `year_binary` takes the value $1$ and our prediction is\n\n$$\n\\widehat{\\text{hwy}} = 23.46 - 0.026*1 = 23.434\n$$\n\nWhile for a car that not from $1999$, the variable `year_binary` takes the value $1$ and our prediction is\n\n$$\n\\widehat{\\text{hwy}} = 23.46 - 0.026*0 = 23.46\n$$\n\n# Model comparison\n\n## Alternative model\n\nLet us fit an alternative model using engine size (`disp`) as a predictor $$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{displ}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(displ, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\nThe least-square estimates for the coefficients are\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ displ, data = d)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n```\n:::\n:::\n\nNote that the slope coefficient is negative; which makes sense since cars with larger engines would tend to be less efficient.\n\n. . .\n\nWe now have two models. Which is the best?\n\n. . .\n\n-   We could start by looking at the residuals\n\n## Comparing residuals\n\n::: columns\n::: {.column width=\"50%\"}\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = d)\nm_augment <- augment(m)\nggplot(m_augment) +\n  geom_histogram(aes(.resid))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d) %>%\n  augment %>%\n  ggplot() +\n  geom_histogram(aes(.resid))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n:::\n\n## \n\nThe first model seems to have residuals of smaller magnitude.\n\n. . .\n\nBut we need a more systematic approach for comparing models:\n\n-   looking at a plot can be misleading (illusions)\n\n-   difficult to compare models with similar residuals\n\n## SSR\n\nInstead of comparing histograms to identify the model with the smaller residuals, we can compute the **SSR** (sum of squared residuals!)\n\n-   small residuals will give a small SSR\n\n-   large residuals will give a large SSR\n\n$$\nSSR = r_1^2+r_2^2+\\dots+r^2_n\n$$\n\n. . .\n\nSimply choose the model with the smaller SSR!\n\n. . .\n\nðŸ“‹ The textbook uses the term **SSE** (sum of squared errors).\n\n## $R^2$ {.smaller}\n\nThe SSR is also useful in describing the strength of the model.\n\n. . .\n\nThe **SST** (total sum of squares) is the sum of squared distance to the mean. $$\nSST = (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2\n$$ It measured the total amount of variability in the data.\n\n. . .\n\nRemember the formula for SSR $$\nSSR = (x_1 - \\hat{x})^2 + (x_2 - \\hat{x})^2 + \\dots + (x_n - \\hat{x})^2\n$$ is the amount of variability in the data left unexplained by the model.\n\n. . .\n\nBy analogy, $SST - SSR$ is the amount of variation explained by the model: $$\n\\text{data} = SST = SSR + (SST-SSR) = \\text{residuals} + \\text{model}\n$$\n\n## \n\nThe coefficient of determination $R^2$ measures the proportion of variation in the data that is explained by the model\n\n$$\nR^2 = 1-\\dfrac{SSR}{SST} = \\dfrac{SST-SSR}{SST}\n$$\n\n## \n\nNote that $0\\le R^2 \\le 1$.\n\n-   A good model will have small residuals, a small SSR and therefore a large \\$R\\^2\n-   A great model will have a $R^2$ close to $1$.\n-   A poor model has an $R^2$ close to 0 (SSR close to SST)\n\n## Computing $R^2$ in `R`\n\nTo compute $R^2$ in `R`, simply use the command `glance`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_glance <- glance(m)\nm_glance\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.914         0.913  1.75     2459. 1.87e-125     1  -462.  931.  941.\n# ... with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nm_glance$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9137752\n```\n:::\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d) %>% glance %>% .[[\"r.squared\"]]\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.5867867\n```\n:::\n:::\n\nThe model with `cty` as a predictor has a $R^2$ value of $0.914$, and the model that uses `displ` has a $R^2$ of $0.59$ (worse).\n\n. . .\n\nWe would therefore prefer the first model.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - Interpretation\n\n-   Exercise 7.23 (a-d)\n-   Compute $R^2$ \"by hand\" for the model with parameters $(\\beta_0, \\beta_1) = (1, 1.3)$. You can use the following `R` code.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nd <- ggplot2::mpg\ncty <- d$cty\nhwy <- d$hwy\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771a07\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Outliers\n\nRemember, in a boxplot, outliers are observation far from the bulk of the data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_boxplot(aes(hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=80%}\n:::\n:::\n\n. . .\n\nIn the context of regression model, an **outlier** is: an observation that falls far from the cloud of points\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - outlier\n\nLook at the scatterplot for our data. Are there outliers?\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(cty, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=80%}\n:::\n:::\n\nExercise 7.25\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_62771c9c\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Outliers, leverage and influential points\n\n-   **outlier**: observation that falls far from the cloud of points\n\n-   **High leverage point**: observation that falls horizontally away from the cloud of points\n\n-   **influential point**: observation that influences the slope of the regression line\n\n. . .\n\n-   All influential points are high leverage points.\n\n-   All leverage points are outliers.\n\n-   (Venn Diagram)\n\n## Dealing with outliers\n\nIn regression, outliers have the potential to highly influence the least-square estimates - the least-square estimates are not *robust* to the presence of outliers\n\n## Impact of outliers\n\nLet us contaminate the data with an outlier (`cty` $=10$ and `hwy` $=1000$) and fit the same regression model.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_contaminated <- select(d, cty, hwy) %>%\n  add_row(cty = 40, hwy = 400) # add outlier\nlm(hwy ~ cty, data = d_contaminated)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d_contaminated)\n\nCoefficients:\n(Intercept)          cty  \n    -26.046        3.013  \n```\n:::\n:::\n\n. . .\n\nThe value of the slope has tripled!\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d_contaminated) +\n  geom_point(aes(cty, hwy)) +\n  geom_abline(intercept = -26.046, slope = 3.013)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-32-1.png){fig-align='center' width=80%}\n:::\n:::\n\nThe regression line not longer fits the data well.\n\n# Recap\n\n## Recap\n\n::: incremental\n-   simple linear regression model $$\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n    $$\n-   residuals\n-   least-square estimates\n-   parameter interpretation\n-   model comparison with \\$R\\^2%\n-   outliers\n:::",
    "supporting": [
      "lec-4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}