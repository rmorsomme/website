{
  "hash": "9c2221ccf212bbf6957f2f1f5f03302f",
  "result": {
    "markdown": "---\ntitle: \"Simple Linear Regression Models\"\nsubtitle: \"STA 101 - Summer I 2022\"\nauthor: \"Raphael Morsomme\"\nfooter:  \"[sta101-suI22.github.io/website](https://sta101-suI22.github.io/website/)\"\nlogo: \"images/logo.jpg\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-link: true\n    code-copy: true\n    self-contained: true\n    scrollable: true\neditor: visual\nexecute:\n  freeze: auto\n  echo: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Welcome\n\n## Announcements\n\n-   Homework 1 has been returned\n-   Homework 2 has been published\n    -   Feel free to re-use the template from HW 1\n\n## Recap of last lecture\n\n::: incremental\n-   Histogram, scatterplot, boxplot\n-   Average, median, variance, sd and IQR; robustness\n-   Frequency, contigency and proportion tables\n-   Barplot, mosaic plot\n-   Effective communication: well-edited figures, $\\ge3$ variables (symbols, colors, facets)\n-   [R for Data Science - chapters 3 and 7](https://r4ds.had.co.nz/data-visualisation.html)\n:::\n\n## Outline\n\n-   simple linear regression model\n-   least-square estimates\n-   model comparison\n-   outliers\n\n# Simple linear regression\n\n## Regression model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd <- ggplot2::mpg\nhead(d, n = 4)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 4 x 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa~\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa~\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa~\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa~\n```\n:::\n:::\n\n. . .\n\nSuppose the variable `hwy` (fuel efficiency on highway) is very expensive to measure.\n\nWe decide to estimate it using the other variables. To do so, we will fit a **regression model**.\n\n$$\n\\text{hwy} \\approx \\text{model}(\\text{other variables})\n$$\n\n## Simple regression\n\nWe expect the variable `cty` to be a good proxy for `hwy`.\n\nAfter all, if a car is efficient in the city, we expect it to also be efficient on the highway! We will therefore consider a **simple regression** model\n\n$$\n\\text{hwy} \\approx \\text{model}(\\text{cty})\n$$\n\n## Simple linear regression\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\nThe variables `cty` and `hwy` are *linearly* associated.\n\nWe therefore opt for a **simple linear regression** model\n\n$$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n$$\n\n## Parameters\n\nThe numbers $\\beta_0$ and $\\beta_1$ are the two parameters of the model.\n\n. . .\n\nWe now want to find good values for these unknown parameters.\n\n. . .\n\nLet us look at two sets of values:\n\n-   $\\beta_0 = 1, \\beta_1 = 1.3 \\qquad \\Rightarrow \\qquad \\text{hwy} \\approx 1 + 1.3 \\text{cty}$\n\n-   $\\beta_0 = 15, \\beta_1 = 0.5 \\qquad \\Rightarrow \\qquad \\text{hwy} \\approx 15 + 0.5 \\text{cty}$\n\n## \n\n::: panel-tabset\n## Good model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 1, slope = 1.3)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Bad model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3\"}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 15, slope = 0.5)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n## Prediction\n\nWe can use our models to estimate `hwy` for new vehicles.\n\n. . .\n\nImagine there is a new vehicle with $\\text{cty} = 30$. Instead of measuring its `hwy` (expensive), we use our model to estimate it. Using the \"good\" model gives the following estimate\n\n$$\n\\begin{align*}\n\\text{hwy} \n& \\approx \\beta_0 + \\beta_1 \\text{cty} \\\\ \n& = 1 + 1.3 * 30\\\\\n& = 40\n\\end{align*}\n$$\n\n## \n\nIs this the true `hwy` of the new vehicle? No!\n\n-   this is only an estimate based on the value of the variable `cty` and our \"good\" model.\n\n. . .\n\nCan we do better? Yes!\n\n-   Take additional variable into account in the model (e.g. engine size, vehicle age, etc)\n-   Use better values for $\\beta_0$ and $\\beta_1$.\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - parameters\n\n1.  What is the prediction for the new vehicle if we use the bad model?\n\n2.  copy and paste the following piece of code and try different values for the parameters to find a good set of values.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nd <- ggplot2::mpg\nbeta_0 <- 15\nbeta_1 <- 0.5\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = beta_0, slope = beta_1)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa51f\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Linear association\n\nA simple linear regression model is only applicable if the relation between the predictor and the response is **linear**.\n\n. . .\n\nIf the relation is not linear, the simple linear regression is not suitable.\n\nIn this case, we need to *model* the non-linearity (next lecture).\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - linear association\n\nExercise [7.3](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa417\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Leat-square estimates\n\n## Residuals\n\nOur predictions are only approximate.\n\n-   Let us represent our prediction with $\\widehat{\\text{hwy}}$ and the true value with $\\text{hwy}$\n-   the error we make is $\\text{hw} - \\widehat{\\text{hwy}}$\n-   this is called the **residual**\n\n$$e = \\text{hwy} - \\widehat{\\text{hwy}}$$\n\n## Visualizing residuals {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n::: nonincremental\n-   Black circles: Observed values ($\\text{hwy}$)\n-   Pink solid line: Least-squares regression line\n-   Maroon triangles: Predicted values ($\\widehat{\\text{hwy}}$)\n-   **Gray lines:** Residuals\n:::\n:::\n\n::: {.column width=\"75%\"}\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n:::\n:::\n\n## Residual plot\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"3|4|7|8\"}\nd %>%\n  mutate(\n    hwy_hat = 1 + 1.3 * cty,\n    resid   = hwy - hwy_hat\n    ) %>%\n  ggplot() +\n  geom_point(aes(cty, resid)) +\n  geom_abline(intercept = 0, slope = 0, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - residuals\n\nExercises [7.1](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises), [7.17](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises), [7.19](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa40c\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">05</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Good estimates\n\nWe want to choose estimates that give a model that *fits the data well*.\n\n-   a model with a regression line that is close to the data\n\n. . .\n\nWe want to minimize the residuals.\n\n## Minimizing residuals\n\nPerhaps the most natural thing to do is to find the values for $\\beta_0$ and $\\beta_1$ that minimize the **sum of absolute residuals** $$\n|e_1|+|e_2|+\\dots+|e_n|\n$$\n\n. . .\n\nFor practical reasons, the **sum of squared residuals** (SSR) is a more common criterion $$\ne_1^2+e_2^2+\\dots+e_n^2\n$$\n\n## Why squaring the residuals?\n\n-   can work by hand (pre-computer era)\n-   reflects the assumptions that being off by $4$ is more than twice as bad as being off by $2$\n-   nice mathematical properties\n-   mainstream\n\n## Least-square estimates\n\nWe find the values for $\\beta_0$ and $\\beta_1$ that minimize the SSR with the R command `lm`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ cty, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n```\n:::\n:::\n\nWe denote the least-square estimates with the symbols $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\nIn our model, we therefore have $$\\hat{\\beta}_0 = 0.892, \\qquad \\hat{\\beta}_1 = 1.337$$\n\n## Visualizing the least square regression line\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(x = cty, y = hwy)) +\n  geom_abline(intercept = 0.892, slope = 1.337)\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## Reading `R` output\n\n`R` can provide the results of a model in different formats.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = d)\n```\n:::\n\n::: panel-tabset\n## Estimates only\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n```\n:::\n:::\n\nPrinting the model itself provides the least-square estimates. This is sufficient for now.\n\n## Raw `R` output\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3408 -1.2790  0.0214  1.0338  4.0461 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.89204    0.46895   1.902   0.0584 .  \ncty          1.33746    0.02697  49.585   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.752 on 232 degrees of freedom\nMultiple R-squared:  0.9138,\tAdjusted R-squared:  0.9134 \nF-statistic:  2459 on 1 and 232 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThis format contains information that will be useful when we do inference. It is, however, difficult to read.\n\n## Tidy `R` output\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m) %>%\n  tidy() %>%\n  kable(digits = 2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n   <td style=\"text-align:right;\"> 0.47 </td>\n   <td style=\"text-align:right;\"> 1.90 </td>\n   <td style=\"text-align:right;\"> 0.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> cty </td>\n   <td style=\"text-align:right;\"> 1.34 </td>\n   <td style=\"text-align:right;\"> 0.03 </td>\n   <td style=\"text-align:right;\"> 49.58 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\nThis format contains the information necessary for doing inference *and* is easy to read. This is the format used the textbook.\n:::\n\n## Exploring the model\n\nWe can access the residuals and predictions (fitted values) with the command `augment`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2\"}\nm <- lm(hwy ~ cty, data = d)\naugment(m)[,1:4]\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 234 x 4\n     hwy   cty .fitted .resid\n   <int> <int>   <dbl>  <dbl>\n 1    29    18    25.0 4.03  \n 2    29    21    29.0 0.0214\n 3    31    20    27.6 3.36  \n 4    30    21    29.0 1.02  \n 5    26    16    22.3 3.71  \n 6    26    18    25.0 1.03  \n 7    27    18    25.0 2.03  \n 8    26    18    25.0 1.03  \n 9    25    16    22.3 2.71  \n10    28    20    27.6 0.359 \n# ... with 224 more rows\n```\n:::\n:::\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - exploring a model\n\nFor the third observation of the sample,\n\n1.  what is the value of `cty`?\n2.  what is the predicted value for `hwy` based on the model?\n3.  what is the actual value for `hwy`?\n4.  what is the residual?\n5.  did the model over- or under-predict?\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa4d3\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">04</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Intepreting the parameters {.smaller}\n\n-   the intercept estimate $\\hat{\\beta}_0$ is the prediction for a car with $\\text{cty} = 0$\n    -   meaningless in our case\n\n. . .\n\n-   the slope estimate $\\hat{\\beta}_1$ is the increase in a our prediction for `hwy` for each additional unit of `cty`\n    -   \"for each additional unit of `cty`, we expect `hwy` to increase by 1.337\"\n\n. . .\n\n::: callout-tip\n## Extrapolation\n\n-   stick to the range of the data\n-   the intercept will not always be meaningful\n:::\n\n## Special case: categorical predictor\n\nLet us create a binary predictor indicating if the car is from $1999$.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_binary <- mutate(d, year_binary = if_else(year == 1999, 1, 0))\nhead(select(d_binary, hwy, year, year_binary))\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 x 3\n    hwy  year year_binary\n  <int> <int>       <dbl>\n1    29  1999           1\n2    29  1999           1\n3    31  2008           0\n4    30  2008           0\n5    26  1999           1\n6    26  1999           1\n```\n:::\n:::\n\nThe variable `year_binary` takes the value $1$ if the car is from $1999$ and $0$ otherwise.\n\n##  {.smaller}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ year_binary, data = d_binary)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ year_binary, data = d_binary)\n\nCoefficients:\n(Intercept)  year_binary  \n   23.45299     -0.02564  \n```\n:::\n:::\n\nThe model equation is\n\n$$\n\\text{hwy} \\approx 23.46 - 0.026 \\text{year_binary}\n$$\n\nFor a new car from $1999$, the variable `year_binary` takes the value $1$ and our prediction is\n\n$$\n\\widehat{\\text{hwy}} = 23.46 - 0.026*1 = 23.46 - 0.026 = 23.434\n$$\n\nWhile for a car that not from $1999$, the variable `year_binary` takes the value $1$ and our prediction is\n\n$$\n\\widehat{\\text{hwy}} = 23.46 - 0.026*0 = 23.46 - 0 = 23.46\n$$\n\n# Model comparison\n\n## Alternative model\n\nLet us fit an alternative model using engine size (`disp`) as a predictor $$\n\\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{displ}\n$$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(displ, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=80%}\n:::\n:::\n\n## \n\nThe least-square estimates for the coefficients are\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ displ, data = d)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n```\n:::\n:::\n\nNote that the slope coefficient is negative; which makes sense since cars with larger engines would tend to be less efficient.\n\n## \n\nWe now have two models. Which is the best?\n\n. . .\n\n-   We could start by looking at the residuals\n\n## Comparing residuals\n\n::: columns\n::: {.column width=\"50%\"}\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm <- lm(hwy ~ cty, data = d)\nm_augment <- augment(m)\nggplot(m_augment) +\n  geom_histogram(aes(.resid))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d) %>%\n  augment %>%\n  ggplot() +\n  geom_histogram(aes(.resid))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=80%}\n:::\n:::\n:::\n:::\n\n## \n\nThe first model seems to have smaller residuals.\n\n$\\Rightarrow$ choose the first model (`cty`)!\n\n. . .\n\nBut looking at a plot can be misleading\n\n-   illusions\n\n-   difficult to compare models with similar residuals\n\n. . .\n\nWe need a more *systematic* approach for comparing models.\n\n## SSR\n\nInstead of comparing histograms of residuals, we can compute the **SSR** (sum of squared residuals!)\n\n$$\nSSR = r_1^2+r_2^2+\\dots+r^2_n\n$$\n\n-   small residuals will give a small SSR\n\n-   large residuals will give a large SSR\n\n. . .\n\n$\\Rightarrow$ choose the model with the smaller SSR!\n\n. . .\n\nðŸ“‹ The textbook uses the term **SSE** (sum of squared errors).\n\n## $R^2$ {.smaller}\n\nWhile the SSR can be used to select a model, it is also useful in describing the goodness of fit of the model.\n\n. . .\n\nThe **SST** (total sum of squares) is the sum of squared distance to the mean. $$\nSST = (x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2\n$$ It measures the total amount of variability in the data.\n\n. . .\n\nRemember the formula for **SSR** $$\nSSR = (x_1 - \\hat{x})^2 + (x_2 - \\hat{x}_1)^2 + \\dots + (x_n - \\hat{x}_n)^2\n$$ It measures the amount of variability in the data left unexplained by the model.\n\n. . .\n\n$SST - SSR$ is therefore the amount of variation explained by the model: $$\n\\text{data} = SST = (SST-SSR) + SSR = \\text{model} + \\text{residuals}\n$$\n\n## \n\nThe statistic $R^2$ measures the proportion of variation in the data that is explained by the model.\n\n$$\nR^2 = 1-\\dfrac{SSR}{SST} = \\dfrac{SST-SSR}{SST} = \\dfrac{\\text{var. explained by model}}{\\text{total var.}}\n$$\n\n## \n\nNote that $0\\le R^2 \\le 1$.\n\n-   good model $\\Rightarrow$ small residuals $\\Rightarrow$ small SSR $\\Rightarrow$ large $R^2$.\n-   great model $\\Rightarrow$ tiny residuals $\\Rightarrow$ tiny SSR $\\Rightarrow$ $R^2$ close to 1.\n-   poor model $\\Rightarrow$ large residuals $\\Rightarrow$ SSR almost as large as SST $\\Rightarrow$ $R^2$ close to 0.\n\n## Computing $R^2$ in `R`\n\nTo compute $R^2$ in `R`, simply use the command `glance`.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm_glance <- glance(m)\nm_glance$r.squared\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.9137752\n```\n:::\n\n```{.r .cell-code}\nlm(hwy ~ displ, data = d) %>% glance %>% .[[\"r.squared\"]]\n```\n\n::: {.cell-output-stdout}\n```\n[1] 0.5867867\n```\n:::\n:::\n\nThe model with `cty` as a predictor has a $R^2$ value of $0.914$, and the model that uses `displ` has a $R^2$ of $0.59$ (worse).\n\n. . .\n\nThe first model is better!\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - Interpretation\n\n-   Exercise [7.23 (a-d)](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)\n-   Compute $R^2$ \"by hand\" in `R` (do not use `glance`) for the model with parameters $(\\beta_0, \\beta_1) = (1, 1.3)$. You can use the following code.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nd <- ggplot2::mpg\ncty <- d$cty\nhwy <- d$hwy\nbeta_0 <- 1\nbeta_1 <- 1.3\nhwy_hat <- . . .\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa55e\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n# Outliers\n\nRemember, in a boxplot, outliers are observations far from the bulk of the data\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_boxplot(aes(hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=80%}\n:::\n:::\n\n. . .\n\nIn the context of regression model, an **outlier** is: an observation that falls far from the cloud of points\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - outlier\n\nLook at the scatterplot for the `mpg` data set. Are there outliers?\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(cty, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=80%}\n:::\n:::\n\nExercises [7.25](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa34f\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Outliers, leverage and influential points\n\n-   **outliers**: observations that fall far from the cloud of points\n\n-   **High leverage points**: observations that fall horizontally away from the cloud of points\n\n-   **influential points**: observations that influence the slope of the regression line\n\n. . .\n\n-   All influential points are high leverage points.\n\n-   All leverage points are outliers.\n\n-   (Venn Diagram)\n\n## \n\n::: {.callout-tip icon=\"false\"}\n## Group exercise - outlier\n\nLook at the scatterplot for the `mpg` data set. Are there outliers?\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d) +\n  geom_point(aes(cty, hwy))\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n:::\n\nExercises [7.27](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div class=\"countdown\" id=\"timer_627fa596\" style=\"right:0;bottom:0;\" data-warnwhen=\"0\">\n<code class=\"countdown-time\"><span class=\"countdown-digits minutes\">03</span><span class=\"countdown-digits colon\">:</span><span class=\"countdown-digits seconds\">00</span></code>\n</div>\n```\n:::\n:::\n\n## Dealing with outliers\n\nIn regression, outliers have the potential to highly influence the least-square estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$.\n\n::: callout-warning\nLeast-square estimates are not *robust* to the presence of outliers.\n:::\n\n. . .\n\nEstimates that are robust include\n\n-   least absolute deviation estimates (minimize sum of *absolute* residuals instead of SSR)\n-   Bayesian estimates\n\n## Impact of outliers\n\nLet us contaminate the data with an outlier (`cty` $=10$ and `hwy` $=1000$)\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nd_contaminated <- select(d, cty, hwy) %>%\n  add_row(cty = 40, hwy = 500) # add outlier\narrange(d_contaminated, desc(hwy)) %>% slice(1:5)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 5 x 2\n    cty   hwy\n  <dbl> <dbl>\n1    40   500\n2    33    44\n3    35    44\n4    29    41\n5    28    37\n```\n:::\n:::\n\n. . .\n\n...and compare the two regression models.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(hwy ~ cty, data = d)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d)\n\nCoefficients:\n(Intercept)          cty  \n      0.892        1.337  \n```\n:::\n\n```{.r .cell-code}\nlm(hwy ~ cty, data = d_contaminated)\n```\n\n::: {.cell-output-stdout}\n```\n\nCall:\nlm(formula = hwy ~ cty, data = d_contaminated)\n\nCoefficients:\n(Intercept)          cty  \n    -33.841        3.498  \n```\n:::\n:::\n\n. . .\n\nThe value of the slope has almost tripled!\n\n## \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(d_contaminated) +\n  geom_point(aes(cty, hwy)) +\n  geom_abline(intercept = -26.046, slope = 3.013, col = \"purple\")\n```\n\n::: {.cell-output-display}\n![](lec-4_files/figure-revealjs/unnamed-chunk-35-1.png){fig-align='center' width=80%}\n:::\n:::\n\nThe regression line not longer fits the data well.\n\n# Recap\n\n## Recap\n\n::: incremental\n-   simple linear regression model $$\n    \\text{hwy} \\approx \\beta_0 + \\beta_1 \\text{cty}\n    $$\n-   residuals\n-   least-square estimates\n-   parameter interpretation\n-   model comparison with $R^2$\n-   outliers\n:::",
    "supporting": [
      "lec-4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"..\\site_libs/countdown-0.3.5/countdown.css\" rel=\"stylesheet\" />\r\n<script src=\"..\\site_libs/countdown-0.3.5/countdown.js\"></script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}