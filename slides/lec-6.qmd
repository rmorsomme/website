---
title: "Model Selection"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    code-link: true
    self-contained: true
    scrollable: true
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   assignments

## Recap

# Competing Models

-   Earlier, we saw that adding the predictor $\dfrac{1}{\text{displ}}$ gave a better fit.

-   Let us see if the same idea work with the `trees` dataset.

## a nonlinear association?

Suppose we want to predict volume using only the variable `girth`.

```{r}
d_tree <- datasets::trees
ggplot(d_tree) +
  geom_point(aes(Girth, Volume))
```

One could argue that there is a slight nonlinear association

## `R` function to compute $R^2$

```{r}
compute_R2 <- function(model){
  
  model_glanced <- glance(model)
  R2 <- model_glanced$r.squared
  R2_rounded <- round(R2, 3) 
  
  return(R2_rounded)
  
}
```

## Starting simple...

We start with the simple model

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth}
$$

```{r}
m1 <- lm(Volume ~ Girth, data = d_tree)
compute_R2(m1)
```

## ...taking it up a notch...

To capture the nonlinear association between `Girth` and `Volume`, we consider the predictor $\text{Girth}^2$.

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth} + \beta_2 \text{girth}^2
$$

The `R` to fit this model is

```{r}
d_tree2 <- mutate(d_tree, Girth2 = Girth^2)
m2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)
compute_R2(m2)
```

. . .

$R^2$ has increased! It went from `r compute_R2(m1)` to `r compute_R2(m2)`.

## ...before taking things to the extreme

What if we also include the predictors $\text{Girth}^3, \text{Girth}^4, \dots, \text{Girth}^{k}$ for some larger number $k$?

The following `R` code fits such a model with $k=29$, that is, $$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth} + \beta_2 \text{girth}^2 + \dots + \beta_{29} \text{girth}^{29}
$$

```{r}
m_extreme <- lm(Volume ~ poly(Girth, degree = 30, raw = TRUE), data = d_tree)
compute_R2(m_extreme)
```

$R^2$ has again increased! It went from `r compute_R2(m2)` to `r compute_R2(m_extreme)`.

## Overfitting

In fact, if we keep adding predictors, $R^2$ will always increase. - additional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.

. . .

Is the `m_extreme` model a good model? - does it accurately represent the *population*? - remember that we want to learn about the relation between `Volume` and `Girth` present in the *population* (not the sample).

## 

::: panel-tabset
## `R` code

```{r}
#| code-line-numbers: "1|2|3|4|"
Girth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.01)
new_data <- tibble(Girth = Girth_new)
Volume_pred <- predict(m_extreme, new_data)
new_data <- mutate(new_data, Volume_pred = Volume_pred)
head(new_data)
```

## Regression line

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred))
```

## Regression line (zoomed)

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred)) +
  ylim(10, 77)
```
:::

## 

The model `m_extreme` **overfits** the data.

A model overfits data when it corresponds very closely to the data set and does a poor job for new data.

. . .

Remember that we want to learn about the *population*, not the *sample*!

# Model selection: criteria

## Adjusted-$R^2$

We saw that $R^2$ keeps increasing as we add predictors.

$$
R^2_{\text{adj}} = 1 - \dfrac{SSR}{SST}
$$

$R^2$ can therefore not be used to identify models that overfit the data.

. . .

Instead, we use the **adjusted-**$R^2$.

$$
R^2_{\text{adj}} = 1 - \dfrac{SSR}{SST}\dfrac{n-1}{n-p-1}
$$

where $p$ corresponds to the number of predictors.

## 

The ratio $\dfrac{SSR}{SST}$ favors model that closely fit the data.

The ratio $\dfrac{n-1}{n-k-1}$ penalizes model with many predictors.

. . .

The model with the highest adjusted-$R^2$ typically hits the sweet spot.

## AIC and BIC

Two popular criteria that balance goodness of fit (small SSR) and parsimony (small $p$) are

-   the Akaike Information Criterion (AIC)
-   the Bayesian Inofrmation Criterion (BIC)

. . .

The formula for AIC and BIC are respectively $$
AIC = 2p - \text{GoF}, \qquad BIC = \ln(n)p- \text{GoF}
$$ where $\text{GoF}$ is a measure of the *Goodness of fit* of the model[^1].

[^1]: The exact formula for $\text{GoF}$ is beyond the scope of this class.

. . .

Unlike the adjusted-$R^2$, smaller is better for the AIC and BIC.

. . .

Note that the BIC penalizes the number of parameters $p$ more strongly.

## Computing AIC and BIC

Easily accessible in `R` with the command `glance`. For instance,

```{r}
glance(m1)
glance(m2)
glance(m_extreme)
```

. . .

In this case, all three criteria (ajudtsed-$R^2$, AIC and BIC) indicate that `m2` id the best model.

-   For AIC and BIC, smaller is better.

# Model selection: predictive performance

## Limitations of the previous methods

The adjutsed-$R^2$, AIC and BIC all try to balance

1.  goodness of fit
2.  parsimony

. . .

They achieve this balance by favoring models with small SSR while penalizing models with larger $p$.

. . .

The form of the penalty for $p$ may seem somewhat arbitrary. - E.g. AIC versus BIC.

## Predictive performance

Instead, we could look for the model with the best **predictions performance**; that is, the model that makes predictions for new observations that are the closest to the true values.

## The holdout method

The holdout method is a simple method to evaluate the predictive performance of a model.

. . .

1.  Randomly partition your sample into two sets: a **training set** (typically 2/3 of the sample) and a **test set**.

. . .

2.  Fit your model to the training set.

. . .

3.  Evaluate the prediction accuracy on the test set.

. . .

Note that the test set consists of *new* observations for the model.

. . .

A good model will model will have good prediction accuracy in step 3

## Step 1: training and test sets

The following `R` function splits a sample into a training and a test set

```{r}
construct_training_test <- function(sample, prop = 2/3){
  
  n          <- nrow(sample)
  n_training <- round(n*prop)
  
  sample_random   <- slice_sample(sample, n = n)
  sample_training <- slice(sample_random,   1:n_training )
  sample_test     <- slice(sample_random, -(1:n_training))
  
  return(list(
    training = sample_training, test = sample_test
    ))
  
}
```

## 

```{r}
d_car <- ggplot2::mpg
training_test_sets <- construct_training_test(d_car) 
training_set <- training_test_sets[["training"]]
training_set
test_set     <- training_test_sets[["test"]]
test_set
```

## Step 2: fit the model to the training set

We now fit our regression model to the training set, as we have done many times.

```{r}
m <- lm(hwy ~ cty, data = training_set)
```

## Step 3: Evaluate the prediction accuracy on the test set

To evaluate the prediction accuracy, we start by computing the predictions for the test set.

```{r}
hwy_hat <- predict(m, test_set)
```

. . .

A good model will make predictions that are closed to the true values of the response variable.

. . .

A common measure of prediction accuracy is the **root mean of squared errors** (RMSE):

$$
RMSE = \sqrt{\dfrac{SSE}{m}} = \sqrt{\dfrac{(y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_m - \hat{y}_m)^2}{m}}
$$

where $m$ corresponds to the size of the test set.

## Selecting a model

Apply steps 2 and 3 on different models.

-   use the same training and test sets for the different models

Simply choose the model with the lowest SSE.

This model has the best **out-of-sample** accuracy.

## Cross-validation

Cross-validation (CV) is the same as the holdout method, but repeated many times.

. . .

drawback of the holdout method is that the test set matters a lot.

Repeating steps 2 and 3 with a different partition from step 1 may give different results.

. . .

Idea of CV: let each observation be in the test set once

## $k$-fold CV

Choose a number of folds $k$ (typically $5$ or $10$).

. . .

1.  Partition the data into $k$ folds of equal size.

. . .

2.  Let the test set be composed of fold $1$ and the training set of the other folds.

. . .

3.  Apply steps 2 and 3 of the holdout method.

. . .

4.  Go back to step 2, this time letting the next fold be the test set.

## 

```{r, eval = FALSE}
set.seed(345)

n_folds <- 10

county_2019_nc_folds <- county_2019_nc %>%
  slice_sample(n = nrow(county_2019_nc)) %>%
  mutate(fold = rep(1:n_folds, n_folds)) %>%
  arrange(fold)

predict_folds <- function(i) {
  fit <- lm(uninsured ~ hs_grad, data = county_2019_nc_folds %>% filter(fold != i))
  predict(fit, newdata = county_2019_nc_folds %>% filter(fold == i)) %>%
    bind_cols(county_2019_nc_folds %>% filter(fold == i), .fitted = .)
}

nc_fits <- map_df(1:n_folds, predict_folds)

p_nc_fits <- ggplot(nc_fits, aes(x = hs_grad, y = .fitted, group = fold)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, size = 0.3, alpha = 0.5) +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  labs(
    x = "High school graduate", y = "Uninsured",
    title = "Predicted uninsurance rate in NC",
    subtitle = glue("For {n_folds} different testing datasets")
    )

p_nc_fits
```

```{r, eval = FALSE}
#| out.width: "100%"

set.seed(123)

n_folds <- 10

county_2019_ny_folds <- county_2019_ny %>%
  slice_sample(n = nrow(county_2019_ny)) %>%
  mutate(fold = c(rep(1:n_folds, 6), 1, 2)) %>%
  arrange(fold)

predict_folds <- function(i) {
  fit <- lm(uninsured ~ hs_grad, data = county_2019_ny_folds %>% filter(fold != i))
  predict(fit, newdata = county_2019_ny_folds %>% filter(fold == i)) %>%
    bind_cols(county_2019_ny_folds %>% filter(fold == i), .fitted = .)
}

ny_fits <- map_df(1:n_folds, predict_folds)

p_ny_fits <- ggplot(ny_fits, aes(x = hs_grad, y = .fitted, group = fold)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, size = 0.3, alpha = 0.5) +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  labs(
    x = "High school graduate", y = "Uninsured",
    title = "Predicted uninsurance rate in NY",
    subtitle = glue("For {n_folds} different testing datasets")
    )

p_ny_fits
```

## Taking things to the extreme: LOOCV

Set $k=n$; that is, we use $n$ folds, each of size $1$. The test sets will therefore consist of a single observation and the training sets of $n-1$ observations.

# Model selection: stepwise procedures

-   Not my favorite method,

-   but this is something you should learn because it is widely used.

. .

-   Akin the "throw cooked spaghetti to the wall and see what sticks" technique.

. . .

Stepwise selection procedures ar of two types: (i) forward and (ii) backward.

## Forward selection procedure

1.  Choose a criterion that balances model fit (smaller SSR) and parsimony (small $k$).

<!-- -->

i.  e.g. adjusted-$R^2$, **AIC** or BIC

. . .

2.  Start with the empty model $Y \approx \beta_0$, i.e. the model with no predictor. This is our **current model**

3.  Fit all possible models with one additional predictor.

. .

4.  Compute the AIC of each of these models

<!-- -->

i.  Identify the model with the smallest AIC. This is our **candidate model**.

. . .

ii. If the AIC of the candidate model is *smaller* (better) than the AIC of the current model, the candidate model becomes the current model, and we go back to step 3.

iii. If the AIC of the candidate model is *larger* than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model.

## Backward selection procedure

Similar to forward selection, except that

-   we start with the full model,
-   remove one predictor at a time
-   until removing any predictors makes the model worse.

. . .

Note that forward and backward selection need not agree; they may select different models.

. . .

-   What to do in that case? Nobody knows.

## 

::: {.callout-tip icon="false"}
## Group exercise - stepwise selection

-   Exercises 8.11, 8.13

-   In addition

i.  fit the first models in `R`
ii. identify the baseline level of the categorical variable in each model.

```{r}
library(openintro)
d <- openintro::births14
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

# How to prevent overfiting?

-   Parsimony
    -   Use domain knowledge
    -   One-in-ten rule (could be one-in-five)

- Multicollinearity
    - select one predictor
    - combine predictors, e.g. take the average.




# Recap

## Recap