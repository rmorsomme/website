---
title: "Model Selection"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   Homework 4 is due on **Thursday**.
-   Homework 5 will be due after the prediction project on Sunday June 5.
-   Prediction project
    -   Find your teammate [here](slides/prediction-project-teams.html).

    -   If you have not started yet, start now. You can do already do 80% of the project.

    -   You will have the tools to do the remaining 20% after this lecture.

## Recap {.smaller}

::: incremental
-   simple linear regression model $$
    Y \approx \beta_0 + \beta_1 X
    $$

-   multiple linear regression model $$
    Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots +  + \beta_p X_p 
    $$

-   categorical predictor

    -   $(k-1)$ indicator variables

-   feature engineering

    -   transforming variables
    -   combining variables
    -   interactions
:::

## Outline

-   Overfitting
    -   limitations of $R^2$ for model selection
-   Model selection through
    -   overall criteria (adjusted-$R^2$, AIC, BIC)
    -   predictive performance (holdout method, cross-validation)
    -   stepwise procedure (forward, backward)

# Competing models

## Competing models

-   Earlier, we saw that adding the predictor $\dfrac{1}{\text{displ}}$ gave a better fit.

-   Let us see if the same idea work with the `trees` dataset.

## A nonlinear association?

Suppose we want to predict volume using only the variable `girth`.

```{r}
set.seed(0)
d_tree <- datasets::trees %>% 
  mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5)) # add some random noise to girth for illustration purpose
ggplot(d_tree) +
  geom_point(aes(Girth, Volume))
```

One could argue that there is a slight nonlinear association

## Function to compute $R^2$

```{r}
compute_R2 <- function(model){
  
  model_glanced <- glance(model)
  R2 <- model_glanced$r.squared
  R2_rounded <- round(R2, 3) 
  
  return(R2_rounded)
  
}
```

## Starting simple...

We start with the simple model

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth}
$$

```{r}
m1 <- lm(Volume ~ Girth, data = d_tree)
compute_R2(m1)
```

## 

::: panel-tabset
## `R` code

```{r}
Girth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)
new_data <- tibble(Girth = Girth_new)
Volume_pred <- predict(m1, new_data)
new_data <- mutate(new_data, Volume_pred = Volume_pred)
head(new_data)
```

## Regression line

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred))
```
:::

## ...taking it up a notch...

To capture the nonlinear association between `Girth` and `Volume`, we consider the predictor $\text{Girth}^2$.

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth} + \beta_2 \text{girth}^2
$$

The command to fit this model is

```{r}
d_tree2 <- mutate(d_tree, Girth2 = Girth^2)
m2 <- lm(Volume ~ Girth + Girth2, data = d_tree2)
compute_R2(m2)
```

. . .

$R^2$ has increased! It went from `r compute_R2(m1)` (model 1) to `r compute_R2(m2)`.

## 

::: panel-tabset
## `R` code

```{r}
Girth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)
new_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2)
Volume_pred <- predict(m2, new_data)
new_data <- mutate(new_data, Volume_pred = Volume_pred)
head(new_data)
```

## Regression line

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred))
```
:::

## ...taking it up another notch...

Let us consider the predictor $\text{Girth}^3$.

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth} + \beta_2 \text{girth}^2 + \beta_3 \text{girth}^3
$$

```{r}
d_tree3 <- mutate(d_tree, Girth2 = Girth^2, Girth3 = Girth^3)
m3 <- lm(Volume ~ Girth + Girth2 + Girth3, data = d_tree3)
compute_R2(m3)
```

. . .

$R^2$ has increased! It went from `r compute_R2(m2)` (model 2) to `r compute_R2(m3)`.

## 

::: panel-tabset
## `R` code

```{r}
Girth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.001)
new_data <- tibble(Girth = Girth_new) %>% mutate(Girth2 = Girth^2, Girth3 = Girth^3)
Volume_pred <- predict(m3, new_data)
new_data <- mutate(new_data, Volume_pred = Volume_pred)
head(new_data)
```

## Regression line

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred))
```
:::

## ...before taking things to the extreme

What if we also include the predictors $\text{Girth}^4, \text{Girth}^5, \dots, \text{Girth}^{k}$ for some larger number $k$?

The following `R` command fits the model with $k=81$, that is,

$$
\text{Volume} \approx \beta_0 + \beta_1 \text{girth} + \beta_2 \text{girth}^2 + \dots + \beta_{81} \text{girth}^{81}
$$

```{r}
m_extreme <- lm(Volume ~ poly(Girth, degree = 81, raw = TRUE), data = d_tree)
compute_R2(m_extreme)
```

$R^2$ has again increased! It went from `r compute_R2(m3)` (model 3) to `r compute_R2(m_extreme)`.

## A limitation of $R^2$

As we keep adding predictors, $R^2$ will always increase.

-   additional predictors allow the regression line to be more flexible, hence to be closer to the points and reduce the residuals.

. . .

Is the model `m_extreme` a good model?

::: callout-note
We want to learn about the relation between `Volume` and `Girth` present in the *population* (not the sample).

A good model should accurately represent the *population*.
:::

## 

::: panel-tabset
## `R` code

```{r, cache=TRUE}
#| code-line-numbers: "1|2|3|4|"
Girth_new <- seq(min(d_tree$Girth), max(d_tree$Girth), by = 0.0001)
new_data <- tibble(Girth = Girth_new)
Volume_pred <- predict(m_extreme, new_data)
new_data <- mutate(new_data, Volume_pred = Volume_pred)
head(new_data)
```

## Regression line

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred))
```

## Regression line (zoomed)

```{r}
ggplot() +
  geom_point(data = d_tree  , aes(Girth, Volume     )) +
  geom_line (data = new_data, aes(Girth, Volume_pred)) +
  ylim(0, 90)
```
:::

## Overfitting

The model `m_extreme` **overfits** the data.

A model overfits the data when it fits the sample extremely well but does a poor job for new data.

. . .

Remember that we want to learn about the *population*, not the *sample*!

# Model selection: overall criteria

## $R^2$

We saw that $R^2$ keeps increasing as we add predictors.

$$
R^2 = 1 - \dfrac{SSR}{SST}
$$

. . .

$R^2$ can therefore not be used to identify models that overfit the data.

## Adjusted-$R^2$

The **adjusted-**$R^2$ is similar to $R^2$, but penalizes large models:

$$
R^2_{\text{adj}} = 1 - \dfrac{SSR}{SST}\dfrac{n-1}{n-p-1}
$$

where $p$ corresponds to the number of predictors (model size).

## 

The adjusted-$R^2$ therefore balances goodness of fit and parsimony:

-   The ratio $\dfrac{SSR}{SST}$ favors model with good fit (like $R^2$)
-   The ratio $\dfrac{n-1}{n-k-1}$ favors parsimonious models

. . .

The model with the highest adjusted-$R^2$ typically provides a good fit without overfitting.

## 

::: {.callout-tip icon="false"}
## Group exercise - a function for the adjusted-$R^2$

Consider the function `compute_R2` which takes a model as input and returns its $R^2$ rounded to the 3rd decimal.

```{r}
compute_R2 <- function(model){
  
  model_glanced <- glance(model)
  R2 <- model_glanced$r.squared
  R2_rounded <- round(R2, 3) 
  
  return(R2_rounded)
  
}
```

Adapt the function so that it computes the adjusted-$R^2$ instead of $R^2$. Give an appropriate name to the new function.
:::

```{r, echo = F}
countdown(minutes = 4)
```

## AIC and BIC {.smaller}

Two other popular overall criteria that balance goodness of fit (small SSR) and parsimony (small $p$) are

-   the Akaike Information Criterion (**AIC**)
-   the Bayesian Information Criterion (**BIC**)

. . .

The formula for AIC and BIC are respectively

$$
AIC = 2p - \text{GoF}, \qquad BIC = \ln(n)p- \text{GoF}
$$

where $\text{GoF}$ measures the *goodness of fit* of the model[^1].

[^1]: The exact formula for $\text{GoF}$ is beyond the scope of this class.

## AIC and BIC in practice

::: callout-warning
Unlike the adjusted-$R^2$, **smaller is better for the AIC and BIC**.
:::

. . .

::: callout-note
The BIC penalizes the number of parameters $p$ more strongly than AIC. BIC will therefore tend to favor smaller models (models with fewer parameters).
:::

## Computing AIC and BIC

AIC and BIC are easily accessible in `R` with the command `glance`.

```{r}
rbind(glance(m1), glance(m2), glance(m3), glance(m_extreme))
```

. . .

In this case, BIC favor `m2`, AIC cannot decide between `m2` and `m3` and the adjusted-$R^2$ (wrongly) favors `m_extreme`.

-   For AIC and BIC, smaller is better!
-   BIC favors more parsimonious models than AIC.

# Model selection: predictive performance

## Limitations of the previous criteria

The adjusted-$R^2$, AIC and BIC all try to balance

1.  goodness of fit
2.  parsimony

. . .

They achieve this balance by favoring models with small SSR while penalizing models with larger $p$.

. . .

...But

-   The form of the penalty for $p$ is somewhat arbitrary, e.g. AIC versus BIC.
-   The adjusted-$R^2$ failed to penalize `m_extreme`, although it was clearly overfitting the data.

## Predictive performance

Instead of using these criteria, we could look for the model with the best **predictions performance**.

That is, the model that makes predictions for **new** observations that are the closest to the true values.

. . .

We will learn two approaches to accomplish this

-   the holdout method
-   cross-validation

## The holdout method {.smaller}

The holdout method is a simple method to evaluate the predictive performance of a model.

. . .

1.  Randomly partition your sample into two sets: a **training set** (typically 2/3 of the sample) and a **test set** (the remaining 1/3).

. . .

2.  Fit your model to the training set.

. . .

3.  Evaluate the prediction accuracy of the model on the test set.

. . .

Note that the test set consists of *new* observations for the model.

. . .

$\Rightarrow$ Select the model with the best prediction accuracy in step 3.

## 

![The holdout method.](images/lec-6/holdout.PNG){fig-align="center"}

Source: [IMS](https://openintro-ims.netlify.app/inf-model-mlr.html#comparing-two-models-to-predict-body-mass-in-penguins)

## Step 1: training and test sets

The following `R` function splits a sample into a training and a test set

```{r}
construct_training_test <- function(sample, prop_training = 2/3){
  
  n          <- nrow(sample)
  n_training <- round(n * prop_training)
  
  sample_random   <- slice_sample(sample, n = n)
  sample_training <- slice(sample_random,    1 : n_training )
  sample_test     <- slice(sample_random, - (1 : n_training))
  
  return(list(
    training = sample_training, test = sample_test
    ))
  
}
```

## 

::: panel-tabset
## Sample

```{r}
d_tree # entire sample
```

## Training set

```{r}
set.seed(0)
training_test_sets <- construct_training_test(d_tree) 
training_set <- training_test_sets[["training"]]
training_set
```

## Test set

```{r}
test_set <- training_test_sets[["test"]]
test_set
```
:::

## Step 2: fit the model to the training set

We simply fit our regression model to the training set.

```{r}
m1 <- lm(Volume ~ Girth, data = training_set)
```

## Step 3: Evaluate the prediction accuracy on the test set

To evaluate the prediction accuracy, we start by computing the predictions for the observations in test set.

```{r}
Volume_hat <- predict(m1, test_set)
```

. . .

A good model will make predictions that are closed to the true values of the response variable.

## Sum of squared errors

A good measure of prediction accuracy is the **sum of squared errors** (SSE).

$$
SSE = (y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_m - \hat{y}_m)^2
$$

Small SSE is better.

```{r}
Volume <- test_set$Volume
sum((Volume - Volume_hat)^2)
```

## Mean squared error

In practice, the **(root) mean sum of squared errors** ((R)MSE) is often used.

$$
MSE = \dfrac{SSE}{m} = \dfrac{(y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_m - \hat{y}_m)^2}{m}
$$

$$
RMSE = \sqrt{\dfrac{SSE}{m}} = \sqrt{\dfrac{(y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_m - \hat{y}_m)^2}{m}}
$$ Again, **small (R)MSE is better**.

## 

```{r}
SSE <- sum((Volume - Volume_hat)^2)
m <- nrow(test_set)
MSE <- SSE / m
RMSE <- sqrt(MSE)
SSE; MSE; RMSE
```

::: callout-tip
## (R)MSE

The advantage of (R)MSE over the SSE is that we can compare models evaluate with test sets of different sizes.
:::

## Selecting a model

Apply steps 2 and 3 on different models.

-   use the same training and test sets from step 1 for the different models

. . .

$\Rightarrow$ Choose the model with the lowest (R)MSE!

This model has the best **predictive accuracy**.

## Function to compute the RMSE

```{r}
compute_RMSE <- function(test_set, model){
  
  y     <- test_set$Volume
  y_hat <- predict(model, test_set)
  
  E    <- y - y_hat
  MSE  <- mean(E^2)
  RMSE <- sqrt(MSE)
  
  return(RMSE)
  
}
```

## 

```{r}
# Step 2
m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)
m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)
m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)
m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)

# Step 3
compute_RMSE(test_set, m1)
compute_RMSE(test_set, m2)
compute_RMSE(test_set, m3) # best
compute_RMSE(test_set, m48)
```

We select `m3`.

## Limitation of the holdout method

The previous analysis was conducted with `set.seed(0)`.

. . .

Models `m2` and `m3` were pretty close.

Could we have obtained a different result with a different seed?

## 

```{r}
# Step 1
set.seed(1) # new seed
training_test_sets <- construct_training_test(d_tree) 
training_set <- training_test_sets[["training"]]
test_set <- training_test_sets[["test"]]

# Step 2
m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)
m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)
m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)
m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)

# Step 3
compute_RMSE(test_set, m1)
compute_RMSE(test_set, m2) # best
compute_RMSE(test_set, m3)
compute_RMSE(test_set, m48)
```

## The test set matters!

With `set.seed(1)`, the holdout method indicates that `m2` is the best model.

-   Our previous best model `m3` is even to be worse than `m1`!

. . .

A drawback of the holdout method is that the test set matters a lot.

-   Repeating steps 2 and 3 with a different partition from step 1 may give different results.

## 
::: {.callout-tip icon="false"}
## Group exercise - limitation of the holdout method

Copy and paste the following code in your `R` session. Try a few different seed numbers. Which models usually have the smallest RMSE? Is `m1` ever the best model?

You can ignore the warning message.

```{r, eval=FALSE}
# Setup
set.seed(0) # do not change this seed number
d_tree <- datasets::trees %>% mutate(Girth = Girth + rnorm(nrow(.), 0, 0.5))
compute_RMSE <- function(test_set, model){
  
  y     <- test_set$Volume
  y_hat <- predict(model, test_set)
  
  E    <- y - y_hat
  MSE  <- mean(E^2)
  RMSE <- sqrt(MSE)
  
  return(RMSE)
  
}
construct_training_test <- function(sample, prop_training = 2/3){
  
  n          <- nrow(sample)
  n_training <- round(n * prop_training)
  
  sample_random   <- slice_sample(sample, n = n)
  sample_training <- slice(sample_random,    1 : n_training )
  sample_test     <- slice(sample_random, - (1 : n_training))
  
  return(list(
    training = sample_training, test = sample_test
    ))
  
}

# Step 1
set.seed(2) # try a few different seed numbers
training_test_sets <- construct_training_test(d_tree) 
training_set <- training_test_sets[["training"]]
test_set <- training_test_sets[["test"]]

# Step 2
m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)
m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)
m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)
m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)

# Step 3
compute_RMSE(test_set, m1)
compute_RMSE(test_set, m2)
compute_RMSE(test_set, m3)
compute_RMSE(test_set, m48)
```

:::
```{r, echo = F}
countdown(minutes = 5)
```

## Cross-validation

Cross-validation (CV) is the natural generalization of the holdout method:

-   simply repeat the holdout method many times.

\##

0.  randomly partition the sample into $k$ folds of equal size. $k$ is typically $5$ or $10$.

. . .

Repeat the following steps $k$ times

1.  Let fold 1 be the test set and the remaining folds be the training set.

2.  Fit the model to the training set (like the holdout method)

3.  Evaluate the prediction accuracy of the model on the test set (like the holdout method)

4.  Go back to 1, letting the next fold be the test set.

. . .

$\Rightarrow$ Select the model with the best overall prediction accuracy in step 3.

## 

![5-fold cross-validation](images/lec-6/cv.PNG){fig-align="center"}

Source: [towardsdatascience](https://towardsdatascience.com/validating-your-machine-learning-model-25b4c8643fb7)

## Step 0

```{r}
set.seed(0)

# Setup
n <- nrow(d_tree)
n_folds <- 10

# Fold assignment
folds <- c(rep(1:n_folds, n %/% n_folds), seq_along(n %% n_folds))
folds

# Step 1
d_tree_fold <- d_tree %>%
  slice_sample(n = n) %>%
  mutate(fold = folds)
head(d_tree_fold)
```

## Steps 1-4 -- Setup

```{r}
create_empty_RMSE <- function(){
  tibble(
    rmse_m1  = numeric(), 
    rmse_m2  = numeric(), 
    rmse_m3  = numeric(), 
    rmse_m48 = numeric()
  )
}

RMSE <- create_empty_RMSE()
RMSE
```

## Steps 1-4

```{r, warning=FALSE}
RMSE <- create_empty_RMSE()

for(i in 1:n_folds){
  
  # Step 1
  training_set <- filter(d_tree_fold, fold != i)
  test_set     <- filter(d_tree_fold, fold == i)
  
  # Step 2
  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)
  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)
  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)
  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)
  
  # Step 3
  rmse_m1  <- compute_RMSE(test_set, m1 )
  rmse_m2  <- compute_RMSE(test_set, m2 )
  rmse_m3  <- compute_RMSE(test_set, m3 )
  rmse_m48 <- compute_RMSE(test_set, m48)
  
  RMSE <- RMSE %>% add_row(rmse_m1, rmse_m2, rmse_m3, rmse_m48)
  
}
```

## Model selection with RMSE

Which model has the lowest RMSE across all folds?

```{r}
RMSE
```

```{r}
summarise_all(RMSE, mean)
summarise_all(RMSE, sum) # what the book uses
```

`m2` has slightly better average RMSE.

## 

::: {.callout-tip icon="false"}
## Group exercise - cross-validation

Exercise [25.9](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises), [25.11](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises), [25.12](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)

In part c, replace *left* and *right*, by *top* and *bottom*.
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Short podcast on cross-validation

Still confused by corss-validation?

Check out this [podcast](https://dataskeptic.com/blog/episodes/2014/cross-validation) to hear more about this important topic.

. . .

These other two podcasts on [muliple linear regression](https://dataskeptic.com/blog/episodes/2016/multiple-regression) and [$R^2$](https://dataskeptic.com/blog/episodes/2016/r-squared) are also excellent.

## Taking things to the extreme: LOOCV

Set $k=n$.

-   Use $n$ folds, each of size $1$.

-   The test sets will therefore consist of a single observation and the training sets of the remaining $n-1$ observations.

## LOOCV {.smaller}

```{r}
RMSE <- create_empty_RMSE()

# Step 0
n <- nrow(d_tree)
d_tree_loocv <- mutate(d_tree, folds = 1 : n) # every observation belongs to a different fold 

for(i in 1:n){
  
  # Step 1
  training_set <- filter(d_tree_loocv, folds != i)
  test_set     <- filter(d_tree_loocv, folds == i)
  
  # Step 2
  m1  <- lm(Volume ~ poly(Girth, degree = 1 , raw = TRUE), data = training_set)
  m2  <- lm(Volume ~ poly(Girth, degree = 2 , raw = TRUE), data = training_set)
  m3  <- lm(Volume ~ poly(Girth, degree = 3 , raw = TRUE), data = training_set)
  m48 <- lm(Volume ~ poly(Girth, degree = 48, raw = TRUE), data = training_set)
  
  # Step 3
  rmse_m1  <- compute_RMSE(test_set, m1 )
  rmse_m2  <- compute_RMSE(test_set, m2 )
  rmse_m3  <- compute_RMSE(test_set, m3 )
  rmse_m48 <- compute_RMSE(test_set, m48)
  
  RMSE <- add_row(RMSE, rmse_m1, rmse_m2, rmse_m3, rmse_m48)
  
}

summarize_all(RMSE, mean)
```

# Model selection: stepwise procedures

-   Not my favorite method,

-   but this is something you should learn because it is widely used.

. . .

-   Akin the "throw cooked spaghetti to the wall and see what sticks" technique.

. . .

Two types of stepwise procedures: (i) **forward selection** and (ii) **backward elimination**.

## Forward selection {.smaller}

1.  Choose an overall criterion that balances model fit (smaller SSR) and parsimony (small $p$)

    $\Rightarrow$ adjusted-$R^2$, AIC or BIC

. . .

2.  Start with the **empty** model $Y \approx \beta_0$, i.e. the model with no predictor. This is our **current model**

3.  Fit all possible models with one additional predictor.

. . .

4.  Compute the AIC of each of these models.

5.  Identify the model with the smallest AIC. This is our **candidate model**.

. . .

6.  If the AIC of the candidate model is *smaller* (better) than the AIC of the current model, the candidate model becomes the current model. Go back to step 3.

    If the AIC of the candidate model is *larger* than the AIC of the current model (no new model improves on the current one), the procedure stops, and we select the current model.

## 

::: {.callout-tip icon="false"}
## Group exercise - forward selection

Exercise [8.13](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises). In addition, fit the candidate model (a single model) in `R` with the command `lm`.

```{r}
library(openintro)
d <- openintro::births14
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

## Backward elimination

Similar to forward selection, except that

-   Start with the full model.
-   Remove one predictor at a time,
-   until removing any predictor makes the model worse.

## 

::: {.callout-tip icon="false"}
## Group exercise - backward elimination

Exercise [8.11](https://openintro-ims.netlify.app/model-mlr.html#chp8-exercises). In addition, fit the current and candidate models in `R` with the command `lm`.

```{r}
library(openintro)
d <- openintro::births14
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

## 

Note that forward and backward stepwise selection need not agree; they may select different models.

. . .

-   What to do in that case? Nobody knows.

# How to prevent overfitting?

::: tabset-tip
## Parsimony

To obtain a parsimonious model

-   use your knowledge of the subject to **carefuly** choose which variables to consider and construct new ones, and
-   implement a model selection procedure.
:::

# Recap

## Recap

-   Overfitting
    -   limitations of $R^2$ for model selection
-   Model selection through
    -   overall criteria (adjusted-$R^2$, AIC, BIC)

    -   predictive performance (holdout method, cross-validation)

    -   stepwise procedure (forward, backward)

You are now ready for the prediction project!
