---
title: "Simple Linear Regression Models"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    code-copy: true
    scrollable: true
    link-external-newwindow: true
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false
#| echo: false
#| message: false
library(countdown)
library(tidyverse)       # for data wrangling
library(openintro)
library(kableExtra)
library(usdata)
library(scales)
library(tidymodels)
library(glue)
library(broom) # augment
library(patchwork)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(
  ggplot2::theme_minimal(base_size = 16)
  )

# set default figure parameters for knitr
knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  fig.align = "center",
  out.width = "80%"
)
```

# Welcome

## Announcements

-   Find someone you have not worked with yet.
-   Homework 1 has been released
    -   Ask us if anything is unclear

    -   regrade window of 48 hours.
-   Homework 2 has been published
    -   Due on Wednesday (tomorrow)
    -   Feel free to re-use the template from HW 1
    -   There should be no need for external code -- look in the lab!

## Recap of last lecture

::: incremental
-   Histogram, scatterplot, boxplot
-   Average, median, variance, sd and IQR; robustness
-   Frequency, contigency and proportion tables
-   Barplot, mosaic plot
-   Effective communication: well-edited figures, $\ge3$ variables (symbols, colors, facets)
-   [R for Data Science - chapters 3 and 7](https://r4ds.had.co.nz/data-visualisation.html)
:::

## Outline

-   simple linear regression model
-   least-square estimates
-   model comparison
-   outliers

# Simple linear regression

## Regression model

```{r}
d <- ggplot2::mpg
head(d, n = 4)
```

. . .

Suppose the variable `hwy` (fuel efficiency on highway) is very expensive to measure.

We decide to estimate it using the other variables. To do so, we will fit a **regression model**.

$$
\text{hwy} \approx \text{model}(\text{other variables})
$$

## Simple regression

We expect the variable `cty` to be a good proxy for `hwy`.

After all, if a car is efficient in the city, we expect it to also be efficient on the highway! We will therefore consider a **simple regression** model

$$
\text{hwy} \approx \text{model}(\text{cty})
$$ Simple here means that we include a single predictor in the model.

## Simple linear regression

```{r}
ggplot(d) +
  geom_point(aes(x = cty, y = hwy))
```

The variables `cty` and `hwy` are *linearly* associated.

We therefore opt for a **simple linear regression** model

$$
\text{hwy} \approx \beta_0 + \beta_1 \text{cty}
$$

## Parameters

The numbers $\beta_0$ and $\beta_1$ are the two parameters of the model.

. . .

We now want to find good values for these unknown parameters.

. . .

Let us look at two sets of values:

```{=tex}
\begin{align*}
\beta_0 = 1, \beta_1 = 1.3 \qquad & \Rightarrow \qquad \text{hwy} \approx 1 + 1.3 \text{cty} \\

\beta_0 = 15, \beta_1 = 0.5 \qquad & \Rightarrow \qquad \text{hwy} \approx 15 + 0.5 \text{cty}
\end{align*}
```
## 

::: panel-tabset
## Good model

```{r}
#| code-line-numbers: "3"
ggplot(d) +
  geom_point(aes(x = cty, y = hwy)) +
  geom_abline(intercept = 1, slope = 1.3)
```

## Bad model

```{r}
#| code-line-numbers: "3"
ggplot(d) +
  geom_point(aes(x = cty, y = hwy)) +
  geom_abline(intercept = 15, slope = 0.5)
```
:::

## Prediction

We can use our models to estimate `hwy` for new vehicles.

. . .

Imagine there is a new vehicle with $\text{cty} = 30$. Instead of measuring its `hwy` (expensive), we use our model to estimate it. Using the "good" model gives the following estimate

$$
\begin{align*}
\text{hwy} 
& \approx \beta_0 + \beta_1 \text{cty} \\ 
& = 1 + 1.3 * 30\\
& = 40
\end{align*}
$$

## 

Is this the true `hwy` of the new vehicle? No!

. . .

-   this is only an **estimate** based on the value of the variable `cty` and our "good" model.

. . .

Can we do better? Yes!

. . .

-   Take additional variables into account in the model (e.g. engine size, vehicle age, etc)

. . .

-   Use better values for $\beta_0$ and $\beta_1$.

## 

::: {.callout-tip icon="false"}
## Group exercise - parameters

1.  What is the prediction for the new vehicle ($\text{cty} = 30$) if we use the bad model ($\beta_0 = 15, \beta_1=0.5$)?

2.  Copy and paste the following piece of code and try different values for the parameters to find a good set of values.

```{r, eval=FALSE}
library(tidyverse)
d <- ggplot2::mpg
beta_0 <- 15
beta_1 <- 0.5
ggplot(d) +
  geom_point(aes(x = cty, y = hwy)) +
  geom_abline(intercept = beta_0, slope = beta_1)
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

## Linear association

A simple linear regression model is only applicable if the relation between the predictor and the response is **linear**.

. . .

If the relation is not linear, the simple linear regression is not suitable.

In this case, we need to *model* the non-linearity (next lecture).

## 

::: {.callout-tip icon="false"}
## Group exercise - linear association

Exercise [7.3](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Leat-square estimates

## Residuals

Our predictions are only approximate.

-   Let us represent our prediction with $\widehat{\text{hwy}}$ and the true value with $\text{hwy}$
-   the error we make is $\text{hw} - \widehat{\text{hwy}}$
-   this is called the **residual**

$$e = \text{hwy} - \widehat{\text{hwy}}$$

## Visualizing residuals {.smaller}

::: columns
::: {.column width="25%"}
::: nonincremental
-   Black circles: Observed values ($\text{hwy}$)
-   Pink solid line: Least-squares regression line
-   Maroon triangles: Predicted values ($\widehat{\text{hwy}}$)
-   **Gray lines:** Residuals
:::
:::

::: {.column width="75%"}
```{r}
#| out-width: "100%"
#| echo: false
d_m_small <- d %>%
  filter(cty > 28) %>%
  mutate(hwy_hat = 1 + 1.3 * cty)

ggplot(d, aes(x = cty, y = hwy)) +
  geom_point() +
  geom_abline(intercept = 1, slope = 1.3, color = "pink") +
  geom_point(data = d_m_small, aes(y = hwy_hat), color = "maroon", shape = "triangle", size = 2) +
  geom_segment(data = d_m_small, aes(xend = cty, yend = hwy_hat), size = 0.5, color = "gray20") +
  ylim(25.1, 50)+
  xlim(17.1, 37.5)
```
:::
:::

## Residual plot

```{r}
#| code-line-numbers: "3|4|7|8"
d %>%
  mutate(
    hwy_hat = 1 + 1.3 * cty, # prediction
    resid   = hwy - hwy_hat  # residual
    ) %>%
  ggplot() +
  geom_point(aes(cty, resid)) +
  geom_abline(intercept = 0, slope = 0, col = "red")
```

## Assessing linearity with residual plots

```{r, echo = F}
neg_lin <- simulated_scatter %>% filter(group == 6)
neg_cur <- simulated_scatter %>% filter(group == 7)
random  <- simulated_scatter %>% filter(group == 8)
neg_lin_mod <- augment(lm(y ~ x, data = neg_lin))
neg_cur_mod <- augment(lm(y ~ x, data = neg_cur))
random_mod  <- augment(lm(y ~ x, data = random))
p_neg_lin <- ggplot(neg_lin, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_neg_cur <- ggplot(neg_cur, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_random <- ggplot(random, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_neg_lin_res <- ggplot(neg_lin_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_neg_cur_res <- ggplot(neg_cur_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_random_res <- ggplot(random_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))
p_neg_lin     + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + 
p_neg_cur     + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_random + 
p_neg_lin_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + 
p_neg_cur_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_random_res +
  plot_layout(ncol = 3, heights = c(2, 1))
```

Source: [IMS](https://openintro-ims.netlify.app/model-slr.html#resids)

## 

::: {.callout-tip icon="false"}
## Group exercise - residuals

Exercises [7.1](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises), [7.17](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises), [7.19](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)
:::

```{r, echo = F}
countdown(minutes = 5)
```

## Good estimates

We want to choose estimates that give a model that *fits the data well*.

-   a model with a regression line that is close to the data

. . .

We want to minimize the residuals.

## Minimizing residuals

Perhaps the most natural thing to do is to find the values of $\beta_0$ and $\beta_1$ that minimize the **sum of absolute residuals**

$$
|e_1|+|e_2|+\dots+|e_n|
$$

. . .

For practical reasons, the **sum of squared residuals** (SSR) is a more common criterion

$$
e_1^2+e_2^2+\dots+e_n^2
$$

## Why squaring the residuals?

-   nice mathematical properties
    -   can work by hand (pre-computer era; first derived by Gauss in the late 1700s ([Stigler, 1981](https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-3/Gauss-and-the-Invention-of-Least-Squares/10.1214/aos/1176345451.full)))
-   reflects the assumptions that being off by $4$ is more than twice as bad as being off by $2$
-   mainstream, e.g. the command `lm` in `R`.

## Least-square estimates

We simply find the values for $\beta_0$ and $\beta_1$ that minimize the SSR with the R command `lm`

```{r}
m <- lm(hwy ~ cty, data = d) # find least-square estimates
m
```

The symbols $\hat{\beta}_0$ and $\hat{\beta}_1$ denote least-square estimates.

In our model, we therefore have $$\hat{\beta}_0 = 0.892, \qquad \hat{\beta}_1 = 1.337$$

## Visualizing the least square regression line

```{r}
#| code-line-numbers: "3"
ggplot(d) +
  geom_point(aes(x = cty, y = hwy)) +
  geom_abline(intercept = 0.892, slope = 1.337, col = "red")
```

## Reading `R` output

`R` can provide the results of a model in different formats.

```{r}
m <- lm(hwy ~ cty, data = d)
```

::: panel-tabset
## Estimates only

```{r}
m
```

Printing the model itself provides the least-square estimates. This is sufficient for now.

## Raw `R` output

```{r}
summary(m)
```

This format contains information that will be useful when we do inference. It is, however, difficult to read.

## Tidy `R` output

```{r}
summary(m) %>%
  tidy() %>%
  kable(digits = 2)
```

This format contains the information necessary for doing inference *and* is easy to read. This is the format used the textbook.
:::

## Exploring the model

The command `augment` from the `R` package `broom` gives us the residuals (`.resid`) and predictions (`.fitted`).

```{r}
#| code-line-numbers: "1|2|3"
library(broom)
m <- lm(hwy ~ cty, data = d)
augment(m)[,1:4]
```

## 

::: {.callout-tip icon="false"}
## Group exercise - exploring a model

Consider the third observation in the sample. What is its

1.  value of `cty`?
2.  predicted value for `hwy` based on the model?
3.  actual value for `hwy`?
4.  residual?
5.  Does the model over- or under-predict?
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Intepreting the parameters {.smaller}

-   the intercept estimate $\hat{\beta}_0$ is the prediction for a car with $\text{cty} = 0$
    -   meaningless in our case

. . .

-   the slope estimate $\hat{\beta}_1$ is the increase in a our prediction for `hwy` for each additional unit of `cty`
    -   "for each additional unit of `cty`, we expect `hwy` to increase by 1.337"

. . .

::: callout-tip
## Extrapolation

-   Stick to the range of the data
    -   if you extrapolate, do so with care, not like [this](https://www.cc.com/video/l4nkoq/the-colbert-report-science-catfight-joe-bastardi-vs-brenda-ekwurzel).
-   The intercept will not always be meaningful
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - least-square estimates

Exercise [7.21](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)

-   skip part c
-   start by fitting the model in `R` with the following command,

```{r, eval=FALSE}
d <- openintro::coast_starlight
m <- lm(travel_time ~ dist, data = d)
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

## Special case: categorical predictor

Let us create a binary predictor indicating if the car is from $1999$.

```{r}
d_binary <- mutate(d, year_binary = if_else(year == 1999, 1, 0))
head(select(d_binary, hwy, year, year_binary))
```

The variable `year_binary` takes the value $1$ if the car is from $1999$ and $0$ otherwise.

##  {.smaller}

```{r}
m_binary <- lm(hwy ~ year_binary, data = d_binary)
m_binary
```

The model equation is

$$
\text{hwy} \approx 23.46 - 0.026 \text{year_binary}
$$

. . .

For a new car from $1999$, the variable `year_binary` takes the value $1$ and our prediction is

$$
\widehat{\text{hwy}} = 23.46 - 0.026*1 = 23.46 - 0.026 = 23.434
$$

. . .

While for a car that not from $1999$, the variable `year_binary` takes the value $0$ and our prediction is

$$
\widehat{\text{hwy}} = 23.46 - 0.026*0 = 23.46 - 0 = 23.46
$$

# Model comparison

## Alternative model

Let us fit an alternative model using engine size (`disp`) as a predictor

$$
\text{hwy} \approx \beta_0 + \beta_1 \text{displ}
$$

```{r}
ggplot(d) +
  geom_point(aes(displ, hwy))
```

## 

The least-square estimates for the coefficients are

```{r}
lm(hwy ~ displ, data = d)
```

Note that the slope coefficient is negative; which makes sense since we would expect cars with larger engines to be less efficient.

## 

We now have two models. Which is the best?

. . .

-   We could start by looking at the residuals

## Comparing residuals

::: columns
::: {.column width="50%"}
```{r}
m1 <- lm(hwy ~ cty, data = d)
m_augment <- augment(m1)
ggplot(m_augment) +
  geom_histogram(aes(.resid)) +
  xlim(-15, 15)
```
:::

::: {.column width="50%"}
```{r}
lm(hwy ~ displ, data = d) %>%
  augment %>%
  ggplot() +
  geom_histogram(aes(.resid)) +
  xlim(-15, 15)
```
:::
:::

. . .

The first model seems to have smaller residuals.

$\Rightarrow$ choose the first model!

## Comparing models in a systematic way

But looking at a plot can be misleading

-   illusions

-   difficult to compare models with similar residuals

. . .

We need a more *systematic* approach for comparing models.

## SSR

Instead of comparing histograms of residuals, we can compute the **SSR** (sum of squared residuals!)

```{=tex}
\begin{align*}
SSR
& = r_1^2+r_2^2+\dots+r^2_n \\
& = (y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_n - \hat{y}_n)^2
\end{align*}
```
-   small residuals will give a small SSR

-   large residuals will give a large SSR

. . .

$\Rightarrow$ choose the model with the smaller SSR!

. . .

📋 The textbook uses the term **SSE** (sum of squared errors).

## 

SSR of the first model:

```{r}
m1 <- lm(hwy ~ cty, data = d)
m_augment <- augment(m1)
resid <- m_augment$.resid
sum(resid^2)
```

SSR of the second model:

```{r}
lm(hwy ~ displ, data = d) %>%
  augment %>% 
  .[[".resid"]] %>% 
  .^2 %>% 
  sum
```

. . .

We opt for the first model (smaller SSR).

## $R^2$ {.smaller}

While the SSR is useful for comparing models, it can also be used to describe the goodness of fit of the model.

. . .

The **SST** (total sum of squares) is the sum of squared distance to the mean.

$$
SST = (y_1 - \bar{y})^2 + (y_2 - \bar{y})^2 + \dots + (y_n - \bar{y})^2
$$

It measures the total amount of variability in the data.

. . .

Remember the formula for **SSR**

$$
SSR = (y_1 - \hat{y}_1)^2 + (y_2 - \hat{y}_2)^2 + \dots + (y_n - \hat{y}_n)^2
$$

It measures the amount of variability in the data left unexplained by the model.

. . .

$SST - SSR$ (total - residual) is therefore the amount of variation explained by the model:

$$
\text{data} = SST = (SST-SSR) + SSR = \text{model} + \text{residuals}
$$

## 

The statistic $R^2$ measures the proportion of variation in the data that is explained by the model.

$$
\begin{align*}
R^2 
& = 1-\dfrac{SSR}{SST}\\
& = \dfrac{SST-SSR}{SST} \\
& = \dfrac{\text{var. explained by model}}{\text{total var.}}
\end{align*}
$$

## 

Note that $0\le R^2 \le 1$.

-   good model $\Rightarrow$ small residuals $\Rightarrow$ small SSR $\Rightarrow$ large $R^2$.
-   great model $\Rightarrow$ tiny residuals $\Rightarrow$ tiny SSR $\Rightarrow$ $R^2$ close to 1.
-   poor model $\Rightarrow$ large residuals $\Rightarrow$ SSR almost as large as SST $\Rightarrow$ $R^2$ close to 0.

## Computing $R^2$ in `R`

To compute $R^2$ in `R`, simply use the command `glance`.

```{r}
m1 <- lm(hwy ~ cty, data = d)
m_glance <- glance(m1)
m_glance$r.squared

lm(hwy ~ displ, data = d) %>% 
  glance %>% 
  .[["r.squared"]]
```

The model with `cty` as a predictor has a $R^2$ value of $0.914$, and the model that uses `displ` has a $R^2$ of $0.59$ (worse).

. . .

The first model is better!

## 

::: {.callout-tip icon="false"}
## Group exercise - Interpretation

-   Exercise [7.23](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)
    -   fit the model in `R`; do you obtain the same estimates?
    -   do parts a-d
    -   compute the SST, SSR and $R^2$ "by hand" in `R` (do not use `glance`). You can use the command `augment` to compute the residuals.

```{r, eval=FALSE}
d <- usdata::county_2019
m <- lm(. . .)
m_augment <- augment(m)
SST <- ...
```
:::

```{r, echo = F}
countdown(minutes = 5)
```

# Outliers

## Outliers in regression

Remember, in a boxplot, outliers are observations far from the bulk of the data

```{r}
ggplot(d) +
  geom_boxplot(aes(hwy))
```

. . .

In the context of regression models, an **outlier** is an observation that falls far from the cloud of points

## Identifying outliers

In the following scatterplot, we see two outliers

```{r}
ggplot(d) +
  geom_point(aes(cty, hwy))
```

## 

::: {.callout-tip icon="false"}
## Group exercise - outlier in regression

Exercise [7.25](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)
:::

```{r, echo = F}
countdown(minutes = 2)
```

## Outliers, leverage and influential points {.smaller}

-   **outliers**: observations that fall far from the cloud of points;

-   **high leverage points**: observations that fall horizontally away from the cloud of points;

-   **influential points**: observations that influence the slope of the regression line;

. . .

-   All influential points are high leverage points.

-   All leverage points are outliers.

-   (Venn Diagram)

## 

```{r, echo=FALSE}
d1 <- simulated_scatter %>% 
  filter(group == 24) %>%
  mutate(outlier = if_else(y == min(y), TRUE, FALSE))
d2 <- simulated_scatter %>% 
  filter(group == 25) %>%
  mutate(outlier = if_else(y == min(y), TRUE, FALSE))
d3 <- simulated_scatter %>% 
  filter(group == 26) %>%
  mutate(outlier = if_else(y == max(y), TRUE, FALSE))
m1_aug <- augment(lm(y ~ x, data = d1)) %>%
  mutate(outlier = if_else(y == min(y), TRUE, FALSE))
m2_aug <- augment(lm(y ~ x, data = d2)) %>%
  mutate(outlier = if_else(y == min(y), TRUE, FALSE))
m3_aug <- augment(lm(y ~ x, data = d3)) %>%
  mutate(outlier = if_else(y == max(y), TRUE, FALSE))
p_1 <- ggplot(d1, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = d1 %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["red", "full"], stroke = 2) +
  labs(title = "Outl.") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(expand = expansion(mult = 0.12))
p_1_res <- ggplot(m1_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(data = m1_aug %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["red", "full"], stroke = 2) +
  labs(x = "Predicted y", y = "Residual") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))
p_2 <- ggplot(d2, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = d2 %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["green", "full"], stroke = 2) + 
  labs(title = "Outl., H.L.") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(expand = expansion(mult = 0.12))
p_2_res <- ggplot(m2_aug, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(data = m2_aug %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["green", "full"], stroke = 2) +
  labs(x = "Predicted y", y = "Residual") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))
p_3 <- ggplot(d3, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(data = d3 %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["pink", "full"], stroke = 2) +
  labs(title = "Outl., H.L., infl.") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(expand = expansion(mult = 0.12))
p_3_res <- ggplot(m3_aug, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(data = m3_aug %>% filter(outlier), 
             size = 5, shape = "circle open", 
             color = IMSCOL["pink", "full"], stroke = 2) +
  labs(x = "Predicted y", y = "Residual") +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    panel.border = element_rect(colour = "gray", fill = NA, size = 1)
    ) +
  scale_x_continuous(expand = expansion(mult = 0.12)) +
  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))
p_1     + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + 
p_2     + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_3 + 
p_1_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + 
p_2_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_3_res +
  plot_layout(ncol = 3, heights = c(2, 1))
```

Source: [IMS](https://openintro-ims.netlify.app/model-slr.html#outliers-in-regression)

## 

::: {.callout-tip icon="false"}
## Group exercise - outlier

Exercise [7.27](https://openintro-ims.netlify.app/model-slr.html#chp7-exercises)
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Least-square estimates are not robust {.smaller}

In regression, outliers have the potential to highly influence the least-square estimates $\hat{\beta}_0$ and $\hat{\beta}_1$.

::: callout-warning
Least-square estimates are not *robust* to the presence of outliers.
:::

. . .

Estimates that are robust include (not covered in this class)

-   least absolute deviation estimates, which minimize the SAR (sum of *absolute* residuals) instead of the SSR

$$
SAR = |e_1| + |e_2| + \dots + |e_n|
$$

-   Bayesian estimates (STA360)

## Impact of outliers

Let us contaminate the data with an outlier (`cty` $=10$ and `hwy` $=1000$)

```{r}
d_contaminated <- select(d, cty, hwy) %>%
  add_row(cty = 40, hwy = 500) # add outlier
arrange(d_contaminated, desc(hwy)) %>% slice(1:5)
```

## 

...and compare the two regression models.

```{r}
lm(hwy ~ cty, data = d)
lm(hwy ~ cty, data = d_contaminated)
```

. . .

The slope estimate has almost tripled!

## 

```{r}
ggplot(d_contaminated) +
  geom_point(aes(cty, hwy)) +
  geom_abline(intercept = -33.841, slope = 3.498, col = "purple")
```

The regression line not longer fits the data well.

## Statistics as an art - dealing with outliers

::: callout-note
## Dealing with outliers

Outliers can unduly influence parameter estimates. How to deal with an outlier depends on why the observation stands out. Outliers can either be

-   removed
-   corrected
-   ignored
:::

# Recap

## Recap

-   simple linear regression model

$$
    \text{hwy} \approx \beta_0 + \beta_1 \text{cty}
$$

-   residuals
-   least-square estimates
-   parameter interpretation
-   model comparison with $R^2$
-   outliers
