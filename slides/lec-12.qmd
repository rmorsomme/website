---
title: "Classical inference"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(countdown)   # for countdown for in-class exercises
library(openintro)   # for textbook data sets
library(kableExtra)
library(janitor)
library(patchwork)   # for combining ggplot figures
library(broom)       # for tidy model output

# set default theme and larger font size for ggplot2
theme_set(theme_minimal(base_size = 16))

options(scipen = 5)
```

# Welcome

## Announcements {.smaller}

-   

## Recap {.smaller}

-   Simple linear regression (case 5)

    -   HT via simulation

    -   CI via bootstrap

## Outline

-   Normal approximation
-   Statistical inference via normal approximation
    -   Hypothesis test

    -   Confidence interval
-   Conditions

# Normal approximation

## Normal distribution

```{r}
tibble(x = seq(-5, 5, by = 0.01)) %>%
  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%
  ggplot() + 
  geom_line(aes(x, normal))
```

$\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped

## Normal approximation

The normal distribution describes the variability of the different statistics

-   $\hat{p}$, $\bar{x}$, $\hat{\beta}$

-   simply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!

. . .

**Classical approach**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.

## Normal approximation for $\bar{x}$

We have seen that if a numerical variable $X$ is normally distributed

$$
X\sim N(\mu, \sigma^2)
$$

then the sample average is also normally distributed

$$
\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

## Condition for the normality of $\bar{x}$  {.smaller}

In practice, we cannot assume that the variable $X$ is *exactly* normally distributed.

But as long as

1.  the sample is large $(n\ge 30)$, or

2.  the variable is *approximately* normal: unimodal, roughly symmetric and no serious outlier

$\bar{x}$ is well approximated by a normal distribution

$$
\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

. . .

See the numerous histograms for case 3 (one mean) where the distribution of $\bar{x}$ always looks pretty normal.

## Normal approximation for $\hat{p}$

If

1.  the sample is large enough $(n\ge30)$

2.  $p$ is not extreme $(pn\ge 10 \text{ and } (1-p)n\ge 10)$

the distribution of $\hat{p}$ can be approximated by a normal distribution

$$
\hat{p} \sim N\left(p, \frac{p(1-p)}{n}\right)
$$

. . .

See the numerous histograms for case 1 (one proportion) where the distribution of $\hat{p}$ always looks pretty normal.

## Conditions are satisfied

```{r, cache=TRUE}
p <- 0.4; n <- 100 # conditions are satisfied: n>30, p*n>10 and (1-p)*n>10
results <- tibble(p_hat = numeric())
for(i in 1 : 1e4){
  sim     <- purrr::rbernoulli(n, p)
  p_hat   <- mean(sim)
  results <- results %>% add_row(p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)
```

## Normal approximation is good

```{r, cache=TRUE}
tibble(p_hat = seq(0.2, 0.6, by = 0.001)) %>%
  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%
  ggplot() +
  geom_line(aes(x = p_hat, y = normal_approximation))
```

## Conditions are *not* satisfied

```{r, cache=TRUE}
p <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 
results <- tibble(p_hat = numeric())
for(i in 1 : 1e4){
  sim     <- purrr::rbernoulli(n, p)
  p_hat   <- mean(sim)
  results <- results %>% add_row(p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)
```

## Normal approximation fails

```{r, cache=TRUE}
tibble(p_hat = seq(-0.05, 0.1, by = 0.001)) %>%
  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%
  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)
```

## Normal approximation for $\hat{\beta}$

If

the distribution of $\hat{\beta}$ can also be approximated by a normal distribution.

# The classical approach to HT

## The classical approach

Step 1: we are interested in the distribution of the statistic under $H_0$.

-   **Modern approach**: *simulate* from this distribution

-   **Classical approach**: *approximate* this distribution with a normal distribution

. . .

Step 2: we want to compute the p-value

-   **Modern approach**: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample

-   **Classical approach**: the p-value is the *area under the curve* of the normal distribution that is at least as extreme as the observed statistic.

## The classical approach in `R`

`R` will compute the p-value for you. Here is what `R` does behind the scene:

## HT for one proportion

```{r}
n <- 1500 # sample size
x <- 780  # number of successes
prop.test(
  x, n,             # observed data
  p = 0.5,          # value in the null hypothesis
  conf.level = 0.99 # confidence level for CI
  )
```

## Comparison with simulation-based HT

The simulation-based HT yielded a p-value of 0.127.

::: callout-note
## A good normal approximation

When the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.
:::

## 

::: {.callout-tip icon="false"}
## Individual exercise - HT for one proportion

Suppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for $p$, the proportion of US adults who are democrats? What is the length of the interval?

What 95% CI do you obtained if 6000 out of 10000 US adults say they are democrat? What is its length?
:::

```{r, echo = F}
countdown(minutes = 3)
```

## HT for two proportions

```{r}
n_m <- 24; n_f <- 24     # sample sizes
x_m <- 14; x_f <- 21     # numbers of promotions
prop.test(
  c(x_m, x_f), c(n_m, n_f), # observed data
  )
```

## Comparison with simulation-based HT

Using the simulation-based HT, we found a p-value of 0.0435.

::: callout-note
## A good normal approximation

When the conditions for the normal approximation are *not* satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.

A simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied.
:::

## HT for one mean

```{r}
d <- ggplot2::mpg
x <- d$hwy
t.test(x, mu = 25)
```

## 

::: {.callout-tip icon="false"}
## Individual exercise - HT for one mean

Construct a 99% CI for `hwy`.

Hint: use the command `help(t.test)` to access the help file of the function `t.test` and see what parameter determines the confidence level.
:::

```{r, echo = F}
countdown(minutes = 3)
```

## HT for two means

```{r}
d <- ggplot2::mpg
x1 <- d$hwy
x2 <- d$cty
t.test(x1, x2)
```

## HT for simple linear regression

```{r}
m <- lm(hwy ~ cty, data = mpg)
tidy(m)
```

## HT for multiple linear regression

```{r}
m <- lm(hwy ~ cty + displ, data = mpg)
tidy(m)
```

## HT for logistic regression

```{r}
d <- heart_transplant %>% mutate(survived_binary = survived == "alive")
m <- glm(survived_binary ~ age + transplant, family = "binomial", data = d)
tidy(m)
```

# Standard error

## Standard error 

**Standard error (SE)**: standard deviation of the normal approximation

-   $SE(\hat{p})=\sqrt{\frac{p(1-p)}{n}}$

-   $SE(\bar{x})=\sqrt{\frac{\sigma^2}{n}} \approx \sqrt{\frac{s^2}{n}}$ where $s^2$ is an estimate of the population variance $\sigma^2$ based on the sample.

-   $SE(\hat{\beta})$ has a complicated form.

```{=html}
<!-- -->
```
-   

## Confidence interval

$$
CI = \text{point estimate} \pm \text{critical value} *SE
$$

The critical value depends on the confidence level (90%, 95%, 99% CI) and the type of data.

To find the critical value corresponding to the confidence level `cl` use the following commands

-   Proportion: `qnorm(1-(1-cl)/2)`

    -   for a 95% CI, the critical value is 1.96, for a 99% CI it is 2.57

-   Mean and regression slope: `qt(1-(1-cl)/2, n-1)` where $n$ is the sample size.

    -   these critical values are slightly larger than those for proportions

    -   as $n$ increases, this difference decreases.

## Larger sample give a smaller SE

::: callout-note
## Sample size matters

Large $n$

$\Rightarrow$ small SE

$\Rightarrow$ normal approximation with small sd

$\Rightarrow$ normal approximation is more concentrated

$\Rightarrow$ tighter CI and smaller p-values.
:::

# Recap

## Recap
