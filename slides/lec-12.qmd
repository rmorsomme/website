---
title: "Classical inference"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(countdown)   # for countdown for in-class exercises
library(openintro)   # for textbook data sets
library(kableExtra)
library(janitor)
library(patchwork)   # for combining ggplot figures
library(broom)       # for tidy model output

# set default theme and larger font size for ggplot2
theme_set(theme_minimal(base_size = 16))

options(scipen = 5)
```

# Welcome

## Announcements {.smaller}

-   

## Recap

-   Simple linear regression (case 5)

    -   HT via simulation

    -   CI via bootstrap

## Outline

-   Normal approximation
-   Classical approach to statistical inference
    -   case 1 -- one proportion
    -   case 2 -- two proportions
    -   case 3 -- one mean
    -   case 4 -- two means
    -   case 4bis -- paired means
    -   case 5 -- regression
-   Standard error

## 

::: {.callout-tip icon="false"}
## Individual exercise - warm up

Exercise [14.5](https://openintro-ims.netlify.app/decerr.html#chp14-exercises)

Exercise [17.2](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- parts a and c

Exercise [17.4](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) -- part c
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Normal approximation

## Normal distribution

```{r}
tibble(x = seq(-5, 5, by = 0.01)) %>%
  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%
  ggplot() + 
  geom_line(aes(x, normal))
```

$\Rightarrow$ unimodal, symmetric, thin tails -- bell-shaped

## HT using a normal approximation

![](images/lec-12/p-value.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

## CI using a normal approximation

![](images/lec-12/3-sigma.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

## Normal approximation

The normal distribution describes the variability of the different statistics

-   $\hat{p}$, $\bar{x}$, $\hat{\beta}$

-   simply look at all the histograms we have constructed from simulated samples (HT) and bootstrap samples (CI)!

. . .

**Classical approach**: instead of simulating the sampling distribution via simulation (HT) or bootstrapping (CI), we approximate it with a normal distribution.

## Normal approximation for $\bar{x}$

We have seen that if a numerical variable $X$ is normally distributed

$$
X\sim N(\mu, \sigma^2)
$$

then the sample average is also normally distributed

$$
\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

## Condition for the normality of $\bar{x}$ {.smaller}

In practice, we cannot assume that the variable $X$ is *exactly* normally distributed.

But as long as

1.  the sample is large, or

2.  the variable is *approximately* normal: unimodal, roughly symmetric and no serious outlier

$\bar{x}$ is well approximated by a normal distribution

$$
\bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

. . .

See the numerous histograms for case 3 (one mean) where the distribution of $\bar{x}$ always looks pretty normal.

## Normal approximation for $\hat{p}$ {.smaller}

If

1.  the observations are independent -- the **independence** condition

2.  $p$ is not extreme and $n$ is not small $(pn\ge 10 \text{ and } (1-p)n\ge 10)$ -- the **success-failure** condition

the distribution of $\hat{p}$ can be approximated by a normal distribution

$$
\hat{p} \sim N\left(p, \frac{p(1-p)}{n}\right)
$$

::: callout-important
## Success-failure condition for CI

For CI, we verify the success-failure condition using the sample proportion $\hat{p}$:

$$
\hat{p}n\ge 10 \text{ and } (1-\hat{p})n\ge 10
$$
:::

## Conditions are satisfied

```{r}
p <- 0.4; n <- 100 # conditions are satisfied: p*n>10 and (1-p)*n>10
results <- tibble(p_hat = numeric())
for(i in 1 : 5e3){
  sim     <- purrr::rbernoulli(n, p)
  p_hat   <- mean(sim)
  results <- results %>% add_row(p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01)
```

## Normal approximation is good

```{r}
tibble(p_hat = seq(0.2, 0.6, by = 0.01)) %>%
  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%
  ggplot() +
  geom_line(aes(x = p_hat, y = normal_approximation))
```

## 

::: callout-tip
## When the conditions are satisfied

When the conditions are satisfied, the normal distribution will be a good approximation. The classical and modern (simulation, bootstrap) approaches to statistical inference will give the same results.
:::

## Conditions are *not* satisfied

```{r}
p <- 0.025; n <- 100 # conditions are not satisfied: p*n<10 
results <- tibble(p_hat = numeric())
for(i in 1 : 5e3){
  sim     <- purrr::rbernoulli(n, p)
  p_hat   <- mean(sim)
  results <- results %>% add_row(p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat), binwidth = 0.01) + xlim(-0.05, 0.1)
```

## Normal approximation fails

```{r}
tibble(p_hat = seq(-0.05, 0.1, by = 0.01)) %>%
  mutate(normal_approximation = dnorm(p_hat, mean = p, sd = sqrt(p*(1-p)/n))) %>%
  ggplot() + geom_line(aes(x = p_hat, y = normal_approximation)) + xlim(-0.05, 0.1)
```

## 

::: callout-important
## When conditions are not satisfied

When the conditions are not satisfied, the normal distribution will not be a good approximation to the sampling distribution. In this case, we should not use the classical approach to statistical inference, but instead use simulation (HT) or bootstrap (CI).
:::

# The classical approach to HT and CI

## The classical approach

Step 1: we are interested in the distribution of the statistic under $H_0$.

-   **Modern approach**: *simulate* from this distribution

-   **Classical approach**: *approximate* this distribution with a normal distribution

## HT

Step 2: we want to compute the p-value

-   **Modern approach**: the p-value is the proportion of simulations with a statistic at least as extreme as that of the observed sample

-   **Classical approach**: the p-value is the *area under the curve* of the normal distribution that is at least as extreme as the observed statistic.

## What `R` does

`R` will compute the p-value for you. Here is what `R` does behind the scene:

![](images/lec-12/p-value.png){fig-align="center"}

## CI

Step 2: identify the upper and lower bounds of the CI

-   **Modern approach**: find the appropriate percentiles among the simulated values

-   **Classical approach**:find the appropriate percentiles of the normal approximation

## What `R` does

`R` will compute the upper and lower bounds for you. Here is what `R` does behind the scene:

![](images/lec-12/3-sigma.png){fig-align="center"}

## Case 1 -- one proportion

```{r}
n <- 1500 # sample size
x <- 780  # number of successes
prop.test(
  x, n,             # observed data
  p = 0.5,          # value in the null hypothesis
  conf.level = 0.99 # confidence level for CI
  )
```

## Comparison with simulation-based HT

The simulation-based HT yielded a p-value of 0.127.

::: callout-note
## A good normal approximation

When the conditions for the normal approximation are satisfied, the results based on simulations (modern) and the normal approximation (classical) will be similar.
:::

**Conditions**: independence, success-failure condition

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for one proportion

Suppose you interview 2000 US adults about their political preferences and 1200 of them say that they are democrat. What is the 95% confidence interval for $p$, the proportion of US adults who are democrats? What is the length of the interval?

What 95% CI do you obtain if 6000 out of 10000 US adults say they are democrat? What is its length?

Exercise [16.19](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- find the 95% CI. Are the conditions satisfied?

Exercise [16.21](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)

Exercise [16.25](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)
:::

```{r, echo = F}
countdown(minutes = 5)
```

## Case 2 -- two proportions

Consider the gender discrimination study.

```{r}
n_m <- 24; n_f <- 24 # sample sizes
x_m <- 14; x_f <- 21 # numbers of promotions
prop.test(c(x_m, x_f), c(n_m, n_f))
```

## Conditions

1.  Independence within groups (same as case 1)

2.  Independence between groups

3.  Success-failure condition for each group (10 successes and 10 failures in each group)

## Comparison with simulation-based HT

Using the simulation-based HT, we found a p-value of 0.0435.

::: callout-note
## A good normal approximation

When the conditions for the normal approximation are *not* satisfied, the results based on simulations (modern) and the normal approximation (classical) may differ.

A simulation-based HT will always give exact results. A HT based on the normal distribution will only give the same (exact) results when the conditions are satisfied.
:::

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for two proportions

Exercise [17.7](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)

Exercise [17.13](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)

Exercise [17.19](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)
:::

```{r, echo = F}
countdown(minutes = 6)
```

## Case 3 -- one mean

```{r}
d <- ggplot2::mpg
t.test(d$hwy, mu = 25)
```

## Conditions

1.  Independence

2.  Normality -- can be relaxed for larger samples $(n\ge30)$

. . .

::: callout-tip
## Statistics as an art

The normality assumption is vague. The most important feature of the sample to verify is the presence of outliers.

Rule of thumb: if $n<30$, there should not be any clear outlier; if $n\ge30$, there should not be any extreme outlier.
:::

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for one mean

Make a histogram and a boxplot of the variable. Are the conditions satisfied?

Construct a 99% CI for `hwy`.

Hint: run the command `help(t.test)` to access the help file of the function `t.test` and see what parameter determines the confidence level.
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Case 4 -- two means

There are two implementation; which on is more convenient depends on the structure of the data.

::: panel-tabset
## Two vectors

```{r}
d <- ggplot2::mpg
t.test(hwy ~ year, data = d)
```

## Formula

```{r}
d <- ggplot2::mpg
t.test(d$cty, d$hwy)
```
:::

## Conditions

1.  Independence within groups

2.  Independence between groups

3.  Normality in each group (same as case 3 -- one mean)

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for two means

What is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?

Are the conditions satisfied?
:::

```{r, echo = F}
countdown(minutes = 2)
```

## Case 4bis -- paired means

```{r}
d <- ggplot2::mpg
t.test(d$cty, d$hwy, paired = TRUE)
```

## Conditions

1.  Paired observations

2.  Independence between pairs

3.  Normality

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for paired means

What is the 99% CI for the difference in fuel efficiency on the highway and in the city? How long is this CI?

Are the conditions satisfied?
:::

```{r, echo = F}
countdown(minutes = 2)
```

. . .

::: callout-important
## Always pair the observations

If the data can paired, you should always do it! Pairing data yields an analysis that is more *powerful*:

-   narrower CI

-   smaller p-value
:::

## Case 5 -- regression

::: panel-tabset
## simple linear regression

```{r}
m <- lm(hwy ~ cty, data = mpg)
tidy(m)
```

## multiple linear regression

```{r}
m <- lm(hwy ~ cty + displ, data = mpg)
tidy(m)
```

$H_0: \beta_1 = 0$ when `displ` is included in the model

$H_a: \beta_1 \neq 0$ when `displ` is included in the model

$H_0: \beta_2 = 0$ when `cty` is included in the model

$H_a: \beta_2 \neq 0$ when `cty` is included in the model

## logistic regression

```{r}
d <- heart_transplant %>% mutate(survived_binary = survived == "alive")
m <- glm(survived_binary ~ age + transplant, family = "binomial", data = d)
tidy(m)
```
:::

## Conditions (LINE) -- linear regression

1.  Linearity

2.  Independence

3.  Normality

4.  Equal variability (homoskedasticity)

$\Rightarrow$ verify with a residual plot!

## 

::: {.callout-tip icon="false"}
## Individual exercise - the classical approach for regression

What condition(s) are violated by each of the following data sets (see next slide)?

Exercise [24.10](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises)

Exercise [24.13](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- parts a and b

Exercise [24.15](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- part b

Exercise [25.3](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)

Exercise [25.7](https://openintro-ims.netlify.app/inf-model-mlr.html#chp25-exercises)

Exercise [26.1](https://openintro-ims.netlify.app/inf-model-logistic.html)

Exercise [26.1](https://openintro-ims.netlify.app/inf-model-logistic.html)
:::

```{r, echo = F}
countdown(minutes = 10)
```

## Data sets

![](images/lec-12/conditions.png){fig-align="center"}

# Standard error

## Standard error

**Standard error (SE)**: standard deviation of the normal approximation

It measures the variability of the sample statistic.

-   $SE(\hat{p})=\sqrt{\frac{p(1-p)}{n}}$

-   $SE(\hat{p}_{diff})=\sqrt{\frac{p_1(1-p_1)}{n_1}+\frac{p_2(1-p_2)}{n_2}}$

-   $SE(\bar{x}) = \sqrt{\frac{\sigma^2}{n}}$

-   $SE(\bar{x}_{diff}) = \sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}$

-   $SE(\hat{\beta})$ has a complicated form.

::: callout-note
Note the role of the sample size!
:::

## Larger samples have a smaller SE

::: callout-note
## Sample size matters

Large $n$

$\Rightarrow$ small SE

$\Rightarrow$ normal approximation with small sd

$\Rightarrow$ normal approximation is more concentrated

$\Rightarrow$ tighter CI and smaller p-values.
:::

## 

::: {.callout-tip icon="false"}
## Individual exercise - sample size and CI

Exercise [16.13](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part e

Exercise [16.15](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- part b

Exercise [13.4](https://openintro-ims.netlify.app/foundations-mathematical.html#chp13-exercises) -- part d
:::

```{r, echo = F}
countdown(minutes = 1)
```

# Recap

## Recap

-   Normal approximation
-   Classical approach to statistical inference
    -   case 1 -- one proportion
    -   case 2 -- two proportions
    -   case 3 -- one mean
    -   case 4 -- two means
    -   case 4bis -- paired means
    -   case 5 -- regression
-   Standard error
