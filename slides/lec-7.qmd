---
title: "Logistic Regression"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)
library(openintro)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   

## Recap {.smaller}

::: incremental
-   Simple and multiple lrm

    -   numerical response

-   model selection

    -   overall criterion

    -   predictive performance

    -   stepwise procedures
:::

## Outline

-   Modeling a binary response
-   Logistic regression
-   One predictor
-   Multiple predictors

# Modeling a binary response

## Example -- [Stanford University Heart Transplant Study](https://www.jstor.org/tc/accept?origin=%2Fstable%2Fpdf%2F2285502.pdf%3Fcasa_token%3DtsfisPZrE4QAAAAA%3AjmSc4CpJPjkyTwKwoUdROLd5ZMHodUmGdclzMIXQGWUXmT0FQ7zbwLympmqvjcGrsbH-iU2qfA3DYSY5oxC4E0qSbRvoQN4hAlB5Jy01zwHbvwVrHPU&is_image=False)

::: columns
::: {.column width="60%"}
-   Goals: to determine whether an experimental heart transplant program increases lifespan

-   observations: patients

-   response: survival after 5 years (binary)

-   predictors: age, prior surgery, waiting time for transplant.
:::

::: {.column width="40%"}
![](images/lec-7/surgery.jpg){fig-align="center"}
:::
:::

## Data

```{r}
d <- heart_transplant
d
```

## Effect of age on survival

::: panel-tabset
## geom_point

```{r}
d %>%
  ggplot(aes(x = age, y = survived)) +
  geom_point()
```

## geom_jitter

```{r}
d %>%
  ggplot(aes(x = age, y = survived)) +
  geom_jitter(     # add random noise to point location
    width = 0,     # horizontal jitter
    height = 0.05, # vertical jitter
    alpha = 0.5    # transparency
    )
```
:::

## Binary response

-   Technical problem: levels of the response are labels

    -   can't fit a regression model to words, only to numbers

-   use `mutate` to create a binary variable (either 0 or 1)

```{r}
d <- d %>%
  mutate(is_alive = if_else(survived == "alive", 1, 0))
```

## 

```{r}
ggplot(data = d, aes(x = age, y = is_alive)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5)
```

## Linear regression?

Intuitively, `age` should be a good predictor of survival.

. . .

Let us fit a linear regression model

$$
\text{is_alive} \approx \beta_0 + \beta_1 age
$$

```{r}
m_lrm <- lm(is_alive ~ age, data = d)
```

## 

```{r}
ggplot(data = d, aes(x = age, y = is_alive)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5) +
  geom_abline(intercept = 0.80955, slope = -0.01205)
```

## Problems with linear regression

-   The response is not continuous

    -   unlike fuel consumption, tree volume or newborn weight

-   nonsensical predictions, even for reasonable value of `age`.

## 

::: {.callout-tip icon="false"}
## Group exercise - limitation of linear regression

What is the prediction for a 70-year-old patient?
:::

```{r, echo = F}
countdown(minutes = 2)
```

# Generalized linear models

## GLMs {.smaller}

**Generalized linear models** (GLMs -- STA310) extend the linear regression framework to settings where the response variable is *restricted*:

::: incremental
-   **logistic regression** with binary response

-   multinomial regression with nominal (categorical) response, e.g. voting in multiparty systems

-   ordinal regression with ordinal (categorical) response, e.g. course letter grade

-   poisson regression with count response, e.g. number of children

-   gamma regression with positive response variable, e.g. mpg, insurance costs

-   beta regression with continuous response between 0 and 1 (percentage), e.g. cancer rates in counties
:::

# Logistic regression

## Logistic regression

-   Example of a *generalized linear model*

-   We'll focus on implementation and interpretation

-   Useful for inference project (not the prediction project)

**Logistic regression**: model the probability of success $p$ based on a set of predictors $X_1, \dots, X_p$.

## Binary outcome and probability

Let $Y_i$ denote the response of person $i$.

The logistic regression model assumes that

$$
Y_i = 
\begin{cases}
1 \text{ (success)}, \text{with probability } p_i, \\
0 \text{ (failure)}, \text{with probability } 1-p_i.
\end{cases}
$$

where $p_i$ denotes the probability of success (survival) of person $i$.

## Modeling a probability with linear regression? {.smaller}

We saw that we cannot model $p$ with a lrm ($p \approx \beta_0 + \beta_1\text{age}$)

-   $0\le p_i\le1$

-   ...but $\beta_0 + \beta_1 \text{age}$ may be negative or larger than 1!

    -   in fact, the regression line extends infinitely in either either direction

::: callout-warning
Binary variables should **not** be modeled using linear regression!
:::

## Modeling a probability with logistic regression

Intuitively, an older patient should have a smaller probability of surviving.

$\Rightarrow$ We want a model that associates a smaller probability $p$ for a patient with a larger `year` variable.

. . .

$\Rightarrow$ We need to find a way to go from `age` (could be any number) to $p$ (between 0 and 1)

## The logit transformation

Consider the **logit transformation**

$$
p = \dfrac{e^{\mu}}{1+e^{\mu}}
$$

where $\mu$ can be any number.

## 

```{r}
tibble(mu = seq(-7.5, 7.5, by = 0.01)) %>%
  mutate(p = exp(mu)/(1+exp(mu))) %>%
  ggplot() + 
  geom_line(aes(x = mu, y = p))
```

Note that $\dfrac{e^{\mu}}{1+e^{\mu}}$ is bounded between 0 and 1.

Moreover, larger values of $\mu$ will give larger $p$, and smaller $\mu$ will give smaller $p$.

## 

::: callout-tip
## Group exercise - logit transformation

-   What value of $p$ corresponds to

    -   $\mu = 0.5$?

    -   $\mu = 2$?

    -   $\mu = -2$?

-   What value of $\mu$ gives

    -   $p=0.5$?

    -   $p=0.9$?

    -   $p=0.1$?
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Modeling $\mu$ {.smaller}

We can now simply model $\mu$ using a simple lrm with age

$$
\mu \approx \beta_0 + \beta_1 \text{age}
$$

and then transform $\mu$ into $p$ using the logit transformation

$$
p = \dfrac{e^{\mu}}{1+e^{\mu}}
$$

. . .

Putting everything together gives the **logistic regression model**

$$
p = \dfrac{e^{\mu}}{1+e^{\mu}} \approx \dfrac{e^{\beta_0 + \beta_1 \text{age}}}{1+e^{\beta_0 + \beta_1 \text{age}}}
$$

## 

::: callout-note
## Alternative formulation

The formulation

$$
\log\left(\frac{p}{1-p}\right) \approx \beta_0 +\beta_1 \text{age}
$$

is also widely to describe the logistic regression model. This formulation is equivalent to that used on the previous slide.
:::

## Maximum likelihood estimates

How are the unknown coefficients $\beta_0$ and $\beta_1$ estimated?

When we fit a logistic regression model with `R`, the so-called *maximum likelihood estimates* (MLE[^1]) are returned.

[^1]: MLE are extremely popular in statistics.

# Implementation

## `glm`

We fit a logistic regression model in `R` with the command `glm` (not `lm`)

```{r}
#| code-line-numbers: "1|2|3|4|"
m <- glm( # glm, not lm
  is_alive ~ age, 
  family = binomial, # logistic regression
  data = d
  )
m
```

## `R` output {.smaller}

For the moment simply focus on

-   coefficient estimates

-   AIC (no (adjusted-) $R^2$) for model selection

::: callout-warning
## glm not lm

To fit a logistic regression model in `R`, use the command `glm` (not `lm`) with the argument `family = binomial`.
:::

## Interpretation

A positive coefficient estimate indicates that a higher value of the predictor is associated with a higher probability of success; and vice-versa for a smaller value.

In our case, the coefficient estimate for age (`r signif(m$coefficients[2], 2)`) is negative, so the model indicates that older participants have a smaller probability of surviving.

## Visualizing the regression curve

```{r}
d_augm <- augment(m, type.predict = "response")
d_augm # .fitted is equivalent to p_hat 
```

## 

```{r}
d_augm %>%
  ggplot(aes(x = age)) +
  geom_jitter(aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +
  geom_line(aes(y = .fitted), col = "maroon", size = 2) # .fitted is equivalent to p_hat 
```

## Extending the regression curve

```{r}
#artificial data with larger range (0 to 100)
d_artificial <- tibble(age = seq(0, 100, by = 0.1))
p_hat <- predict(m, newdata = d_artificial, type = "response")

d_artificial <- mutate(d_artificial, p_hat = p_hat)
d_artificial
```

## 

```{r}
ggplot(mapping = aes(x = age)) +
  geom_jitter(data = d, aes(y = is_alive), width = 0, height = 0.05, alpha = 0.5) +
  geom_line(data = d_artificial, aes(y = p_hat), col = "maroon", size = 2)
```

## Multiple logistic regression

Based on what we know about multiple lrm, it is easy to extend the previous framework to **multiple** logistic regression:

$$
p = \dfrac{e^{\mu}}{1+e^{\mu}}
$$

where

$$
\mu \approx \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
$$

## Implementation in `R`

```{r}
m2 <- glm( # glm, not lm
  is_alive ~ age + transplant, 
  family = binomial, # logistic regression
  data = d
  )
m2
```

## 

::: {.callout-tip icon="false"}
## Group exercise - interpretation

What is the interpretation of the coefficient estimates for `age` and `transplant`?
:::

```{r, echo = F}
countdown(minutes = 2)
```

## 

::: {.callout-tip icon="false"}
## Group exercise - interpretation

Exercise [9.5](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)
:::

```{r, echo = F}
countdown(minutes = 4)
```

## 

::: {.callout-tip icon="false"}
## Group exercise - implementation

Exercise [9.3](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)

**You do not need to do parts a and b.**

Simply fit the two models in `R`. The `possum` data used in this exercise can be found in the `openintro` `R` package. You should obtain the same coefficient estimates as in the book, though with an opposite sign.

Save these models; you will need them later!
:::

```{r, echo = F}
countdown(minutes = 4)
```

## Artificial data

```{r}
d_artificial <- expand.grid(
  age = seq(0, 100, by = 0.1),
  transplant = c("treatment", "control")
  ) %>%
  as_tibble() %>%
  arrange(age)
d_artificial
```

## Visualization

```{r}
p_hat <- predict(m2, d_artificial, type = "response")
d_artificial %>%
  mutate(p_hat = p_hat) %>%
  ggplot() +
  geom_line(aes(x = age, y = p_hat, col = transplant))
```

## Prediction

```{r}
d_augm <- augment(m2, type.predict = "response") %>%
  mutate(is_alive_hat = if_else(.fitted < 0.5, 0, 1)) %>%
  select(is_alive, age, transplant, .fitted, is_alive_hat)
d_augm
```

## Confusion matrix

```{r}
d_augm %>%
  select(is_alive, is_alive_hat) %>%
  table()
```

The model got $71+8=79$ observations right out of $71+4+20+8=103$; so its **accuracy** is

$$
\frac{79}{103} \approx 77\%
$$

To estimate the prediction accuracy on new data, simply use the holdout method or cross-validation.

## Model selection

-   AIC, BIC (not adjusted-$R^2$)

-   the holdout method using *prediction* *accuracy* (not RMSE)

-   cross-validation using *prediction* *accuracy* (not RMSE)

::: callout-tip
## AIC and BIC

With AIC and BIC, lower is better!

$$
AIC = 2p - \text{Goodness of fit}, \qquad BIC = p\ln(n) - \text{Goodness of fit}
$$
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - model comparison with AIC

Go back to the two models you fitted for exercise [9.3](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises). What are their respective AIC? Which model is better?

Similarly, compare the AIC of the simple (`age`) and multiple (`age + implant`) logistic regression models for the heart transplant study. Which model is better?
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - stepwise model selection with AIC

Exercise [9.7](https://openintro-ims.netlify.app/model-logistic.html#chp09-exercises)
:::
