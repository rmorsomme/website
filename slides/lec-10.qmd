---
title: "Inference for means"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)
library(openintro)
library(kableExtra)
library(janitor)
library(patchwork)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   Homework 6 due **Thursday**

-   Inference project

    -   Start to think about data or a question that interests you.

-   Remaining homework due on Mon/Thu instead of Wed/Sun?

## Announcements {.smaller}

Midterm survey

-   review holdout and CV

-   more live coding (?)

-   more individual assignments in class

-   OH before Sunday's HW $\Rightarrow$ HW due on Monday (OH Monday morning)

-   website

    -   external links will open a new window

    -   slide changes will not be pushed to browser history

-   see these [steps](https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf) to print the slides

## Recap

-   One proportion

    -   HT via simulation

    -   CI via bootstrap

-   Two proportions

    -   HT via simulation

    -   CI via bootstrap

## Outline

-   The normal distribution
-   One mean (case 3)
-   Two means (case 4)
-   Paired means (case 3)

# The normal distribution

## The normal distribution

The distribution of a numerical variable is often modeled with a **normal distribution**.

```{r}
tibble(x = seq(-5, 5, by = 0.01)) %>%
  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%
  ggplot() + 
  geom_line(aes(x, normal))
```

## Two parameters

The mean ($\mu$) -- location

The standard deviation ($\sigma$) -- spread

$$
N(\mu, \sigma)
$$ The *standard* normal distribution $N(\mu = 0, \sigma = 1)$ is plotted on the previous slide.

## 

::: {.callout-tip icon="false"}
## Individual exercise - normal distribution

Modify the following `R` code to plot the two normal distributions

-   $N(\mu = 0, \sigma = 0.5)$
-   $N(\mu = 1, \sigma = 0.5)$

```{r, eval = FALSE}
tibble(x = seq(-5, 5, by = 0.01)) %>%
  mutate(normal = dnorm(x, mean = 0, sd = 1)) %>%
  ggplot() + 
  geom_line(aes(x, normal))
```
:::

```{r, echo = F}
countdown(minutes = 3)
```

## Property I

If the variable $X\sim N(\mu, \sigma)$, then for any number $a$ and $b$

$$
X + a \sim N(\mu+a, \sigma),
$$

$$
bX \sim N(b\mu, b \sigma)
$$

and

$$
bX + a \sim N(b\mu+a, b\sigma).
$$

## Property II

If the variables $X\sim N(\mu_1, \sigma_1)$ and $Y\sim N(\mu_2, \sigma_2)$ are independent, then

$$
X+Y \sim N(\mu_{tot} = \mu_1 + \mu_2, \sigma_{tot} = \sqrt{\sigma_1^2 + \sigma_2^2})
$$

## Alternative parameterization

$N(\mu, \sigma^2)$, where $\sigma^2$ is the variance.

. . .

Property I gives

$$
bX + a \sim N(b\mu + a, b^2 \sigma^2)
$$

and property II gives

$$
X+Y \sim N(\mu_{tot} = \mu_1 + \mu_2, \sigma_{tot}^2 = \sigma_1^2 + \sigma_2^2)
$$

. . .

The mean of the sum is the sum of the means.

The variance of the sum is the sum of the variances.

## 

::: {.callout-tip icon="false"}
## Individual exercise - normal property

Suppose you have a sample with $n=10$ independent observations in which each observation follows a standard normal distribution. That is,

$$
X_1 \sim N(0,1), X_2 \sim N(0,1), \dots, X_{10} \sim N(0, 1). 
$$

Use the normal properties to derive the distribution of the sample average

$$
\bar{x} = \dfrac{X_1 + X_2 + \dots + X_{10}}{10}
$$

What happens to the sd/variance of $\bar{x}$ as the sample size $n$ increases?
:::

```{r, echo = F}
countdown(minutes = 4)
```

## 

::: {.callout-tip icon="false"}
## Individual exercise -

Exercise [19.5](https://openintro-ims.netlify.app/inference-one-mean.html#chp19-exercises) -- for our purpose in part b **standard error** is equivalent to **standard deviation**.
:::

```{r, echo = F}
countdown(minutes = 2)
```

## The 68, 95, 99.7 rule

![](images/lec-10/68-95-99.png){fig-align="center"}

# One mean

## Setup

**Population parameter**: mean $\mu$

**Sample statistic**: sample average $\bar{x}$

. . .

**Hypothesis testing**:

-   $H_0:\mu=\mu_0$ where $\mu_0$ is a fixed number

-   $H_a:\mu\neq \mu_0$

**Confidence interval**: range of plausible values for $\mu$.

## 

::: {.callout-tip icon="false"}
## Individual exercise - statistic and parameter

Exercise [19.1](https://openintro-ims.netlify.app/inference-one-mean.html#chp19-exercises)
:::

```{r, echo = F}
countdown(minutes = 1)
```

## Example -- car price

What is the average price of a car on Awesome Car?

```{=tex}
\begin{align*}
\bar{x}
& = \dfrac{18,300+20,100+9,600+10,700+27,000}{5} \\
& = 17,140
\end{align*}
```
![](images/lec-10/cars.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

# Hypothesis test via simulation

## Normal model

Let us assume that car prices follow a normal distribution with mean $\mu$ and sd $\sigma$:

$$
\text{price} \sim N(\mu, \sigma)
$$

$H_0:\mu=10,000$

$H_a:\mu\neq 10,000$

. . .

-   Simulate many samples under $H_0$.

-   Determine if the observed data could have plausibly arisen under $H_0$.

## Problem

To simulate from the normal distribution, we need to specify both the mean $\mu$ and the sd $\sigma$.

**Problem**: under $H_0$ we only know that $\mu = 10,000$; we do not know what value to use for $\sigma$!

. . .

$\Rightarrow$ We cannot conduct a hypothesis test via simulation!

# Confidence interval via bootstrap

## Bootstrapping

Sample with repetition from the observed sample to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ CI

## 

![](images/lec-10/bootstrap.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
sample_observed <- tibble(price = c(18300, 20100, 9600, 10700, 27000))
```

```{r}
sample_bootstrap <- function(data){ # same function as before
  n                <- nrow(data)
  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)
  return(sample_bootstrap)
}
```

```{r}
set.seed(1)
sample_bootstrap(sample_observed)
```

## 

```{r, cache=TRUE}
results <- tibble(stat_boot = numeric())
set.seed(0)
for(i in 1 : 1e3){
  d_boot    <- sample_bootstrap(sample_observed) # bootstrap sample
  stat_boot <- mean(d_boot$price)                # bootstrap statistic
  results   <- results %>% add_row(stat_boot) 
}
```

```{r}
quantile(results$stat_boot, c(0.025, 0.975)) # 95% CI
```

```{r}
quantile(results$stat_boot, c(0.01 , 0.99 )) # 98% CI (wider)
```

## 

```{r}
ggplot(results) + 
  geom_histogram(aes(stat_boot), binwidth = 1350) + 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.025, 0.975)), col = "gold1", size = 2) + # 95% CI 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.01 , 0.99)), col = "maroon", size = 2) # 98% CI
```

## 

::: {.callout-tip icon="false"}
## Group exercise - Bootstrap CI

Exercises [19.11](https://openintro-ims.netlify.app/inference-one-mean.html#chp19-exercises) and [19.13](https://openintro-ims.netlify.app/inference-one-mean.html#chp19-exercises)
:::

```{r, echo = F}
countdown(minutes = 3)
```

## CI for the standard deviation $\sigma$

```{r}
#| code-line-numbers: "|5|"
results <- tibble(stat_boot = numeric())
set.seed(0)
for(i in 1 : 1e3){
  d_boot    <- sample_bootstrap(sample_observed)
  stat_boot <- sd(d_boot$price  )              # sd instead of mean
  results   <- results %>% add_row(stat_boot) 
}
```

## 

```{r}
ggplot(results) + 
  geom_histogram(aes(stat_boot)) + 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.05 , 0.95 )), col = "gold1", size = 2) + # 90% CI 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.01, 0.99)), col = "maroon", size = 2) # 98% CI
```

## Any statistic, but not any sample

::: callout-note
## Bootstrap any statistic

You can bootstrap almost any statistic you want!
:::

::: callout-note
## The quality of the sample matters!

A statistical analysis can only be as good as the sample collected. Here, a sample of $5$ cars contains very limited information about the population; it would be useful to have a larger sample.
:::

# Two means

## Setup

A **population** divided in two groups.

**Population parameter**: difference in mean

$$
\mu_{diff}=\mu_1-\mu_2
$$

**Sample statistic**: difference in proportion in the sample

$$
\bar{x}_{diff}=\bar{x}_1-\bar{x}_2
$$

. . .

$H_0:\mu_{diff}=0$ (no difference between the two groups)

$H_a:\mu_{diff}\neq0$

## 

::: {.callout-tip icon="false"}
## Individual exercise -

Exercise [20.1](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises)
:::

```{r, echo = F}
countdown(minutes = 1, seconds = 30)
```

## Example -- two class exams

A professor considers two designs for an exam. Are the two types of exam equally difficult?

```{r}
d <- openintro::classdata %>% 
  filter(lecture %in% c("a", "b")) %>%
  rename(score = m1, exam = lecture)
d
```

## 

```{r}
ggplot(d, aes(score, exam, col = exam)) + 
  geom_boxplot() +
  geom_jitter(width = 0, height = 0.1) # add vertical jitter
```

## 

::: {.callout-tip icon="false"}
## Group exercise - two proportions

Compute are $\bar{x}_a$, $\bar{x}_b$ and $\bar{x}_{diff}$? Do you *intuitively feel* that the data provide convincing evidence that the two exams are not equally difficult?

Hint: use the command `summarize`.

```{r, eval = FALSE}
d <- openintro::classdata %>% 
  filter(lecture %in% c("a", "b")) %>%
  rename(score = m1, exam = lecture)
```
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Hypothesis test via simulation

## 

$H_0:\mu_{diff}=0$

$H_a:\mu_{diff}\neq 0$

. . .

-   Simulate many samples under $H_0$ (no difference)

-   Determine if the observed data could have plausibly arisen under $H_0$

## Simulating under $H_0$

Under $H_0$, there is no difference between the two exams

$\Rightarrow$ the score is independent of the type of exam

$\Rightarrow$ randomly re-assign the scores independently of the exam type.

. . .

::: callout-tip
This is very similar to the procedure for two proportions.
:::

## 

![](images/lec-10/simulation.png) Source: [IMS](https://www.openintro.org/book/ims/)

## 

```{r}
d_sim <- d %>% mutate(score = sample(score)) # shuffle the scores
d_sim
```

## Function for computing the test statistic

```{r}
#| code-line-numbers: "|3,4|5|"
compute_x_diff <- function(data){
  x_bar <- data %>%
    group_by(exam) %>%
    summarize(x_bar = mean(score))
  x_diff_bar <- x_bar$x_bar[1] - x_bar$x_bar[2]
  return(x_diff_bar)
}
compute_x_diff(d_sim)
```

## For-loop for simulating under $H_0$

```{r, cache=TRUE}
# Setup
results   <- tibble(x_diff_bar = numeric())

# Simulations
set.seed(0)
for(i in 1 : 1e3){
  d_sim <- d %>% mutate(score = sample(score)) # simulate under H0
  x_diff_bar <- compute_x_diff(d_sim) # test statistic
  results <- results %>% add_row(x_diff_bar)
}
```

## Sampling distribution

```{r}
x_diff_obs <- compute_x_diff(d)
x_diff_obs
ggplot(results) + 
  geom_histogram(aes(x_diff_bar)) +
  geom_vline(xintercept = x_diff_obs, col = "maroon", size = 2)
```

## p-value

-   the probability that $\bar{x}_{diff}^{sim}\ge$ `r signif(x_diff_obs,2)` or $\bar{x}_{diff}^{sim}\le$ `r -signif(x_diff_obs,2)`.

```{r}
results %>%
  mutate(is_more_extreme = x_diff_bar >= x_diff_obs | x_diff_bar <= -x_diff_obs) %>%
  summarize(p_value = mean(is_more_extreme))
```

## Conclusion

Using the usual significance level $\alpha = 0.05$, we **fail to reject** the null hypothesis

-   it is plausible that the observed difference in scores is due to random luck
-   the difference is **not statistically significant**.

## 

::: {.callout-tip icon="false"}
## Group exercise - HT

Exercises [20.3](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises) and [20.7](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises)
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Confidence interval via bootstrap

## Bootstrap CI

Same idea as before: sample with repetition from the observed data to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ CI

## 

![](images/lec-10/bootstrap2.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
sample_observed_a <- d %>% filter(exam == "a")
sample_observed_b <- d %>% filter(exam == "b")

set.seed(0)
sample_bootstrap(sample_observed_a) # bootstrap sample
sample_bootstrap(sample_observed_b) # bootstrap sample
```

## 

```{r}
#| code-line-numbers: "|3,4|5|"
results <- tibble(x_diff_bar = numeric())
for(i in 1 : 1e3){
  d_boot_a   <- sample_bootstrap(sample_observed_a) # bootstrap sample
  d_boot_b   <- sample_bootstrap(sample_observed_b) # bootstrap sample
  x_diff_bar <- compute_x_diff(rbind(d_boot_a, d_boot_b)) # bootstrap statistic
  results    <- results %>% add_row(x_diff_bar)
}
```

```{r}
quantile(results$x_diff_bar, c(0.05 , 0.95 )) %>% signif(2) # 90% CI
quantile(results$x_diff_bar, c(0.025, 0.975)) %>% signif(2) # 95% CI
```

## 

```{r}
ggplot(results) +
  geom_histogram(aes(x_diff_bar))  + 
  geom_vline(xintercept = quantile(results$x_diff_bar, c(0.05 , 0.95 )), col = "gold1", size = 2) + # 90% CI 
  geom_vline(xintercept = quantile(results$x_diff_bar, c(0.025, 0.975)), col = "maroon", size = 2) # 95% CI
```

## CI and HT

::: callout-note
## Two sides of the same coin

The two CIs include 0. This indicates that 0 is a plausible value for the difference in mean in the population. This is exactly what the HT concluded!
:::

## 

::: {.callout-tip icon="false"}
## Individual exercises - CI

Exercise [20.5](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises) part a only
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Paired means

## Paired data

**Paired data**: two groups in which each observation in one group has exactly one corresponding observation in the other group.

Example: pre/post-evaluations; supermarket items; batteries and electronic devices; tires and cars.

. . .

::: callout-note
## Paired data are like one mean data

Paired data can be analyzed like the one-mean case (case 3)!
:::

## Example -- tire brand

We want to compare the longevity of two brands of tire. The response variable is tire tread after 1000 miles.

25 cars drove 1000 miles. On each car, one tire was from Smooth Turn and another one was from Quick Spin. The other two tires were baseline tires.

## 

```{r tiredata}
set.seed(1)
bias <- runif(25, -0.01, 0.01)
brandA <- rnorm(25, bias + 0.310, 0.003)
brandB <- rnorm(25, bias + 0.308, 0.003)
car <- c(paste("car", 1:25))
miny <- min(brandA, brandB) - .003
maxy <- max(brandA, brandB) + .003
tires <- tibble(
  tread = c(brandA, brandB),
  car = rep(car, 2),
  brand = c(rep("Smooth Turn", 25), rep("Quick Spin", 25))
) %>%
  arrange(car)
head(tires)
```

## 

```{r, echo = FALSE}
orig_means <- tires %>%
  group_by(brand) %>%
  summarize(mean_tread = round(mean(tread), 3)) %>%
  mutate(
    mean_label = c("bar(x)[QS]", "bar(x)[ST]"),
    mean_label = paste(mean_label, "==", mean_tread)
  )
ggplot(tires, aes(x = brand, y = tread, 
                  color = brand, shape = brand)) +
  geom_boxplot(show.legend = FALSE, 
               outlier.shape = "triangle") +
  geom_text(aes(label = car),
    color = "grey",
    hjust = rep(c(-0.15, 1.3), each = 25),
    show.legend = FALSE, size = 4
  ) +
  geom_line(aes(group = car), color = "grey", size = 0.25) +
  geom_point(show.legend = FALSE) +
  geom_text(
    data = orig_means, 
    aes(label = mean_label, y = 0.318), 
    parse = TRUE, show.legend = FALSE
    ) +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"])) +
  labs(
    x = "Tire brand",
    y = NULL,
    title = "Original data"
  )
```

Source: [IMS](https://www.openintro.org/book/ims/)

## 

::: {.callout-tip icon="false"}
## Individual exercise - paired data

Do you *intuitively feel* that the tire data provide convincing evidence that one tire is more durable than the other?

Exercises [21.1](https://openintro-ims.netlify.app/inference-paired-means.html#chp21-exercises), [21.3](https://openintro-ims.netlify.app/inference-paired-means.html#chp21-exercises) and [21.5](https://openintro-ims.netlify.app/inference-paired-means.html#chp21-exercises)
:::

```{r, echo = F}
countdown(minutes = 4)
```

## CI via booststrap

Exactly the same as with one mean (case 3).

. . .

```{r}
tires_diff <- tires %>% 
  pivot_wider(names_from = brand, values_from = tread) %>%
  mutate(tread_diff = `Smooth Turn` - `Quick Spin`) %>%
  select(car, tread_diff)
head(tires_diff)
```

. . .

Simply construct a CI for the variable `tread_diff` with bootstrap samples.

## 

::: {.callout-tip icon="false"}
## Group exercise - CI for paired data

Use the `R` code from the one-mean case (case 3) to construct a CI for the difference in tire tread.
:::

```{r, echo = F}
countdown(minutes = 10)
```

## HT via simulation

$H_0:\mu_{diff}=0$

$H_a:\mu_{diff}\neq 0$

. . .

-   Simulate many samples under $H_0$ (no difference)

-   Determine if the observed data could have plausibly arisen under $H_0$

## Simulating under $H_0$

Under $H_0$, there is no difference between the two tire brands

$\Rightarrow$ tire tread is independent of tire brand

$\Rightarrow$ randomly re-assign tire tread independently of tire brand.

::: callout-note
## Re-assign *within*

The re-assignment happens *within* a car; either switch the two values or keep the original allocation.
:::

## Re-assigning two cars

```{r echo=FALSE}
set.seed(2)
permdata <- tires %>%
  filter(car %in% c("car 2", "car 3")) %>%
  group_by(car) %>%
  mutate(random_brand = sample(brand))
origplot <- permdata %>%
  ggplot(aes(x = brand, y = tread,
            color = brand, shape = brand)) +
  geom_line(aes(group = car), color = "grey") +
  geom_point(size = 3, show.legend = FALSE) +
  geom_text(aes(label = car), color = "darkgrey", 
            hjust = rep(c(-0.15, 1.3), each = 2),
            show.legend = FALSE, size = 6) +
  ylim(c(miny, maxy)) + 
  labs(
    x = NULL,
    y = NULL,
    color = NULL, shape = NULL,
    title = "Original data"
  ) +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"]))

shuffbrand <- permdata %>%
  ggplot(aes(x = random_brand, y = tread, 
             color = random_brand, shape = random_brand)) +
  geom_line(aes(group = car), color = "grey") +
  geom_point(size = 3) +
  geom_text(aes(label = car),color = "darkgrey", 
            hjust = rep(c(-0.15,1.3), each = 2),
            show.legend = FALSE, size = 6) +
  ylim(c(miny, maxy)) +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"])) +
  theme(legend.position = "none") + 
  labs(
    x = NULL,
    y = NULL,
    color = NULL, shape = NULL,
    title = "Shuffled data"
  )
origplot + shuffbrand
```

## Re-assigning all cars

```{r, echo=FALSE}
set.seed(0)
permdata <- tires %>%
  group_by(car) %>%
  mutate(random_brand = sample(brand))
origplot <- permdata %>%
  ggplot(aes(x = brand, y = tread,
            color = brand, shape = brand)) +
  geom_line(aes(group = car), color = "grey") +
  geom_point(size = 3, show.legend = FALSE) +
  geom_text(aes(label = car), color = "darkgrey", 
            hjust = rep(c(-0.15, 1.3), each = 25),
            show.legend = FALSE, size = 4) +
  ylim(c(miny, maxy)) + 
  labs(
    x = NULL,
    y = NULL,
    color = NULL, shape = NULL,
    title = "Original data"
  ) +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"]))
shuffbrand <- permdata %>%
  ggplot(aes(x = random_brand, y = tread, 
             color = random_brand, shape = random_brand)) +
  geom_line(aes(group = car), color = "grey") +
  geom_point(size = 3) +
  geom_text(aes(label = car),color = "darkgrey", 
            hjust = rep(c(-0.15,1.3), each = 25),
            show.legend = FALSE, size = 4) +
  ylim(c(miny, maxy)) +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"])) +
  theme(legend.position = "none") + 
  labs(
    x = NULL,
    y = NULL,
    color = NULL, shape = NULL,
    title = "Shuffled data"
  )
origplot + shuffbrand
```

## 

::: {.callout-tip icon="false"}
## Individual exercise - simulated difference

Exercise [21.9](https://openintro-ims.netlify.app/inference-paired-means.html#chp21-exercises)
:::

```{r, echo = F}
countdown(minutes = 2)
```

## `R` function to shuffle data

```{r}
shuffle_data <- function(data){
  tires %>%
  group_by(car) %>%
  mutate(tread = sample(tread))
}

set.seed(0)
tires_shuffled <- shuffle_data(tires)
head(tires)
head(tires_shuffled)
```

## `R` function for computing the test statistic

```{r}
compute_stat <- function(data){
  
  data %>% 
    pivot_wider(names_from = brand, values_from = tread) %>%
    mutate(tread_diff = `Smooth Turn` - `Quick Spin`) %>%
    ungroup() %>%
    summarise(x_diff_bar = mean(tread_diff)) %>%
    pull(x_diff_bar)
  
}
compute_stat(tires_shuffled)
```

## For-loop for simulating under $H_0$

```{r, cache=TRUE}
# Setup
results   <- tibble(stat_sim = numeric())

# Simulations
set.seed(0)
for(i in 1 : 1e3){
  d_sim    <- shuffle_data(tires) # simulate under H0
  stat_sim <- compute_stat(d_sim) # test statistic
  results  <- results %>% add_row(stat_sim)
}
```

## Sampling distribution

```{r}
stat_obs <- compute_stat(tires)
stat_obs
ggplot(results) + 
  geom_histogram(aes(stat_sim)) +
  geom_vline(xintercept = stat_obs, col = "maroon", size = 2)
```

## p-value and conclusion

-   the probability that $\bar{x}_{diff}^{sim}\ge$ `r signif(stat_obs,2)` or $\bar{x}_{diff}^{sim}\le$ `r -signif(stat_obs,2)`.

```{r}
results %>%
  mutate(is_more_extreme = stat_sim >= stat_obs | stat_sim <= -stat_obs) %>%
  summarize(p_value = mean(is_more_extreme))
```

Using the usual significance level $\alpha = 0.05$, we **reject** $H_0$.

## 

::: {.callout-tip icon="false"}
## Individual exercise - HT

Exercise [21.11](https://openintro-ims.netlify.app/inference-paired-means.html#chp21-exercises)
:::

```{r, echo = F}
countdown(minutes = 2)
```

# To pair or not to pair?

## 

::: callout-important
## Always pair

If the data can paired, you should always do it! Pairing data yields an analysis that is more *powerful*:

-   narrower CI

-   smaller p-values
:::

## HT for two means (case 4)

Let us conduct a hypothesis test for the tire data, but this time without pairing the data.

$\Rightarrow$ This is simply a **hypothesis test for two means**.

## Larger p-value

```{r}
compute_x_diff <- function(data){
  x_bar <- data %>%
    group_by(brand) %>%
    summarize(x_bar = mean(tread))
  x_diff_bar <- x_bar$x_bar[1] - x_bar$x_bar[2]
  return(x_diff_bar)
}
compute_x_diff(tires)
```

```{r, cache = TRUE}
results   <- tibble(x_diff_bar = numeric())
set.seed(0)
for(i in 1 : 1e3){
  d_sim      <- tires %>% mutate(tread = sample(tread)) # simulate under H0
  x_diff_bar <- compute_x_diff(d_sim) # test statistic
  results    <- results %>% add_row(x_diff_bar)
}
```

```{r}
x_diff_obs <- compute_x_diff(tires)
results %>%
  mutate(is_more_extreme = abs(x_diff_bar) >= abs(x_diff_obs)) %>%
  summarize(p_value = mean(is_more_extreme))
```

The p-value is larger than $\alpha=0.05$; we fail to reject the null hypothesis.

## 

```{r}
ggplot(results) + 
  geom_histogram(aes(x_diff_bar)) +
  geom_vline(xintercept = x_diff_obs, col = "maroon", size = 2)
```

## 

::: {.callout-tip icon="false"}
## Group exercise - wider CI

Use the `R` code from the two-mean case (case 4) to construct a CI for the difference in tire tread (the observations are not paired).

You should obtain a wider interval.
:::

```{r, echo = F}
countdown(minutes = 10)
```

# Recap

## Recap

-   The normal distribution
-   One mean (case 3)
-   Two means (case 4)
-   Paired means (case 3)
