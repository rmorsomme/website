---
title: "Inference for proportions"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)
library(openintro)
library(kableExtra)
library(janitor)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   Data analysis in action -- [Inferring ethnicity from X-rays](https://www.boston.com/news/health/2022/05/18/scientists-create-ai-race-from-x-rays-dont-know-how-it-works-harvard-mit/)
-   Anonymous mid-course feedback

## Recap

-   Statistical inference

-   Five cases

-   Hypothesis test

-   Confidence interval

-   A first glimpse of modern statistics

## Outline

-   One proportion (case 1)
-   Two proportions (case 2)

# One proportion

## Setup

**population parameter**: proportion $p$

**Sample statistics**: sample proportion $\hat{p}$

. . .

**Hypothesis testing**:

-   $H_0:p=p_0$ where $p_0$ is a number between $0$ and $1$

-   $H_a:p\neq p_0$

**Confidence interval**: range of plausible values for $p$.

## 

::: callout-note
## One-sided and two-sided $H_a$

One-sided) $H_a:p>p_0$ or $H_a:p<p_0$

Two-sided) $H_a: p\neq p_0$

It is never wrong to use a two-sided $H_a$.
:::

## 

::: {.callout-tip icon="false"}
## Individual exercise - Hypotheses

Exercise [16.1](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises)
:::

```{r, echo = F}
countdown(minutes = 1)
```

## Example -- legalizing marijuana

What proportion of US adults support legalizing marijuana?

. . .

Sample: 900/1500 support it.

. . .

$$
\hat{p} = \dfrac{900}{1500}=0.6
$$

# Hypothesis test via simulation

## 

$H_0:p=0.5$

$H_a:p\neq 0.5$

. . .

-   Simulate many samples under $H_0$.

-   Determine if the observed data could have plausibly arisen under $H_0$.

::: callout-note
## Parametric bootstrap

The textbook uses the term (parametric) bootstrap to refer to this procedure.
:::

## 

```{r}
#| code-line-numbers: "|4|5|6|"
set.seed(0)
results <-  tibble(p_hat = numeric())
for(i in 1 : 1e3){
  sim     <- rbernoulli(n = 1500, p = 0.5) # simulate a sample under H0
  p_hat   <- mean(sim)                     # compute the sample statistic p_hat
  results <- results %>% add_row(p_hat = p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat))
```

## Conclusion

$\hat{p}=0.6$ is extremely unlikely to happen under $H_0:p=0.5$.

We therefore **reject the null hypothesis** that half of the people support legalizing marijuana.

## 

::: {.callout-tip icon="false"}
## Group exercise - HT for one proportion

Exercise [16.3](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises) -- in part d, you do not need to estimate the p-value.
:::

```{r, echo = F}
countdown(minutes = 3)
```

## p-value

What if we do not have a clear-cut case?

-   e.g., $\hat{p}=0.52 = \dfrac{780}{1500}$

. . .

**p-value**: probability that the sample statistic of a sample simulated under $H_0$ is at least as extreme as that of the observed sample.

-   the probability that $\hat{p}_{sim} > 0.52$ or $\hat{p}_{sim} < 0.48$.

## p-value in `R`

```{r}
results <- results %>%
  mutate(is_more_extreme = p_hat <= 0.48 | p_hat >= 0.52)
results
```

```{r}
results %>% summarize(p_value = mean(is_more_extreme))
```

## p-value in practice

In practice, if a p-value is smaller than 0.05 we reject $H_0$

-   $\alpha = 0.05$ is called the **significance level**.

-   If $p<0.05$, the observed sample is in the top 5% of the most extreme simulated samples.

-   It is highly unlikely that the observed sample could have arisen if $H_0$ were true.

-   the difference $\hat{p}-p_0$ is **statistically significant**.

## 

::: callout-note
## Why $\alpha = 0.05$?

To know why the significance level 0.05 is widely used in practice, check out this short [video](https://www.openintro.org/book/stat/why05/).
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - HT for one proportion

Exercise [16.5](https://openintro-ims.netlify.app/inference-one-prop.html#chp16-exercises).
:::

```{r, echo = F}
countdown(minutes = 4)
```

# Confidence interval via bootstrap

## Bootstrapping

Sample with repetition from the observed sample to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ CI

## 

![](images/lec-9/bootstrap1.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
sample_observed <- tibble(support = c(rep(1, 900), rep(0, 600)))
```

```{r}
set.seed(0)
slice_sample(sample_observed, n = 5) # 5 random rows from the data frame
```

```{r}
#| code-line-numbers: "|1,5|2|3|"
sample_bootstrap <- function(data){
  n                <- nrow(data)
  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)
  return(sample_bootstrap)
}
```

## 

```{r, cache=TRUE}
#| code-line-numbers: "|4|5|6|"
results <- tibble(p_hat = numeric()) # empty data frame to collect the results
set.seed(0)
for(i in 1 : 1e3){
  d_boot    <- sample_bootstrap(sample_observed) # bootstrap sample
  stat_boot <- mean(d_boot$support)              # bootstrap statistic
  results   <- results %>% add_row(p_hat = stat_boot) 
}
```

```{r}
quantile(results$p_hat, c(0.05 , 0.95 )) # 90% CI
```

```{r}
quantile(results$p_hat, c(0.025, 0.975)) # 95% CI (wider)
```

## 

```{r}
ggplot(results) + 
  geom_histogram(aes(p_hat)) + 
  geom_vline(xintercept = quantile(results$p_hat, c(0.05 , 0.95 )), col = "gold1", size = 2) + # 90% CI 
  geom_vline(xintercept = quantile(results$p_hat, c(0.025, 0.975)), col = "maroon", size = 2) # 95% CI
```

## 

::: {.callout-tip icon="false"}
## Group exercise - Bootstrap CI

Exercises [12.1](https://openintro-ims.netlify.app/foundations-bootstrapping.html#chp12-exercises) and [12.7](https://openintro-ims.netlify.app/foundations-bootstrapping.html#chp12-exercises)
:::

```{r, echo = F}
countdown(minutes = 3, seconds = 30)
```

# Two proportions

## Setup

A **population** divided in two groups.

**Population parameter**: difference in proportion

$$
p_{diff}=p_1-p_2
$$

**Sample statistics**: difference in proportion in the sample

$$
\hat{p}_{diff}=\hat{p}_1-\hat{p}_2
$$

. . .

$H_0:p_{diff}=0$ (no difference between the two groups)

$H_a:p_{diff}\neq0$

## Example -- sex discrimination

Are individuals who identify as female discriminated against in promotion decisions?

. . .

```{r, echo=FALSE}
sex_discrimination %>% 
  count(decision, sex) %>% 
  pivot_wider(names_from = decision, values_from = n) %>%
  adorn_totals(where = c("col", "row")) %>% 
  kbl(linesep = "", booktabs = TRUE, caption = "Summary results for the sex discrimination study.") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  add_header_above(c(" " = 1, "decision" = 2, " " = 1)) %>%
  column_spec(1:4, width = "7em")
```

## 

::: {.callout-tip icon="false"}
## Group exercise - two proportions

What are $\hat{p}_f$, $\hat{p}_m$ and $\hat{p}_{diff}$? Do you *intuitively feel* that the data provide convincing evidence of sex discrimination?
:::

```{r, echo = F}
countdown(minutes = 2)
```

# Hypothesis test via simulation

## 

$H_0:p_{diff}=0$

$H_a:p_{diff}\neq 0$

. . .

-   Simulate many samples under $H_0$ (no discrimination)

-   Determine if the observed data could have plausibly arisen under $H_0$

## Simulating under $H_0$

Under $H_0$, there is no discrimination

$\Rightarrow$ whether someone receives a promotion is independent of their sexual identification,

$\Rightarrow$ randomly re-assign the $35$ promotions independently of the sexual identification.

## 

![](images/lec-9/randomization.png) Source: [IMS](https://www.openintro.org/book/ims/)

## Simulation result

```{r, echo=FALSE}
sex_discrimination_rand_1 <- tibble(
  sex   = c(rep("male", 24), rep("female", 24)),
  decision = c(rep("promoted", 18), rep("not promoted", 6),
               rep("promoted", 17), rep("not promoted", 7))
) %>%
  mutate(
    sex   = fct_relevel(sex, "male", "female"),
    decision = fct_relevel(decision, "promoted", "not promoted")
  )
  
sex_discrimination_rand_1 %>% 
  count(decision, sex) %>% 
  pivot_wider(names_from = decision, values_from = n) %>%
  adorn_totals(where = c("col", "row")) %>% 
  kbl(linesep = "", booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  add_header_above(c(" " = 1, "decision" = 2, " " = 1)) %>%
  column_spec(1:4, width = "7em")
```

Source: [IMS](https://www.openintro.org/book/ims/)

## 

```{r}
#| code-line-numbers: "|2|3|4|"
d <- gender_discrimination
promotions_ordered <- c(rep(1, 35), rep(0, 13)) # 35 promotions and 13 non-promotions
promotions_sim <- sample(promotions_ordered) # shuffle the promotions
d_sim <- d %>% mutate(decision = promotions_sim)
d_sim
```

## Function for computing the test statistic

```{r}
#| code-line-numbers: "|3,4|5|"
compute_p_diff <- function(data){
  p_hat <- data %>%
    group_by(gender) %>%
    summarize(p_hat = mean(decision))
  p_diff_hat <- p_hat$p_hat[1] - p_hat$p_hat[2]
  return(p_diff_hat)
}
compute_p_diff(d_sim)
```

## For-loop for simulating under $H_0$

```{r, cache=TRUE}
#| code-line-numbers: "|9|10|"
# Setup
results    <- tibble(p_diff_hat = numeric())
d          <- gender_discrimination
promotions <- c(rep(1, 35), rep(0, 13))

# Simulations
set.seed(0)
for(i in 1 : 1e3){
  d_sim <- d %>% mutate(decision = sample(promotions)) # simulate under H0
  p_diff_hat <- compute_p_diff(d_sim) # test statistic
  results <- results %>% add_row(p_diff_hat = p_diff_hat)
}
```

## Sampling distribution

```{r}
p_diff_obs <- 21 / 24 - 14 / 24
p_diff_obs
ggplot(results) + 
  geom_histogram(aes(p_diff_hat)) +
  geom_vline(xintercept = p_diff_obs, col = "maroon", size = 2)
```

## p-value

**p-value**: probability that the sample statistic of a sample simulated under $H_0$ is at least as extreme as that of the observed sample.

-   the probability that $\hat{p}_{diff}^{sim}\ge$ `r signif(p_diff_obs,2)` or $\hat{p}_{diff}^{sim}\le$ `r -signif(p_diff_obs,2)`.

```{r}
results %>%
  mutate(is_more_extreme = p_diff_hat >= p_diff_obs | p_diff_hat <= -p_diff_obs) %>%
  summarize(p_value = mean(is_more_extreme))
```

## significance level $\alpha = 0.05$

Using the usual significance level $\alpha = 0.05$, we **reject** the null hypothesis

-   the observed difference in promotions is unlikely to be due to random luck
-   the difference is **statistically significant**.

## 

::: callout-note
## Statisticians as messengers

Statisticians are just messengers; they only interpret what the data are indicating.

If you are a scientist and are not happy with the result of a statistical analysis, change the study not the statistician!

-   larger sample
-   smaller measurement errors
-   new variables, e.g. salary instead of promotion
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - HT

[17.1](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) (skip part b)
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Confidence interval via bootstrap

## 

Same idea as before: sample with repetition from the observed data to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ CI

## Bootstrap

![](images/lec-9/bootstrap2.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
d <- gender_discrimination %>%
  mutate(decision = if_else(decision == "promoted", 1, 0))
sample_observed_m <- d %>% filter(gender == "male"  )
sample_observed_f <- d %>% filter(gender == "female")

set.seed(0)
sample_bootstrap(sample_observed_m) # bootstrap sample
sample_bootstrap(sample_observed_f) # bootstrap sample
```

## 

```{r}
#| code-line-numbers: "|3,4|5|"
results <- tibble(p_diff_hat = numeric())
for(i in 1 : 1e3){
  d_boot_m   <- sample_bootstrap(sample_observed_m) # bootstrap sample
  d_boot_f   <- sample_bootstrap(sample_observed_f) # bootstrap sample
  p_diff_hat <- compute_p_diff(rbind(d_boot_m, d_boot_f)) # bootstrap statistic
  results    <- results %>% add_row(p_diff_hat = p_diff_hat)
}
```

```{r}
quantile(results$p_diff_hat, c(0.05 , 0.95 )) # 90% CI
quantile(results$p_diff_hat, c(0.05 , 0.95 )) %>% signif(2) # 90% CI
quantile(results$p_diff_hat, c(0.025, 0.975)) %>% signif(2) # 95% CI
```

## 

```{r}
ggplot(results) +
  geom_histogram(aes(p_diff_hat))  + 
  geom_vline(xintercept = quantile(results$p_diff_hat, c(0.05 , 0.95 )), col = "gold1", size = 2) + # 90% CI 
  geom_vline(xintercept = quantile(results$p_diff_hat, c(0.025, 0.975)), col = "maroon", size = 2) # 95% CI
```

## 

::: callout-note
## Two sides of the same coin

In the two examples, the HT and the CI agree with one another. This is not a coincidence; they will agree in the vast majority of cases!

We can show mathematically that a HT and a CI are really two sides of the same coin.
:::

## What does 90% and 95% mean?

Remember that if we could obtain multiple samples, they'd all be a bit different.

$\Rightarrow$ the corresponding CIs would also be a bit different

. . .

-   A 90% CI will capture the true value of the parameter 90% of the time.

-   A 95% CI will be wider and thus more likely to capture the truth (95% of the time).

. . .

-   Trade off between being informative and true.

## 

![](images/lec-9/ci.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## 

::: {.callout-tip icon="false"}
## Group exercises - CI

Exercises [17.3](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises) (only do part c) and [17.5](https://openintro-ims.netlify.app/inference-two-props.html#chp17-exercises)
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Recap

## Recap

-   One proportion

    -   HT via simulation

    -   CI via bootstrap

-   Two proportions

    -   HT via simulation

    -   CI via bootstrap
