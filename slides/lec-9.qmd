---
title: "Inference for proportions"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(countdown)
library(openintro)
library(kableExtra)
library(janitor)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements

-   Data analysis in action -- [Inferring ethnicity from X-rays](https://www.boston.com/news/health/2022/05/18/scientists-create-ai-race-from-x-rays-dont-know-how-it-works-harvard-mit/)

## Recap

-   Statistical inference

-   Five cases

-   Hypothesis test

-   Confidence interval

-   A first glimpse of modern statistics

## Outline

-   One proportion
-   Two proportions

# One proportion

## Setup

(Unknown) **population parameter**: proportion $p$

**Sample statistics**: sample proportion $\hat{p}$

. . .

**Hypothesis testing**:

-   $H_0:p=p_0$ where $p_0$ is a number between $0$ and $1$

-   $H_a:p\neq p_0$

**Confidence interval**: range of plausible values for $p$.

## Example

What proportion of US adults support legalizing marijuana?

. . .

Sample: 900 out of the 1500 participants in a survey say they do.

. . .

Question: What is the sample proportion $\hat{p}$?

## HT via simulation

$H_0:p=0.5$

$H_a:p\neq 0.5$

. . .

-   Simulate many samples under $H_0$

-   Determine if the observed data could have plausibly arisen under $H_0$

## 

```{r}
#| code-line-numbers: "1|2|3,7|4|5|6|"
set.seed(0)
results <-  tibble(p_hat = numeric())
for(i in 1 : 1e3){
  sim     <- rbernoulli(n = 1500, p = 0.5)      # simulate sample under H0
  p_hat   <- mean(sim)                          # compute sample statistic p_hat
  results <- results %>% add_row(p_hat = p_hat)
}
ggplot(results) + geom_histogram(aes(p_hat))
```

## Conclusion

We see that $\hat{p}=0.6$ is extremely unlikely to happen under $H_0:p=0.5$.

We therefore reject the null hypothesis that $p=0.5$.

. . .

If, on the other hand, the sample statistic had been $\hat{p}=0.51$, then it would have been plausible that $p=0.5$, and we would not have rejected $H_0$.

## p-value

What if we do not have a clear-cut case?

-   e.g., $\hat{p}=0.52$

. . .

**p-value**: probability of observing a sample statistics at least as extreme as the observed one under the null hypothesis.

-   the probability that $\hat{p} > 0.52$ or $\hat{p} < 0.48$ if $p=0.5$.

## p-value in `R`

```{r}
results <- results %>%
  mutate(is_more_extreme = p_hat <= 0.48 | p_hat >= 0.52)
results
```

```{r}
summarize(results, p_value = mean(is_more_extreme))
```

## p-value in practice

In practice, if a p-value is smaller than 0.05 we reject $H_0$

-   less than 5% of the simulated samples were at least as extreme as the observed sample,

-   it is highly unlikely that the observed sample could have arisen if $H_0$ were true.

-   the results are **statistically significant**.

. . .

::: callout-note
## Why 0.05?

To know why 0.05 is widely used in practice, check out this short [video](https://www.openintro.org/book/stat/why05/).
:::

# CI via bootstrap

Sample with repetition from the observed sample to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ bootstrap CI

## Bootstrap

![](images/lec-9/bootstrap1.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
sample_bootstrap <- function(data){
  n                <- nrow(data)
  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)
  return(sample_bootstrap)
}
sample_observed <- tibble(support = c(rep(1, 900), rep(0, 1500 - 900)))
```

```{r}
set.seed(0)
slice_sample(sample_observed, n = 5) # 5 random rows from the sample
```

## 

```{r, cache=TRUE}
results <- tibble(p_hat = numeric()) # empty data frame to collect the results
set.seed(0)
for(i in 1 : 1e3){
  d_boot  <- sample_bootstrap(sample_observed) # bootstrap sample
  results <- results %>% 
    add_row(p_hat = mean(d_boot$support)) # sample statistic from bootstrap sample
}
quantile(results$p_hat, c(0.05 , 0.95 )) # 90% CI
quantile(results$p_hat, c(0.025, 0.975)) # 95% CI
```

## 

```{r}
ggplot(results) + geom_histogram(aes(p_hat))
```

# Two proportions

## Setup

A **population** divided in two groups (group 1 and group 2)

(Unknown) **population parameter**: difference in proportion $p_{diff}=p_1-p_2$

**Sample statistics**: sample proportion $\hat{p}_{diff}=\hat{p}_1-\hat{p}_2$

$H_0:p_{diff}=0$ (there is no difference between the two groups)

$H_a:p_{diff}\neq0$

## Example -- sex discrimination

Are individuals who identify as female discriminated against in promotion decisions made by their managers?

. . .

```{r, echo=FALSE}
sex_discrimination %>% 
  count(decision, sex) %>% 
  pivot_wider(names_from = decision, values_from = n) %>%
  adorn_totals(where = c("col", "row")) %>% 
  kbl(linesep = "", booktabs = TRUE, caption = "Summary results for the sex discrimination study.") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  add_header_above(c(" " = 1, "decision" = 2, " " = 1)) %>%
  column_spec(1:4, width = "7em")
```

## 

::: {.callout-tip icon="false"}
## Group exercise - two proportions

What are $\hat{p}_f$, $\hat{p}_m$ and $\hat{p}_{diff}$? Do you *intuitively feel* that the data provide convincing evidence of sex discrimination?
:::

```{r, echo = F}
countdown(minutes = 3)
```

# HT via simulation

$H_0:\hat{p}_{diff}=0$

$H_a:\hat{p}_{diff}\neq 0$

. . .

-   Simulate many samples under $H_0$ (assuming that there is no discrimination)

-   Determine if the observed data could have plausibly arisen under $H_0$

## Simulating under $H_0$

Under the null hypothesis, there is no discrimination -- whether someone receives a promotion is independent of their sexual identification.

. . .

Goal: model the randomness that would occur if the null hypothesis was true.

To accomplish that, we can randomly re-assign the $35$ promotions among the $45$ individuals, independently of their sexual identification.

## 

![](images/lec-9/randomization.png) Source: [IMS](https://www.openintro.org/book/ims/)

## Simulation result

```{r, echo=FALSE}
sex_discrimination_rand_1 <- tibble(
  sex   = c(rep("male", 24), rep("female", 24)),
  decision = c(rep("promoted", 18), rep("not promoted", 6),
               rep("promoted", 17), rep("not promoted", 7))
) %>%
  mutate(
    sex   = fct_relevel(sex, "male", "female"),
    decision = fct_relevel(decision, "promoted", "not promoted")
  )
  
sex_discrimination_rand_1 %>% 
  count(decision, sex) %>% 
  pivot_wider(names_from = decision, values_from = n) %>%
  adorn_totals(where = c("col", "row")) %>% 
  kbl(linesep = "", booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  add_header_above(c(" " = 1, "decision" = 2, " " = 1)) %>%
  column_spec(1:4, width = "7em")
```

## Simulating under $H_0$

```{r}
#| code-line-numbers: "2|3|4|"
d <- gender_discrimination %>% select(gender)
promotions_ordered <- c(rep(1, 35), rep(0, 48 - 35)) # 35 promotions and 13 non-promotions
promotions_sim <- sample(promotions_ordered, 48) # shuffle promotions
d_sim <- d %>% mutate(response = promotions_sim)
d_sim
```

## Function for computing the test statistic

```{r}
compute_p_diff <- function(data){
  p_hat <- data %>%
    group_by(gender) %>%
    summarize(p_hat = mean(response))
  p_diff_hat <- p_hat$p_hat[1] - p_hat$p_hat[2]
  return(p_diff_hat)
}
compute_p_diff(d_sim)
```

## For-loop for simulating under $H_0$

```{r, cache=TRUE}
# Setup
results    <-  tibble(p_diff_hat = numeric())
d          <- gender_discrimination
promotions <- c(rep(1, 35), rep(0, 48 - 35))

# Simulations
set.seed(0)
for(i in 1 : 1e3){
  d_sim <- d %>% mutate(response = sample(promotions, 48)) # simulate under H0
  results <- results %>% add_row(p_diff_hat = compute_p_diff(d_sim))  # test statistic
}
```

## Sampling distribution

```{r}
p_diff_obs <- 21 / 24 - 14 / 24
ggplot(results) + 
  geom_histogram(aes(p_diff_hat)) +
  geom_vline(xintercept = p_diff_obs, col = "maroon", size = 2)
```

## p-value

**p-value**: the probability of observing a test statistic at least as extreme as the observed one under the null hypothesis.

-   the probability that $\hat{p}_{diff}>$ `r signif(p_diff_obs,2)` or $\hat{p}<$ `r -signif(p_diff_obs,2)` if there is no discrimination.

```{r}
results %>%
  mutate(is_more_extreme = p_diff_hat >= p_diff_obs | p_diff_hat <= -p_diff_obs) %>%
  summarize(p_value = mean(is_more_extreme))
```

## significance level $\alpha = 0.05$

Using the usually significance level $\alpha = 0.05$, we **fail to reject** the null hypothesis

-   the data do not provide sufficient evidence to reject the null hypothesis,
-   it is plausible to observe the sample collected in the study if there is no discrimination just by chance.

::: callout-note
## Statisticians as messengers

Statisticians are only messengers of the data; they only interpret what the data are indicating.
:::

# CI via bootstrap

Same idea as before: sample with repetition from the observed data to construct many **bootstrap samples**.

Bootstrap samples $\Rightarrow$ sampling distribution $\Rightarrow$ bootstrap CI

## Bootstrap

![](images/lec-9/bootstrap2.png)

Source: [IMS](https://www.openintro.org/book/ims/)

## Bootstrap in `R`

```{r}
sample_observed_m <- tibble(response = c(rep(1, 21), rep(0, 3 )), gender = "male"  )
sample_observed_f <- tibble(response = c(rep(1, 14), rep(0, 10)), gender = "female")
set.seed(0)
sample_bootstrap(sample_observed_m) # bootstrap sample
sample_bootstrap(sample_observed_f) # bootstrap sample
```

## 

```{r}
#| code-line-numbers: "3,4|5|6|"
results <- tibble(p_diff_hat = numeric())
for(i in 1 : 1e3){
  d_boot_m   <- sample_bootstrap(sample_observed_m) # bootstrap sample
  d_boot_f   <- sample_bootstrap(sample_observed_f) # bootstrap sample
  p_diff_hat <- compute_p_diff(rbind(d_boot_m, d_boot_f))
  results    <- results %>% add_row(p_diff_hat = p_diff_hat)
}
quantile(results$p_diff_hat, c(0.05 , 0.95 )) # 90% CI
quantile(results$p_diff_hat, c(0.05 , 0.95 )) %>% signif(2) # 90% CI
quantile(results$p_diff_hat, c(0.025, 0.975)) %>% signif(2) # 95% CI
```

## 

```{r}
ggplot(results) + geom_histogram(aes(p_diff_hat))
```
