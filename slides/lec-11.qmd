---
title: "Inference for regression"
subtitle: "STA 101L - Summer I 2022"
author: "Raphael Morsomme"
footer:  "[https://rmorsomme.github.io/website/](https://rmorsomme.github.io/website/)"
logo: "images/logo.jpg"
format: 
  revealjs: 
    theme: slides.scss
    transition: fade
    slide-number: true
    code-fold: false
    code-summary: "Show the code"
    scrollable: true
    link-external-newwindow: true
    history: false
editor: visual
execute:
  freeze: auto
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

# load packages
library(tidyverse)   # for data wrangling and visualization
library(countdown)
library(openintro)
library(kableExtra)
library(janitor)
library(patchwork)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

options(scipen = 100)
```

# Welcome

## Announcements -- remaining deadlines

-   Homework

    -   6 -- tonight

    -   7 -- Monday, June 13

    -   8 -- Thursday, June 16

    -   free one-day extension; homework with lowest grade dropped

-   Inference project

    -   Presentation -- Friday, June 17 (lab)

    -   Written report -- Thursday, June 23

## Announcements -- inference project

-   [Teams](/slides/project-inference-teams.html)

-   [Overview](/project-inference.html)

-   2 data sets or topics before tomorrow's lab

-   Leave lab with a data set

## Recap {.smaller}

-   The normal distribution

. . .

-   One mean

    -   CI via bootstrap

. . .

-   Two means

    -   HT via simulation

    -   CI via bootstrap

. . .

-   Paired means (similar to one mean)

    -   HT via simulation

    -   CI via bootstrap

    -   Always pair!

## Outline

-   Simple linear regression
    -   HT via simulation

    -   CI via bootstrap

# Simple linear regression

## Setup

**Simple linear regression** (slr): $Y \approx \beta_0 + \beta_1 X$

**Population parameter**: slope parameter $\beta_1$

**Sample statistic**: least-square estimate $\hat{\beta}_1$

. . .

**Confidence interval**: range of plausible values for $\beta_1$

**Hypothesis test**: is the response variable $Y$ independent of the predictor $X$?

-   $H_0:\beta_1 = 0$ ($Y$ does not depend on $X$)

-   $H_a:\beta_1\neq0$

## Example -- birth weight

::: columns
::: {.column width="55%"}
```{r}
set.seed(47)
d <- openintro::births14 %>%
  sample_n(100) %>% # take a random sample of 100 births
  select(weight, mage) %>% # only keep the variables weight (newborn's weight) and mage (mother's age) 
  rename(mother_age = mage)
head(d)
```
:::

::: {.column width="5%"}
:::

::: {.column width="36%"}
![](images/lec-11/newborn.jpg){fig-align="center"}
:::
:::

## Original data

```{r}
ggplot(d, aes(x = mother_age, y = weight)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = "maroon") # regression line
```

## 

::: {.callout-tip icon="false"}
## Individual exercise - gut feeling

Do you *intuitively feel* that the pattern could be explained by chance alone?
:::

```{r, echo = F}
countdown(minutes = 1)
```

# Hypothesis test via simulation

## Hypotheses and simulations

$H_0:\beta_1=0$

$H_a:\beta_1\neq 0$

. . .

-   Simulate many samples under $H_0$ (the response is independent of the predictor).

-   Determine if the observed data could have plausibly arisen under $H_0$.

## Simulating under $H_0$

Under $H_0$, there is no relation between the response and the predictor

$\Rightarrow$ the newborn's weight is independent of the mother's age

$\Rightarrow$ randomly re-assign the predictor independently of the response.

## 

```{r}
d_sim <- d %>% mutate(mother_age = sample(mother_age)) # shuffle the response
head(d)
head(d_sim)
```

## Function for computing the test statistic

```{r}
#| code-line-numbers: "|2|3|4|"
compute_beta_LS <- function(data){
  m <- lm(weight ~ mother_age, data = data)
  coef <- m$coefficients
  slope <- coef[["mother_age"]] 
  return(slope)
}
compute_beta_LS(d_sim)
```

## For-loop for simulating under $H_0$

```{r, cache=TRUE}
results <- tibble(stat_sim = numeric())
set.seed(1)
for(i in 1 : 5e3){
  d_sim    <- d %>% mutate(mother_age = sample(mother_age)) # simulate under H0
  stat_sim <- compute_beta_LS(d_sim) # test statistic
  results  <- results %>% add_row(stat_sim)
}
```

## Sampling distribution

```{r}
stat_obs <- compute_beta_LS(d)
stat_obs
ggplot(results) + 
  geom_histogram(aes(stat_sim)) +
  geom_vline(xintercept = stat_obs, col = "maroon", size = 2)
```

## p-value

Probability that $\hat{\beta}_{1}^{sim}\ge \hat{\beta}^{obs}_1$ or $\hat{\beta}_{1}^{sim}\le \hat{\beta}^{obs}_1$.

```{r}
results %>%
  mutate(is_more_extreme = stat_sim >= stat_obs | stat_sim <= -stat_obs) %>%
  summarize(p_value = mean(is_more_extreme))
```

Using the usual significance level $\alpha = 0.05$, we **reject** $H_0$.

. . .

```{r}
m <- lm(weight~mother_age, data = d)
broom::tidy(m)
```

## 

::: {.callout-tip icon="false"}
## Group exercise - HT

Exercises [24.9](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises) -- in part c, refer to the output of the `lm` command (mathematical model) -- and [24.1](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises).
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Confidence interval via bootstrap

## Bootstrap

```{r}
sample_bootstrap <- function(data){ # same function as before
  n                <- nrow(data)
  sample_bootstrap <- slice_sample(data, n = n, replace = TRUE)
  return(sample_bootstrap)
}
```

```{r}
set.seed(0)
sample_bootstrap(d)
```

## 

![](images/lec-11/bootstrap.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

## 

```{r, cache=TRUE}
results <- tibble(stat_boot = numeric())
set.seed(0)
for(i in 1 : 5e3){
  d_boot    <- sample_bootstrap(d)     # bootstrap sample
  stat_boot <- compute_beta_LS(d_boot) # bootstrap statistic
  results   <- results %>% add_row(stat_boot) 
}
```

## 

![](images/lec-11/bootstrap_model.png){fig-align="center"}

Source: [IMS](https://www.openintro.org/book/ims/)

## Simulated slopes

```{r}
ggplot(results) + 
  geom_histogram(aes(stat_boot)) + 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.05, 0.95)), col = "gold1", size = 2) + # 90% CI 
  geom_vline(xintercept = quantile(results$stat_boot, c(0.01, 0.99)), col = "maroon", size = 2) # 98% CI
```

## 

```{r}
quantile(results$stat_boot, c(0.05 , 0.95 )) # 90% CI
```

```{r}
quantile(results$stat_boot, c(0.01, 0.99)) # 98% CI (includes 0)
```

## CI and HT

::: callout-note
## Two sides of the same coin

The 95% CI dos not include 0, but the 98% CI does. This indicates that 0 is a plausible at the (usual) 5% significance level but not the 2% significance level. Again, this is exactly what the HT concluded.
:::

## 

::: {.callout-tip icon="false"}
## Group exercise - CI

Exercise [24.11](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises). Does the CI include 0? Does it agree with the HT?

Exercise [24.3](https://openintro-ims.netlify.app/inf-model-slr.html#chp24-exercises).
:::

```{r, echo = F}
countdown(minutes = 3)
```

# Recap

## Recap

-   Simple linear regression
    -   HT via simulation

    -   CI via bootstrap
